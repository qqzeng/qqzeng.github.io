---
title: 简单文件系统的崩溃一致性和日志
date: 2020-01-07 15:45:53
categories:
- 单机存储
tags:
- 单机存储
- 文件系统
- 崩溃恢复
- 一致性
- 日志
---

上一篇文章从早期 unix 系统所使用的文件系统存在的典型问题切入，介绍`The Fast File System`三个方面的工作：基于数据访问的局部性，优化了磁盘数据块布局方式以提升磁盘效率，同时允许使用更灵活的数据块大小以提升磁盘利用率和传输速率，最后在兼容已有文件系统接口规范的前提下，通过增强已有功能以进一步提升系统易用性。本文主题是文件系统的崩溃一致性(`crash consistency`)和日志(`journaling`)，文件系统的`crash consistency`问题可以通过若干种方式来解决，其中典型的两种方案就是`file system checker(fsck)`和`journaling`。所谓的`crash consistency`问题指的是在文件系统更新磁盘数据结构的过程中发生某种软硬件问题而使得更新被迫中断，最终导致磁盘被更新数据结构处于不一致状态。其中`fsck`是早期文件系统使用的恢复策略，是一种`lazy`的修复策略，存在严重的效率问题。相反，`journaling`则是更为`active`的解决方案，也是一种广泛被使用的解决方案。

<!-- More-->

同前两篇文章类似，本文是一篇总结性的文章。本文的目标为：使用一个简单文件系统作为原型，来深入了解`crash consistency`问题本身，以及能够解决它的各种方法技术具体的工作原理。这对于理解一个文件系统的`crash recover`有较大帮助。本文首先从一个简单的`crash consistency`问题实例切入，即先将问题抛出，然后分别介绍它的各种解决办法，早期的文件系统使用`fsck`在重启文件系统过程中校验并恢复整个文件系统，主流的解决办法是使用`journaling`或者称为`write-ahead logging`，其只需在写入数据的同时花费额外的一点写开销，就能使得文件系统后期高效恢复。

# Crash Consistency Problem

所谓的`crash consistency`也被称为是`consistent update`。问题产生的本质原因是：文件系统对磁盘发出的更新操作通常是一个复合操作，即磁盘无法通过一次写入操作就能完成整个更新请求，相反，它需要依次更新多个相关数据结构，即多次写入磁盘才能顺利完成请求。在这多个依次写入的操作中间，系统发生了某种软件或硬件方面的错误（如突然断电`power loss`或者系统崩溃`system crash`），使得此次更新请求未能顺利完成，即只更新了部分数据结构，这就导致被更新的数据结构处于一个不不一致的状态，这可能导致很多问题，典型包括`space leak`、`garbage data`和`inconsistent data structures`三种 。

我们使用`vsfs`作为原型，通过一个简单实例来阐述`crash consistency`问题。假设目前磁盘上的数据块布局如下，只有一个文件 foo，其`inode`编号为 2，且我们想往文件 foo 追加数据内容。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452123/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-exp-1_zxkxsl.png" alt="foo 文件相关数据块的磁盘布局" style="zoom:50%;" />

从[这里](https://qtozeng.top/2020/01/05/%E7%AE%80%E5%8D%95%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-vsfs/)我们可以了解到更新一个磁盘上的文件包含了对磁盘上相关数据结构的一系列更新操作，在这个实例中，需要更新对应的`inode`（增加一个直接指针，以及更新文件大小、访问和修改时间等属性），写入新的数据块 Db，以及更新`d-bmap`（标记 Db 已被写入）。下图为写入期间未发生任何错误的情况下，更新后磁盘布局。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452123/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-exp-2_h1bexm.png" alt="foo 文件被更新后相关数据块的磁盘布局" style="zoom:50%;" />

若不幸在整个更新过程中发生了错误（一般而言，文件系统通过`page cache`或`buffer cache`来缓存对磁盘的更新，并周期性的实施刷盘操作，但同步刷盘操作中也可能被中断），那么磁盘中关于文件 foo 的状态可能有如下两种情况：

- 仅顺利更新一个数据结构。那么进一步存在三种可能的状态：
  - Db 被顺利写入。事实上，此次写入是无效的，因为没有`inode`能索引到 Db，也没有任何`d-bmap`记录了 Db 所在数据块已被占用，因此后续其可能被其它数据覆盖；
  - `inode`被顺利更新。在这种情况下，虽然文件元数据所包含的指针被更新，但其指向的是`garbage data`，且`d-bmap`也没有任何记录对应的数据块被占用，因此整个文件系统处于一个不一致的状态，必须对它进行修复后，文件系统才能继续工作；
  - `d-bmap`被顺利更新。同上一种情况类似，整个系统同样处于一个不一致的状态。且原本被 Db 占用的空间不会被任何数据填充，造成了磁盘`space leak`。
- 仅顺利更新两个数据结构。同样存在三种可能的状态：
  - `inode`和`d-bmap`都被顺利更新。此时问题很简单，造成了文件系统后续将读入`garbage data`，尽管在文件系统看来，整个系统处于一致状态；
  - `inode`被顺利更新，且 Db 也被顺利写入。此时文件系统处于不一致的状态，需修复后才能继续工作；
  - `d-bmap`被顺利更新，且 Db 也被顺利写入。文件系统同样处于不一致的状态，虽然数据被成功写入，且也被成功标记，但文件系统却不知道 Db 是隶属于哪个文件。

从上面简单分析可以得出，文件系统的崩溃可能导致多种问题，具体包括`space leak`、`garbage data`和`inconsistent data structures`，其中后两种问题可造成严重后果。

# The File System Checker

`fsck`是早期文件系统所采用的崩溃恢复手段，详情可参考文献[1]。它的工作原理很简单：允许不一致的情况发生，但在每次系统重启时，文件系统被挂载前，对它进行修复。但需明确的是，`fsck`并不能修复所有（上述三种）问题，典型的，在发生`garbage data`情况下，其根本无法辨别出数据块是否为`garbage data`。换言之，`fsck`只能解决文件系统存在的不一致问题。`fsck`会对多个方面实施校验检测，具体包括`superblock`检测（比如检测其完整性，否则使用其副本），`free blocks`检测（通过遍历所有数据块临时性重构`d-bmap`以检验其同`inode`之间是否保持一致），`inode state`检测（校验`indoe`完整性，否则清除`inode`及相关数据结构），`inode links`校验（通过临时计算出`inode`的链接数来校验为`inode`保存的链接数属性是否准确），最后也会检测`duplicate pointers`、`bad blocks`以及`directory`，详情可参考原文。

`fsck`存在的问题很明显，除了不能解决`garbage data`的问题外，更让人不能接受的是，修复所耗费时间过长，这在文件系统日渐增长的情况下显得尤为突出。因此，研究人员也在探索其它解决方案。

# Journaling (Write-Ahead Logging)

考虑到`fsck`存在的问题，研究人员对于文件系统的崩溃一致性的解决方案采取了同数据库管理系统类似的办法——`write-ahead logging`，在文件系统中被称为`journaling`。所谓的`write-ahead logging`直译为预写式日志，即在向磁盘写入实际的数据之前，先额外写一些数据（也被称为日志记录`log`）到磁盘指定区域，然后再更新磁盘写入实际数据。如此一来，若在磁盘更新的过程中发生了系统崩溃，则可读取之前写入到磁盘指定区域的`log`，以推断出被中断的更新操作的详细内容，然后针对性地重新执行更新操作。此种方案避免了对整个文件系统的全盘扫描，因此理论上提高了崩溃恢复效率。`ext3`正是在早期`ext2`文件系统之上进一步整合了`journaling`机制。并且假设将`write-ahead log`存储在同一磁盘或分区上（当然其也可存储在其它设备上）。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452123/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-journal-layout_d4wion.png" alt="ext3 简化的磁盘布局（包含日志）" style="zoom:50%;" />

## Data Journaling

`data journaling`是一种简单的`journaling`方式，是`ext3`文件系统提供的一种`journaling`模式。顾名思义，它会将被更新后的完整数据块连同被更新的元数据信息写入到`write-ahead log`中。简单而言，同样以向 foo 文件追加数据操作作为示例，`data journaling`模式会依次写入`TxB`，`I[v2]`、`B[v2]`和`TxE`，这四个数据结构共同构成了一条完整的`data journaling log`。其中`TxB`标记一条新的日志记录的开始，它一般会包含`transaction id`和被更新的 foo 文件所关联的数据结构的地址信息（即`I[v2]`、`B[v2]`和`Db`）；`TxE`标记着日志记录的结束，一般同样会包含`transaction id`。而中间的三个数据块即为需更新到磁盘的相关数据块。考虑到此种`journaling`方式将被更新的数据块也写入到日志中，因此也称为是`physical logging`，其示意图如何所示。相对地，也有对应的`logical logging`模式，它不将具体的数据块也纳入到日志，这种方式后文会阐述。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452123/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-journal-log-layout_niytif.png" alt="data journaling log 数据结构" style="zoom:50%;" />

因此，当采用`data journaling`模式，整个更新过程可划分为两个阶段：
1. `Journal write`，即先写日志，将日志内容写入到磁盘指定区域，包含上述`data journaling`日志组成的 5 个部分；

2. `Checkpoint`，即表示将需要追加或更新的文件数据内容真正地更新到磁盘对应的数据块中。

读者可能很快会发现，这种模式存在一个问题：若在写入日志的过程中，系统崩溃，那么可能会发生意想不到的事情。考虑到文件系统一般会缓存 IO 请求以提升写性能，换言之，文件系统可能在合理的任意时间点以任意顺序写入`data journaling`日志记录所包含的 5 个部分。下面是一种典型的情况，即磁盘调度器可能会先将`TxB`、`I[v2]`、`B[v2]`和`TxE`先写入到磁盘，然后在未来得及写入`Db`时，发生了系统崩溃，那么，此时写入磁盘的日志状态如下图所示。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452123/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-journal-log-layout-bad-case_yqqzk2.png" alt="日志记录可能发生的 torn write 情形" style="zoom:50%;" />

在崩溃恢复阶段，当读入这条日志记录时，可能引起严重错误，因为 Db 的数据块内容是任意`garbage data`。因此，为了应对该问题，文件系统采用一种类似于“二阶段提交”的方式来写入日志：它先写入除`TxE`结构之外的日志部分，等到它们真正被写入磁盘后，再接着写入`TxE`结构到磁盘。如此一来，一方面，若在写入第一部分的日志记录时发生了系统崩溃，那么此条日志记录是不完整的，在文件系统重启执行崩溃恢复时，会将此条日志记录视为非法，因此不会导致任何不一致的状态。另一方面，考虑到`TxE`结构一般较小，不足一个扇区大小，因此其写入操作不会发生`torn write`，换言之，日志记录的第二阶段的写入也是原子性的。总而言之，通过这种类似于“二阶段提交”式的日志写入，整个文件写入过程可进一步划分为三个阶段：

1. `Journal write`，同样先写日志，只不过先要确保除`TxE`结构之外的日志部分先写入到磁盘；
2. `Journal commit`，进一步写入`TxE`，以确保整条日志记录写入的原子性；
3. `Checkpoint`，最后才将需要追加或更新的文件数据内容真正地更新到磁盘对应的数据块中。

在顺利为每个更新操作写入`data journaling`日志记录后，一旦发生任何系统崩溃情况，则在文件系统恢复过程中：一方面，若是在日志记录本身的写入过程中发生了系统崩溃，此时日志记录并不完整，因此恢复程序应当直接跳过，不会造成系统任何不一致现象。另一方面，若是在`checkpoint`阶段发生了系统崩溃，则只需要读取并解析对应日志内容，然后实施日志`replay`即可重新尝试将更新持久化到磁盘，这种日志类型在数据库管理系统中被称为`redo log`。

下图同样表示在对文件的更新过程中，使用`data journaling`策略来保证`crash consistency`，和文件相关的磁盘数据结构写入的相对顺序，图中往下表示时间增长，虚线隔开的各个操作表示它们之间必须严格按照相对顺序执行，而同为虚线框内的一组操作表示它们之间的执行顺序是任意的。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452123/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-journal-data-timeline_nvzrbg.png" alt="data journaling 模式下各操作相对执行顺序" style="zoom:50%;" />

最后补充一点。有读者可能会想，既然为了避免日志写入发生`torn write`，那么是不是可以将构成日志的 5 个部分依次按顺序写入磁盘，且保证前一个部分成功写入磁盘后，才写入后一个部分。理论上，这种方式没有任何问题，但其代价也显而易见，大幅度降低文件系统性能，因为它使得文件系统的写缓冲没有任何作用。基于此，`ext4`文件系统采用了一种更加优雅的解决方案，为了同时保证日志写入的高效以及写入的原子性，在每条日志记录的开始和末尾处增加了日志记录中数据块的`checksum`，同时，在崩溃恢复程序读取日志时，会临时计算数据块的`checksum`，同时对比保存在日志记录中的值，若二者不等，则表明日志记录是不完整的。否则，证明日志记录确实是完整的，即日志记录在写入过程中未发生系统崩溃。详细内容可参考文献[2]。

## Batching Log Updates

`data jounaling`模式能基本解决`crash consistency`问题，其不足之处在于日志记录包含了实际数据块，因此占用了较多额外空间。另外，恢复过程还会引入部分`redundant write`。针对`redundant write`的问题，我们考虑这样一个场景：同一个目录下的两个文件 foo1 和 foo2 依次进行更新，在[这里](https://qtozeng.top/2020/01/05/%E7%AE%80%E5%8D%95%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-vsfs/)我们知道更新目录下的一个文件，至少需要更新的磁盘数据结构包括`i-bmap`、`inode`、目录所关联的`data block`以及目录的`inode`，因此，这些信息全部需要作为日志内容写入到磁盘，那么 foo1 和 foo2 的更新则需要重复写入目录的`inode`及其关联的`data block`，因此会导致较多写操作开销（考虑当更新同一个目录下的多个文件或目录的情况）。针对这一问题，`ext3`文件系统采用`batch update`策略来解决。具体而言，它会先将文件的更新进行缓存，并标记对应的日志记录需要存盘，当发现同一目录中其它文件也需要更新时，会将对应的日志记录合并到前一文件所对应的日志记录的数据块中，最后当达到刷盘周期时，将包含多个文件更新的日志记录一次性写入磁盘。而针对前一个问题，可以使用`metadata journaling`模式来进行优化。

## Making The Log Finite

上一小节提到的`data journaling`方式不仅存在占用过多额外磁盘空间问题，而且也会增加崩溃恢复过程的耗时。其解决方式比较直接，通过日志循环写入(`circular log`)配合日志释放来解决。具体而言，将存储日志的磁盘区域作为一个环形数组即可，环形数组的首尾指针即为没有被释放的日志记录边界，为了方便，可以将这两个指针存储在`journal superblock`中。如下图所示。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452123/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-journal-log-layout-free_vmujha.png" alt="circular log" style="zoom:50%;" />

为了实现`circular log`，需在每次成功文件更新的操作后，即时将日志区域中对应条目释放掉（即更新`circular log`的首尾指针）。通过引入`circular log`后，为了保证`crash consistency`，更新文件的整个过程可扩展为如下四个阶段：

1. `Journal write`，写入除`TxB`结构之外的其它日志记录内容；
2. `Journal commit`，进一步写入日志的`TxB`结构，至此，完成了日志记录的原子性写入；
3.  `Checkpoint`，将文件更新或追加的数据真正写入到磁盘；
4. `Free`，释放掉步骤 1 和 2 中写入的日志记录，以备后续空间复用。

至此，`data journaling`模式已较为完善，但其仍存在的问题是：每一个被更新的文件，其更新数据块需要被写入两次。

## Metadata Journaling

事实上，使用`data journaling`模式必须将更新数据块写入两次所带来的问题具体表现为：在写入元数据和实际数据之间的寻道操作开销较大。因此，进一步实现了`metadata journaling`(`ordered journaling`)来解决此问题。容易想到，`metadata journaling`只将被更新文件的元数据信息写入日志记录，如下图所示。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452124/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-journal-log-layout-meta_t1gexs.png" alt="metadata journaling 模式日志布局" style="zoom:50%;" />

但若只纯粹将被更新的数据块从日志记录中移除，而不对文件更新的总规则进行调整，将引发一个额外的问题。我们不妨考虑何时将更新文件数据块真正写入磁盘，若仍旧按照之前的规则，即在将被更新文件的元数据信息写入日志记录后，才写入更新文件的数据块，则可能导致的问题是：假设在将更新文件的数据块写入磁盘时发生了系统崩溃，那么此时日志记录中包含的元信息索引的数据块实际上是无效的`garbage data`。这导致即使实施了崩溃恢复，且在文件系统的视角看来，整个文件系统确实处于一致状态，但文件中却包含`garbage data`。因此，为了避免这种情形发生，`ext3`文件系统选择先将被更新文件的数据块真正写入到磁盘，然后再写日志记录（二个阶段），最后 再将被更新文件的元数据信息真正写入磁盘。通过将数据块写入的顺序调整到日志中元信息的写入操作之前，避免了日志记录中的元信息引用了无效的数据块内容。总而言之，调整后的文件更新规则如下：

1. `Data write`，将被更新的文件的数据块真正写入到磁盘；
2. `Journal metadata write`，只将被更新的文件的元信息相关的数据块以及`TxB`构成的日志记录写入到磁盘；
3. `Journal commit`，进一步将此日志记录的`TxE`结构写入磁盘；
4. `Checkpoint metadata`，将被更新文件的元信息相关的数据块真正写入到磁盘；
5. `Free`，释放掉步骤 2 和 3 中写入的日志记录，以备后续空间复用。

需要注意的是，步骤 1 和 2 的相对顺序可以任意（即可并发执行），只需确保步骤 3 在 1 和 2 之后完成即可。

同`data journaling`类似，下图同样表示在对文件更新过程中，使用`metadata journaling`模式来保证`crash consistency`，和文件相关的磁盘数据结构写入的相对顺序，图中往下表示时间增长，虚线隔开的各个操作表示它们之间必须严格按照相对顺序执行，而同为虚线框内的一组操作表示它们之间的执行顺序是任意的。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452124/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-journal-meta-timeline_le6j8l.png" alt="metadata journaling timeline" style="zoom:50%;" />

## Tricky Case: Block Reuse

关于使用`journaling`来保证文件更新操作的`crash consistency`的主要内容已阐述完毕。这一小节简单阐述在磁盘数据块被重用时可能出现的棘手问题。如下图所示，foo 表示一个目录，当我们在目录 foo 下创建一个文件时，若采用`metadata journaling`模式，其磁盘数据块布局可简化如下。注意，因为目录所包含内容也被视为元数据，因此会被记录到日志中。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452124/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-journal-meta-reuse-1_przz2u.png" alt="data block reuse-1" style="zoom:50%;" />

此时若用户删除目录 foo 中所有内容以及目录自身，则原本被目录所占用的日志记录空间被释放以备复用。最后，若用户又创建新文件 bar，且文件系统恰好将 bar 文件相关内容写入到原本属于目录 foo 所在磁盘空间。若文件系统成功写入对文件 bar 的日志记录，此时用于记录日志的磁盘布局如下图所示，且文件 bar 也被成功写入磁盘（即成功`checkpoint`），而后某个时间点发生了系统崩溃，那么在执行`replay`过程中，当读取到关于目录 foo 的日志记录时，它会直接简单地重新将目录 foo 的内容写入到磁盘中已经被 bar 文件关联的`data block`占用的空间，因此覆盖了 bar 文件数据，导致用户读取 bar 文件时，产生意想不到的后果。

<img src="https://res.cloudinary.com/turalyon/image/upload/v1578452124/blog/33-Crash-Consistency-Fsck-and-Journaling/ccj-journal-meta-reuse-2_nhk36y.png" alt="data block reuse-2" style="zoom:50%;" />

此种情况的原因很简单：未恰当地处理数据块重用。`ext3`文件系统的处理方式比较简单，它会将那些被删除文件或目录的日志记录标记成`revoke`，且在崩溃恢复过程中，当扫描到包含`revoke`的日志记录时，不会对此日志记录执行`replay`，因此避免了数据块覆盖的情况。

# Other Approaches

除了`fsck`和`journaling`这两种能够保证文件系统`crash consistency`的方法外，还有一些其它的解决方案。其中一种被称为`Soft Updates`，它的基本原理是将所有文件的更新操作请求进行严格排序，并且保证磁盘对应的数据结构不会处于不一致的状态，比如先写文件数据内容，再写文件元信息，以保证`inode`不会关联一个无效的数据块。但是`Soft Updates`实现起来比较复杂，需要充分了解文件系统内部相关数据结构知识，有兴趣的读者可参考文献[3]。另一种解决被称为是`copy-on-write(cow)`的方案则更为流行，我们对`cow`技术并不感到陌生，它的核心原理为：它不直接更新文件包含的数据块，相反，它会创建一个完整的更新后的副本，当完成了若干个更新操作后，再一次性将更新后的数据块关联到被对应的被更新文件。`ZFS`就同时使用`cow`和`journaling`两种技术，有兴趣的读者可参考文献[4]。另外，原书中还提到一种`optimistic crash consistency`技术，它通过使用`transaction checksum`技术（参考文献[2]），主要用于优化磁盘写入日志记录的过程，以减少等待数据刷盘所导致的时间开销。详细内容可参考文献[5]。

简单小节，本文内容较多。先是从文件更新可能造成的`crash consistency`问题切入，详细阐述其可能造成的三种后果。然后，详细介绍了能够保证文件更新的`crash consistency`的两种解决方案：一种是`lazy`性质的`fsck`，其主要不足之处在于校验恢复耗时。另一种则是`journaling`方式，它包括`data journaling`和`metadata journaling`两种模式，后者相较于前者减少了文件数据块的写操作开销，但也更为复杂。在阐述`jouranling`模式时，同时阐述相关操作的一些优化，比如实施`batching log updates`和引入`cicular log`来提高恢复速度和磁盘利用率。最后顺便阐述`metadata journaling`模式在数据块重用的情况下可能存在的一个问题。需要注意的是，除了这两种解决方案之外，也提了其它比较流行和有效的方法。总而言之，关于本文的内容，个人推荐阅读原文，并且，相关的参考文献特别是那些比较新的文献更值得阅读研究。



参考资料
[1]. McKusick M K, Joy W N, Leffler S J, et al. Fsck− The UNIX† File System Check Program[J]. Unix System Manager’s Manual-4.3 BSD Virtual VAX-11 Version, 1986.
[2]. Prabhakaran V, Bairavasundaram L N, Agrawal N, et al. IRON file systems[M]. ACM, 2005.
[3]. Ganger G R, Patt Y N. Metadata update performance in file systems[C]//Proceedings of the 1st USENIX conference on Operating Systems Design and Implementation. USENIX Association, 1994: 5.
[4]. Bonwick J, Moore B. ZFS: The last word in file systems[J]. 2007.
[5]. Chidambaram V, Pillai T S, Arpaci-Dusseau A C, et al. Optimistic crash consistency[C]//Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. ACM, 2013: 228-243.
[6]. Arpaci-Dusseau R H, Arpaci-Dusseau A C. Operating systems: Three easy pieces[M]. Arpaci-Dusseau Books LLC, 2018.
