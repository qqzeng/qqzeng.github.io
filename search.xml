<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于 Local Health 感知的故障检测器 Lifeguard]]></title>
    <url>%2F2021%2F09%2F05%2F%E5%9F%BA%E4%BA%8E-Local-Health-%E6%84%9F%E7%9F%A5%E7%9A%84%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E5%99%A8-Lifeguard%2F</url>
    <content type="text"><![CDATA[上一篇文章阐述了一种可扩展的且具有弱一致性的组成员资格协议——SWIM，它包括一个基于点对点的（peer-to-peer）的随机探测通信的故障检测器，以及一个基于 gossip 并采用消息 piggybacking 的成员更新传播两个部分。在 SWIM 的扩展部分，其引入了suspicion 子协议以降低失败探测的假正比例，即将健康成员误认为是故障成员的情况。尽管如此，节点在缓慢的消息处理的场景下，即在面临较大的 CPU 竞争或者网络延时的情况下，仍然会导致 SWIM 出现较高的假正比例。因此，hashicorp 的研究人员扩展了 SWIM，通过引入节点 local health 的概念，提出了基于 local health 感知的 Lifeguard 故障检测器（Lifeguard: Local Health Awareness for More Accurate Failure Detection）。Lifeguard 对 SWIM 的扩展增强包括三个部分：基于 local health 感知的探测、基于 local health 感知的 Suspicion，以及 Buddy System。在模拟控制和生产环境下的实验结果表明，Lifeguard 的引入将故障检测的假正比例降低了 50 倍，同时减少了故障成员被首次检测的延迟。 本文阐述基于 local health 感知的节点故障检测器 Lifeguard，它致力于对 SWIM 的故障检测存在的较高假正比例的问题进行缓解。具体地，本文首先简要介绍节点慢消息处理出现的实际场景以及由此导致的较高节点故障误判率所引发的问题，然后重点阐述Lifeguard 包含的三个部分。 慢消息处理故障检测的误判的根本原因是节点慢消息处理。具体地，节点 CPU 负载较高或者连接到节点的网络延迟较大，导致节点未能在超时时间内接收到目标探测对象的 ack 消息，这使得节点误认为目标探测节点出现故障。这在集群中多个节点同时处于慢消息处理的状态时更为严重。在生产环境下，web 服务器的请求激增会导致其来不及处理其他的网路请求，类似的，在音视频转码等 CPU 密集型的应用同样会导致网路包处理超时。显然，节点处于慢消息处理状态情形较少，且其持续时间并不会过长，但其影响也不可忽视。当节点的负载或网络延时频繁变化时，会导致健康的节点反复被标记为故障或健康，若节点状态的变化会触发集群执行 failover 操作，常见的如增加新的成员或重分配集群数据，这带来的代价显然非常昂贵。 SWIM Suspicion 子协议的不足SWIM 原论文提出的用于缓解误判的 Suspicion 子协议存在的问题是，其仍然以节点会及时处理消息作为前提。具体地，Refute 消息能够顺利起作用的一个前提是探测节点能够成功接收到这一消息，但当探测结点处于慢消息处理状态时，其根本无法处理任何消息，包括该 Refute 消息，因此，Suspicion 机制的作用也就大大降低了。 因此，误判现象的本质是节点未能及时处理消息，另外节点消息响应的缺失也表明节点处于慢消息处理状态。相应的，Lifeguard 提出的解决办法也很直接，考虑到误判是由节点的负载或网络延时导致的，因此，如果让节点实时掌握自身的负载状况，即节点本地健康感知（local health awareness），则这允许节点基于自身健康状况来调整对目标成员故障检测的超时时限，以尽可能避免将健康的节点标记为故障成员。 Lifeguard基于 local health 感知的故障检测Lifeguard 包含的第一个扩展是基于 local health 感知的故障检测，它将原本 SWIM 所使用的固定探测周期和超时时限调整为根据节点的 local health 状况来动态调整。直观来看，若节点自身健康较差，即节点处于降级状态时，节点必须对判定目标节点为故障成员的操作更加保守，因为其自身可能是不可靠的，因此其基于此得出的结论也不够有说服力。这在 Lifeguard 中是通过一个 Local Health Multiplier（LHM）计数器来实现，它反应了节点最近同其他成员进行的故障检测相关的消息交换情况，此值的大小即表示节点对判定目标节点为故障状态的保守程度。LHM 是通过一种启发式的方法获得，具体计算方法可参考原论文 section Ⅵ part A。 基于 local health 感知的 Suspicion 子协议Lifeguard 的第二个扩展是将 SWIM Suspicion 子协议更改为基于 local health 感知的 Suspicion 方案，它将原本 SWIM Suspicion 阶段的固定超时时限调整为动态变化的超时时限。直观而言，当一个节点发现目标节点可能处于故障状态，或者其从其他节点那得知目标节点可能处于故障状态，此时该节点对目标节点反馈（驳斥）此怀疑的超时时间是最长的，因为我们此时有理由认为很可能只是因为自身或其它节点处于降级状态而怀疑目标节点处于故障状态。但一旦我们发现越来越多的节点也认为目标节点处于故障状态时，我们将减少分配给目标节点的超时时限，因为此时我们有更多的理由认为目标节点可能真的已经处于故障状态了。类似地，此动态超时时限的计算方法同样可参考原论文section Ⅵ part B。 Buddy SystemLifeguard 的最后一个扩展是 Buddy System，即节点通过 ping 消息直接将 Suspect 消息发送给目标节点。SWIM 设计将成员更新消息附加到基于 gossip 的失败检测消息上，这可能导致 Suspect 消息消耗更多的时间才能扩散出去，因为基于 gossip 的消息传递需要若干轮次才能完全扩散此消息。相反，Buddy System 认为任何时候一旦我们认为目标节点可能出现故障时，我们应该将此 Suspect 消息优先级调为最高，并直接将此消息通过 ping 的方式发送给目标节点，以让目标节点尽可能及时的响应（反馈）此怀疑。 至此，关于 Lifeguard 的相关阐述已经完毕。关于 Lifeguard 的具体实验，包括假正比例、故障检测延迟以及网路负载等指标的测试结果可以直接参考原论文，另外本文只是对 Lifeguard 针对 SWIM 的三个拓展进行非正式的且通俗易懂的一个阐述，原论文 section Ⅵ part C 描述了更多的细节。 最后，对 Lifeguard 所做的三个拓展进行扼要概括：其一，基于 local health 感知的故障检测取代了 SWIM 的故障检测，其直观动机是一个降级的节点理应保守地认为目标节点为故障节点；其二，基于 local health 感知的 Suspicion，其直观动机是给予目标节点响应的超时时限应该是动态变化的，且目标节点被越多的节点怀疑，其被分配的响应超时时限也应该越小；最后，Buddy System 即提升 Suspect 消息的优先级，其背后动机是让目标节点尽可能快的响应其他节点对其的怀疑。 参考文献 Making Gossip More Robust with Lifeguard Everybody Talks: Gossip, Serf, memberlist, Raft, and SWIM in HashiCorp Consul Detecting failures and avoiding false positives (hashicorp.com) Lifeguard: Local Health Awareness for More Accurate Failure Detection]]></content>
      <categories>
        <category>分布式系统</category>
        <category>组成员资格协议</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>gossip</tag>
        <tag>组成员资格协议</tag>
        <tag>成员发现</tag>
        <tag>失败检测</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析组成员资格协议 SWIM]]></title>
    <url>%2F2021%2F09%2F05%2F%E6%B5%85%E6%9E%90%E7%BB%84%E6%88%90%E5%91%98%E8%B5%84%E6%A0%BC%E5%8D%8F%E8%AE%AE-SWIM%2F</url>
    <content type="text"><![CDATA[SWIM（Scalable Weakly-consistent Infection-style Process Group Membership Protocol）是一个点对点（peer-to-peer）的组成员资格协议（group membership protocol）。简单而言，组成员资格协议为该组中每个成员提供包含组中其他非故障成员的本地集群成员视图，该成员列表随着成员或进程的加入、退出或故障而相应动态更新。组成员资格协议被广泛应用于分布式场景，包括成员发现（member discovery）、失败检测（failure detection）、负载均衡（load balancing）以及事件广播（event broadcast）等。SWIM 采用的点对点（peer-to-peer）以及随机探测（randomized probe-based）通信的设计使得其相比与传统的基于心跳（heartbeating）的成员协议，具有更好的可扩展性和鲁棒性，即能更好地容忍网络和节点故障，而且也更容易实现、部署和维护。值得一提的是，考虑到它使用的是基于 gossip 的成员更新传播，因此它也是弱一致性或者最终一致性的，即同一时刻，不同成员的本地集群成员视图可能存在差异，但在有限的时间内会趋于一致。 本文内容包含三个部分，首先简单介绍组成员资格协协议包含的失败检测的几个关键特征或指标，以及 SWIM 的特点或者优势，然后阐述 SWIM 包含的两个基本组件——失败检测和成员更新传播，最后阐述原论文中描述的 SWIM 的扩展内容或增强特性。 为了更好的理解失败检测协议的效率和可扩展性，从理论和实践角度，其需要保证或考虑如下几个关键属性： 强完整性（strong completeness）。组中任意成员的故障需要被所有非故障成员检测到； 失败检测速度（speed of failure detection）。组中某成员发生故障到组中某非故障成员发现此故障的延迟尽可能短； 准确性（accuracy）。失败检测的假正比例（false positives）尽可能小。即将健康节点误认为是故障节点的概率要小； 网络消息负载（network message load）。协议每秒的网络传输开销应尽可能低。 在实际环境下，有一些属性是不能被同时完全满足的，比如强完整性和准确性。但强完整性必须要保证，因此往往会牺牲一定的失败检测准确性。另外，准确性的提升也一定程度上会增加网络消息负载。因此，实际分布式应用需要对其所需的安全性和活性进行 trade-off。 SWIM 包含一个基于随机探测的失败检测过程其和基于 gossip 的更新传播过程。相比于其他成员资格协议其具备如下几个特点： 高可扩展性（scalability）。随着组中成员数的增加，其会保证上述属性的稳定，这包括任意成员故障被首次检测的平均时间、准确性（即假正比率）以及网络消息开销。且任意成员失败事件的传播速度只是随组成员数增加而对数减缓； 强鲁棒性（robustness）。其可容忍组中任意范围的成员故障和网络故障，可达到最高的去中心化水平和网络故障容忍能力，这在复杂的分布式环境至关重要； 易于部署和维护（ease of deployment and maintenance）。其完全去中心化的架构设计使得新成员的加入非常简单，只需知道并联系任意组成员加入即可，且成员的离开也不会对整个系统有关键影响； 易于实现（simplicity of implementation）。考虑到 SWIM 的 peer-to-peer 的设计，其不需要由某一中心节点或协调节点来维护关键状态或者执行特殊操作，因此其包含较少的状态和消息种类数。 值得注意的是，上述特征都是基于 SWIM 的设计来保证的。 SWIMSWIM 包含的两个组件——失败检测（failure detection）和更新传播（update dissemination）。其中，失败检测即对组（集群）中故障成员的检测。SWIM 包含的更新传播即为传播导致集群成员的变化的事件，诸如节点的加入、离开或故障。 失败检测SWIM 的成员故障检测是一个完全去中心化的周期性过程，一个周期被称为是协议周期（protocol peroid）。在每个周期内，每个组成员 $M_i$ 从其本地的组成员列表中随机选择一个组成员 $M_j$ 发送 ping 消息以执行直接探测（direct probe），如果在超时时间内，$M_i$ 仍未收到目标探测成员 $M_j$ 返回的 ack 消息，其会紧接着会执行一个间接探测（indirect probe）过程，它首先从其本地成员列表中随机选择 m 个成员，然后向他们发送 ping-req 消息，收到 ping-req 的组成员会向目标探测成员 $M_j$ 发送 ping 消息，若这 m 个组成员任意一个收到了目标探测成员 $M_j$ 响应的 ack 消息，则其会将此 ack 消息转发给 $M_i$。若 $M_i$ 在协议超时时间内未收到 $M_j$ 回应他的 ack 消息，也没有收到其他成员（间接探测成员）转发给他的 ack 消息，则 $M_i$ 将在本地将 $M_j$ 标记为失败状态。 （超时时间选择、协议周期选择、ping&amp;ping-req&amp;ack消息大小以及为什么不是$M_i$直接发送m个ping消息给$M_j$） 更新传播当 $M_i$ 检测到 $M_j$ 成员出现故障后，其通过多播（multicast）的形式将此消息发送给组其他成员。当组其他成员收到此消息后，其同样会将 $M_j$ 标记为失败，并从本地成员列表中删除。对于一个新节点加入组的事件（离开组也是类似），原论文提供了多种方法，包括直接加入某个公开的多播地址（即组关联的多播地址），或者广播新节点加入消息，又或者选择一个或多个协调者，然后由此协调者发送节点加入消息。但实际上这些方法都不够恰当，比如，若使用 IP 或者硬件 multicast 的方式，考虑到 multicast 的best-effort 性质，可能会出现消息丢失的情况而导致组成员未收到此消息。消息广播的代价也较大。下文阐述的 SWIM 的扩展包含了对此问题的解决方案。 SWIM 扩展上文阐述的 SWIM 只是一个包含失败检测和更新传播的基本模型，且不能适用于实际应用，因此原论文在扩展部分针对性的进行改进优化。 基于 Infection-Style 的更新传播模型首先，针对 SWIM 执行的更新传播过程不能很好处理节点加入或离开问题。原论文扩展了 SWIM，在这种情况下，其采用基于 gossip 的更新传播模型，即将成员更新的消息附加（piggyback）到失败检测的消息（ping、ping-req 以及 ack）中，如此一来，就省去了针对这一类型的更新事件而难以选择发送目标成员的问题，也避免产生额外的消息（但无疑使得失败检测消息负载更大）。另外，考虑到每一个成员更新消息会被 $\lambda\log_2n$ 个成员转发，其中n为组大小，而 $\lambda$ 为一个调优参数，另外，每个成员会记录每个更新消息被发送的次数，且对于存在多个成员更新消息时，优先选择发送次数较少的更新。 引入 Suspicion 机制以降低假正率其次，SWIM 规定一旦在协议周期时间内成员 $M_i$ 未收到目标探测成员 $M_j$ 的 ack 消息，则将 $M_j$ 标记为失败。该举措往往会导致较高的假正率（较低的准确率），即误将健康的目标探测节点标记为失败。因为考虑到消息丢失或者目标探测节点处于睡眠状态，甚至 $M_j$ 可能因为负载太高等等，都可能导致 $M_i$ 不能收到，或者不能及时处理 $M_j$ 响应的 ack 消息，因此错误的将目标探测成员标记为失败。 针对这种情况，SWIM 提出了 suspicion 子协议。其工作过程如下：在 $M_i$ 没有直接或间接收到 $M_j$ 的ack消息后，其并不直接将其标记为失败。相反地，$M_i$ 在本地成列表中将 $M_j$ 标记为一个怀疑（suspected）成员，此外，$M_i$ 会将 suspect 消息 {Suspect $M_j$: $M_i$ suspects $M_j$} 并将其通过更新传播模型发送出去。组中任何成员在收到此 suspect 消息后，同样会将 $M_j$ 标记为怀疑对象，但怀疑对象仍然处于成员的本地成员列表中，仍然作为非失败的成员对待，因此其仍然是 ping 消息的接收对象。但假设 $M_j$ 实际上并未发生故障，即某一成员 $M_k$ 在超时时间内成功 ping 通了一个被怀疑的成员 $M_j$，则其首先取消对 $M_j$ 的怀疑标记，然后同样通过更新传播模型发送一个 alive 消息 {Alive $M_j$: $M_k$ knows $M_j$ is alive}，收到此 alive 消息的成员同样会取消 $M_j$ 的怀疑标记，值得注意的是，在收到 suspect 消息时，$M_j$ 自身也可发送 alive 消息。另一种假设是 $M_j$ 确实出现了故障，因此 $M_i$ 在协议超时时间内收不到 alive 消息，则其会进一步将 $M_j$ 标记为失败，并将其从本地成员列表中删除，同时发送一个 confirm 消息 {Confirm $M_j$: $M_i$ declares $M_j$ as faulty}。该 confirm 消息能够覆盖之前的 suspect 和 alive 消息，同时导致收到此消息的成员也将 $M_j$ 从成员列表中删除。 另外，SWIM 为每个成员保存一个全局的计数器 incarnation，以区分不同轮数的 suspect/alive/confirm 消息，因为每个成员可能会经历多次被标记为怀疑对象。incarnation 的值同 suspect/alive/confirm 的优先级相关，具体可参考原论文4.2节。 基于 Round-Robin 选择目标探测成员SWIM 最后一个改进的地方是让每个成员从其本地成员列表中以 round-robin 的方式选择失败检测的目标成员，而不采用完全随机的选择策略。因为在最差的情况下，完全随机的选择策略无法预估故障成员首次被探测成功的最大延迟（因为理论上存在集群成员可能永远不会被选择作为探测目标）。相反，通过使用 round-robin 的选择策略，在保证每个成员的本地成员列表的随机性的基础上，最大的探测延迟是存在上界的，另外新成员的列表插入位置也是随机的，每次选择完一轮所有的本地成员后，会将本地成员列表中的元素位置打乱。 简单小结，SWIM 是一个可扩展的且具有弱一致性的组成员资格协议。其可扩展性是相比于传统的基于 heartbeating 的协议而言，基于 heartbeating 的协议在成员规模变大时容易造成单点的 CPU 和网络瓶颈，而且中心节点的特殊性使得其鲁棒性降低，协议本身也难以实现和维护。相反，SWIM 通过将协议划分为失败检测和成员更新传播两个部分来解决这个问题。其中，SWIM 包含的失败检测通过使用 peer-to-peer 的随机探测通信方式来提升协议的可扩展性，其可保证故障成员被首次探测的时间以及组中每个成员的消息负载都独立于组成员数量。另外，SWIM 的成员更新传播基于一种基于 gossip 的通信方式，同时将成员更新消息附加到失败探测的消息上，以提升协议的有效性和可靠性。最后，通过引入 suspicion 子协议，牺牲部分失败检测时间，以降低失败检测的假正率，而通过整合随机选择和 round-robin 策略以保证本地成员列表中任意成员的故障探测延迟都是有限的。 关于 gossipgossip 协议应用广泛，典型地，gossip 协议可用于将数据分发到网络中的每一个节点。根据不同的具体应用场景，网络中两个节点之间存在三种通信方式：推送模式、拉取模式、Push/Pull。 Push: 节点 A 将数据 (key,value,version) 及对应的版本号推送给 B 节点，B 节点更新 A 中比自己新的数据； Pull：节点 A 仅将数据 (key, version) 推送给 B，B 将本地比 A 新的数据（key, value, version）推送给 A，A 更新本地； Push/Pull：与 Pull 类似，只是多了一步，A 再将本地比 B 新的数据推送给 B，B 则更新本地。 如果把两个节点数据同步一次定义为一个周期，则在一个周期内，Push 需通信 1 次，Pull 需 2 次，Push/Pull 则需 3 次。虽然消息数增加了，但从效果上来讲，Push/Pull 最好，理论上一个周期内可以使两个节点完全一致。直观上，Push/Pull 的收敛速度也是最快的。]]></content>
      <categories>
        <category>分布式系统</category>
        <category>组成员资格协议</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>gossip</tag>
        <tag>组成员资格协议</tag>
        <tag>成员发现</tag>
        <tag>失败检测</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Serf 原理剖析]]></title>
    <url>%2F2020%2F11%2F01%2Fserf%2F</url>
    <content type="text"><![CDATA[上一篇文章简要剖析了基于 SWIM+Lifeguard 的组成员协议 memberlist 协议库的基本原理，另外，也阐明了如何关联 memberlist 和上层应用，即 memberlist 暴露一些 Delegate 接口以让上层应用能够 hook 到协议层内部，以为上层应用提供协议用途之外的能力。而 Serf 基于 memberlist 为分布式应用提供诸如集群成员管理、集成成员故障检测以及自定义事件传播等功能，显然，Serf 是一个完全去中心化的、具备强大故障容忍能力的工具。本文简要阐述 Serf 原理，以了解如何基于 memberlist 构建能解决分布式场景下典型问题的工具。 本文非详细的源码解读教程，只会介绍 Serf 的基本特性以及组成 Serf 的关键组件。因此，若读者有兴趣，可在参阅本文之后详细阅读源码。另外，建议读者在阅读本文前，先了解上一（几）篇文章的内容，这有助于读者理解源码，可参考【这里】和【这里】。最后，本文的分析基于版本。 具体地，本文首先阐述 Serf 的典型实践场景以及提供的基本能力，即了解 Serf 能够解决分布式系统中的哪些问题，而这些能力是通过 hook 到 memberlist 协议库底层来实现的。接下来，重点剖析 Serf 所包含的关键组件，关键组件即代表着 Serf 的基本原理，也代表着对外提供能力的基础构建块，这同样有助于把握阅读源码时的切入点和脉络。最后，简单阐述 Serf 如何利用底层协议库 memberlist 为其提供的 hook 接口。 serf 的应用场景和基本能力Serf 基于组成员协议库 memberlist 并将 memberlist 拥有的能力进行“变现”和输出。上文提到，Serf 能够应用于集群成员管理和集成成员故障检测等场景，这些都是基于底层的 memberlist 协议库实现的。另外，Serf 还支持自定义事件和查询的广播。 具体地，关于集成成员管理，Serf 可以为我们维护集群成员列表，同时，在集成成员动态发生变化时（如节点加入、离开或者失败等），能够执行预先自定义的脚本。这拥有很多实际应用，比如 Serf 实例可以我们负载均衡器维护后端的一系列业务服务器，以使得在业务服务器集群在动态扩张和收缩时（即节点上线和下线），能够即时通知负载均衡器触发相应动作。类似地，它也可以为后端业务服务器动态维护一个底层的存储实例列表。 另外，关于集群成员故障检测，Serf 可自动且即时（秒级）地发现集群中的故障节点，并且通知集群中其它存活的成员，同时，还可以自动触发预先配置的对应事件关联的 handler。且默认情况下，Serf 会自动周期性的重连失败的节点，以使得集群不会因为节点的短暂故障下线而处于非预期状态。 其次，Serf 可以为我们传播自定义（用户）事件（Event），即 Serf 可以将用户自定义的事件广播到集群（显然集群本身的事件，如节点加入等，也可以注册相应的 handler），因此，理论上所有成员都可以收到此事件消息，因为我们可以为事件注册对应的 handler，这可以让节点在收到这些事件后触发动作。此外，我们也可以设置 filter，以指定事件的目标发送节点集合。这同样拥有很多实际应用，比如我们可以集群就绪后通过 Serf 向集群中广播服务部署事件事件以触发节点上的服务部署进程。类似地，我们可以定配置变更的事件，以让节点重新加载配置。值得注意的是，用户自定义事件是一次性的单向广播（fire-and-forget），换言之，收到事件消息的节点只需触发关联的 handler 即可，无需响应或返回执行结果。 最后，Serf 还允许我们将自定义查询（Query）广播到集群，查询是基于事件的，换言之，其也是一种用户自定义事件，但其提供了一种简单的实时请求-响应机制，即收到查询消息的节点可以简单应答也可以执行对应的 handler，并将执行结果返回，因此，Query 能够提供更强的能力。基于此，我们可以实时地收集运行中的集群的指标信息，如通过自定义查询以定期收集节点的负载信息。但值得注意的是，查询消息可能不会被所有节点收到并处理，考虑到自定义查询侧重于请求的实时性，因此在广播自查询时，若有节点正处于故障状态，或者处于不同的网络分区，此节点将不会收到此查询消息，且后续 Serf 也不会重新广播此查询消息，这不同于自定义事件的广播。 serf 的关键组件本节通过阐述 Serf 的几个关键组件来剖析 Serf 的基本原理。如下图所示，Serf 允许我们执行特定的命令来操作 Serf 集群。比如，我们可通过执行serf agent以默认配置启动一个或多个 Serf 节点，此时，即存在一个或多个包含单节点的集群，然后可以执行serf join node-addr来将隔离的集群进行合并，当然，也叫可在启动 agent 的同时加入已存在的集群，通过serf agent -join=node-addr来实现。上层的命令程序将通过 RPCClient 发送相应的 rpc 请求到 agent IPC，agent IPC 在收到 rpc 请求后，充当一个请求路由器 handler 的功能，随即将请求转发给 serf agent 处理，serf agent 进一步将请求中继给 serf 实例处理，serf 实例是核心枢纽，它会底层的 memberlist 协议库以及各个关键组件（如Query、EventHandler、Snapshotter、Coalescence等）对请求进行处理。最后，若请求需要响应返回，则通过 udp 发送请求执行结果。 serf agent 启动逻辑基于上述 serf 基本执行流程，简单介绍 serf agent 启动逻辑：当用户执行serf agent命令时，它首先配置 Serf agent，比如配置 agent 之间的通信地址、初始化底层依赖的 memberlist 配置以及其它配置，随后创建 agent 实例。接下来，它开始启动 agent，在此过程中，其首先注册自定义事件 handler（若有设置），然后正式启动 agent，在启动 agent 的过程中，创建 Serf 实例，同时启动事件监听和处理循环，并在设置 rpc listener 后启动 agent IPC 用以响应和处理用户执行的命令。最后，它开始加入指定的 agent 节点（若有配置），然后等待退出信号。这即为一个 Serf agent 的启动逻辑。当然，在创建 Serf 实例的过程中，还做了其它一系列操作，比如创建 Query，并启动 Query 处理循环，创建 Coordinate 系统、加载 Snapshot 并恢复、配置事件和查询的广播队列、创建 memberlist 实例，最后启动一系列异步 goroutine，用于回收故障节点、周期性重连故障节点等。 在梳理完 serf agent 启动逻辑后，下面简要介绍 serf 依托的各个组件的基本原理： lamport 时钟lamport 时钟用于规定分布式系统中事件发生的先后顺序。考虑到机器的物理时钟因各种原因（如时钟漂移）而并不准确，因此，Leslie Lamport 提出了逻辑时钟的概念，其大致是基于事件发生的因果关系来定义分布式系统中事件的发生顺序，它虽然不能保证此顺序即为实际以物理时钟为依据的排序结果，但它保证能够正确排列系统中具有因果关系的事件，这使得分布式系统在逻辑上不会颠倒具有因果关系的事件的发生顺序，这在绝大部分情况下是足够的。 coordinate 系统coordinate 系统是基于 vivaldi 算法，其通过启发式学习算法，不断计算节点通信时的 RTT 来动态学习系统网络拓扑的算法。在分布式系统中，节点若能知道整个系统中节点分布，即网络的拓扑结构，那么其可作为节点之间进行通信的依据，即节点可选择离其拓扑距离最近的 k 个节点进行广播通信，这样可以尽可能减少网络中充斥着大量的消息，即可以避免网络风暴，而且，通过调整 k 的大小，可以个性消息的传播速度、效率以及对网络带宽的影响。 coalescence 机制coalescence 机制指的是 serf 可将指定时间窗口内的连续若干个 event 合并成一个 event 进行广播发送的机制。比如，当集群中有 5 个成员（几乎）同时加入时，在满足要求的前提下，coalescence 机制会对这 5 条 event 进行合并，因此，只会压入一条 event 到 event channel，避免了集群因这些事件的影响而出现的潜在的 flapping 行为。有两个参数同 coalescence 机制相关，一个是决定执行合并操作的周期，另一个参数指定了允许持续合并的 event 之间的最大时间间隔，即在超过此时间间隔后，即使未达到合并操作周期，也必须执行合并操作，将 event 压入到 event channel。 snapshotter 机制serf 提供的 snapshotter 机制类似于日志的快照机制，snapshotter 机制允许在 serf 失败重启时，通过加载保存有一系列事务数据的快照文件以帮助其快速恢复到宕机前的状态。具体地，agent 收到消息时，snapshotter 机制将成员事件以及对应的 lamport 时钟值写入文件，并定期对文件执行 checkpoint 和 roll over 的操作。因此，在 agent 重启时，通过 replay 快照文件所包含的所有成员事件以获取 agent 需要加入的节点的列表，同时，lamport 时钟值还可以避免 replay 过期的事件。 event 处理器当执行命令serf event name [payload]，它首先构建对应类型的 rpc 请求头和请求参数，然后基于 RPCClient 发送此 rpc 请求，请求会被 agent IPC 接收，然后转交给 agent 处理，agent 进一步转交给 serf 实例处理，serf 实例首先对 event 消息进行简单检查，检查项包括消息大小是否超过限制等，然后更新 serf 实例本地的 lamport 时钟，接着开始处理 event 消息，消息处理逻辑比较简单，在判断消息是否过期（serf 会维护最近收到的 event 缓冲队列，因此，若收到的消息比缓冲中最旧的消息更旧，则证明其是过期的消息），同时判断消息是否已被处理（即判断此消息是否存在于缓冲队列中），在所有检查通过后，其将消息压入 event channel，这使得 event 可以被 event handler、query 以及 snapshotter 组件接收并进行针对性处理。最后，将此 event 消息加入到 event 关联的广播消息队列中，此广播消息队列中的消息会附加到 memberlist 协议库的 gossip 消息中广播出去。 query 处理器可以通过命令serf query load来向 serf 中发送 query 消息。query 消息的处理过程同 event 处理过程类似。其区别在于，query 消息还可制定响应策略，这包括设置目标节点是否应该返回 ack，以及目标节点是否允许再一次此 query 消息。此外，agent IPC 会为所有的目标接收节点设置一个响应超时时限，在定时器超时后，直接返回，不会再等待节点返回 ack 或执行结果。 密钥管理器密钥处理器（Key Manager）用于管理 serf 集群中的加密密钥（encryption keyring）。 serf 如何利用协议库提供的 hook 接口最后，我们阐述 serf 和底层协议库是如何交互的，即 serf 如何利用底层协议库的 hook 接口。首先，serf 实现了 memberlist 提供的 Delegate 接口。其中， NodeMeta 接口使得 serf 实例将其 tags 作为节点的元数据通过广播发送到各个节点； NotifyMsg 接口使得 serf 借助底层协议库传播自自定义的一系列 userMsg，因此，此接口即可作为 userMsg 的 dispatcher，根据 userMsg 具体类型来调用 serf 实例提供对应的 handler。换言之，上一篇文章所说的 memberlist 在收到 userMsg 时会回调上层应用 serf 的逻辑，即将此 userMsg 交由 serf 处理； GetBroadcasts 接口使得 serf 实例借助底层协议库基于 gossip 的消息广播机制来广播自己需要广播的消息，即将 serf 层需要广播的消息附加到底层 memberlist 的 gossip 消息中来达到发送到各节点的目的；其中，serf 层包含了诸如事件广播队列 eventBroadcasts、查询广播队列 queryBroadcasts，以及一个通用广播队列； LocalState 接口使得 serf 实例借助底层协议库定期执行的 push-pull-merge 操作来通过 messagePushPull 消息来交换 serf 实例层各节点的本地状态。如实例最后一个处理消息的时间、实例本地的已 left 成员的视图列表等。定期执行这些信息交互，可使得 serf 实例集群状态快速收敛； 最后，MergeRemoteState 接口使得 serf 实例可以处理其它节点发送给它的 messagePushPull 消息，然后，针对消息中的内容，以将其合并到（借助其调整）实例本地存储的状态。如针对源节点发送的已 left 成员视图列表，本节点会当作收到对应 left 成员的 leave 消息。StatusLTimes 另外，eventDelegate 各接口的实现，即调用 serf 实例的关联的接口。pingDelegate 也是类似，将 PingVersion 作为 pingMsg 的 payLoad，并将节点的 coordinate 信息编码到 pingMsg 中。 Query 实时性、通过tag过滤目标发送对象、定制返回策略（是否返回Ack以及是否应该继续广播） 传送方式（通过 GetBroadcasts 接口附带出去，event 也是类似）]]></content>
      <categories>
        <category>分布式系统</category>
        <category>组成员协议</category>
      </categories>
      <tags>
        <tag>gossip</tag>
        <tag>集群成员管理</tag>
        <tag>故障检测</tag>
        <tag>组成员协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[memberlist 原理剖析]]></title>
    <url>%2F2020%2F10%2F24%2Fmemberlist-%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[在此之前，我们介绍了组成员资格协议 WSIM，其主要包含失败检测和基于 Gossip 的传播更新两个部分。随后，阐述了 Hashicorp 研究人员在 WSIM 的基础之上引入 local health 概念而提出 Lifeguard，致力于降低节点负载较高导致的高误判率，即将健康成员声明为故障成员，Lifeguard 从三个方面对 WSIM 进行拓展改善。前两篇文章侧重于以论文内容为基础，并从协议设计的（理论）角度进行阐述，以了解协议设计思想以及基本运行流程。而本文则深入到源码，并从协议架构以及实现优化的（实践）角度来分析，以了解其关键实现细节以及工程落地的权衡优化。memberlist 是 Hashicorp 在以 Lifeguard 增强 WSIM 后开源实现的组成员资格协议库，被用于 Consul 和 Nomad 开源项目。 本文并非详细的源码解读教程，只会阐述关键的数据结构和协议运行流程。因此，若读者有兴趣，可在参阅本文之后详细阅读源码。另外，建议读者在阅读本文前，先了解协议的基本理论和设计规范，这有助于读者理解源码，可参考【这里】和【这里】。最后，本文的分析基于版本。 正如前文提到的，本文侧重于从协议实现的角度阐述。具体地，本文首先阐述协议框架的总体执行流程，这有助于把握阅读源码时的方向和切入点。其次，理清协议包含的消息的关键处理逻辑。然后，针对性指出 Lifeguard 优化如何落地。最后，简单阐述如何建立协议库 memberlist 和上层应用系统（如 Serf）之间的关联。 协议库模块划分memberlist 协议库源码结构比较清晰，代码量也不大，协议本身也基本无第三方库依赖。关键组件或模块归纳如下： memberlist.go 为协议库的入口文件，定义了 Memberlist 核心结构，其包含集群中的成员需要维护的运行时结构状态，更侧重于辅助于协议实现的字段结构。相应地，其也包含了几个供上层应用调用接入协议层的接口，如启动协议库、关停协议库，以及获取集群成员。另外，也包含导致集群成员变更的操作，如新节点加入集群、成员离开集群。但其并非唯一的同上层应用交互的入口文件，具体在第四小节阐述； config.go 为协议库的核心配置文件，定义 Config 核心结构，不同于 Memberlist，其侧重于维护协议相关的状态或配置，以及上层应用 hook 到协议库的入口，另外包含一些网络配置相关的 help 方法； net.go 为协议库消息中间层处理的文件，定义了大部分消息的结构，如 ping、indirectPing、alive、dead 以及 suspect 消息等，相应地，其也包含对应消息的 handler，其消息处理的逻辑侧重于消息的编解码、消息的合成和分解、消息的压缩和解压缩以及消息处理的 handler 分发，总之，其类似于消息处理的一个中间层，处理逻辑较少或不涉及消息或协议相关的状态； state.go 同样为协议库消息处理的核心文件，其主要包含各类消息的 handler 的核心逻辑，即涉及到协议相关状态变更的处理逻辑，如 ping、indirectPing、alive、dead 、suspect 以及 refute 消息的 handler。另外，也包括故障检测 probe 操作、周期性成员全量状态同步 push/pull 的操作以及周期性的传播集群状态更新的 gossip 操作； transport.go 和 net_transport.go 为协议库的消息传输组件，定义了协议消息传输的核心模型 Transport 接口，并提供了默认的实现 NetTransport 结构。其主要包含协议网络消息处理和传输的操作，比如针对网络连接（或网络包）的创建、监听、读取、发送以及销毁等逻辑； broadcast.go 和 queue.go 两个文件用于处理集群消息广播操作，其核心是基于 b 树结构对集群需要广播消息的存储、检索以及移除操作，其中对 b 树存储消息的操作的一个核心依据是消息被转发的次数； suspiction.go 用于管理 suspect 消息，比如定义成员等待可疑成员响应的超时时限的计算方法以及可疑成员被标记为失败需要得到多个成员的确认的操作逻辑等； awareness.go 用于管理由 Lifeguard 引入的成员 local health 指标，以动态调整成员标记其它成员为失败的操作的可信度； *_delegate.go 包含允许上层应用针对协议状态变化而针对性制定的逻辑的 hook 文件，比如，当集群中出现冲突的成员时，或者当集群成员之间的状态发生合并操作时，上层应用的针对性执行的逻辑。详细内容在第四小节阐述。 协议库基本执行框架memberlist 组成员协议库基本执行框架同协议运行流程类似。上层应用程序（如 Serf）通过提供协议配置并调用 memberlist.go 提供的方法来创建并启动协议库（Create），这首先初始化 Memberlist 结构，然后创建网络监听和对应类型的消息的 handler。具体地，首先创建 tcp 连接处理监听并启动对应的 tcp 消息的处理循环，在 tcp 消息的 handler 中，依次从连接中读取消息，然后根据消息类型（userMsg、pushPullMsg 及 pingMsg）针对性执行对应的消息处理逻辑。其次，再创建 udp 网络包处理监听并启动对应的 udp 消息处理循环，在 udp 消息 handler 中，同样根据消息类型（compundMsg、compressMsg、pingMsg、indirectMsg、ackRespMsg、nackRespMsg、suspectMsg、aliveMsg、deadMsg 以及userMsg）进行针对性处理。 接下来，当前节点将其自身标记为 alive 状态，并向集群成员广播此 alive 消息。 最后，执行周期性的消息发送和接收操作。具体地，首先，执行一个周期性的故障检测操作，即从本地成员列表中随机选择一个成员并根据成员的状态发送探测消息，若认为对方处于 alive 状态，则发送一个 ping 消息，否则，发送一个 ping 消息和 suspect 消息，并根据响应结果进行针对性处理。其次，执行一个周期性的 pull -&gt; push -&gt; merge 消息操作，即随机选择一个集群成员，进行全量的节点状态的同步操作。最后，周期性的给集群中随机若干个成员并向它们发送广播消息（从排队缓存的需要被广播到集群中的消息集合中按顺序取出）。 协议消息处理memberlist 组成员协议库包含的消息种类较多，包括 tcp 和 udp 类型的消息。有些消息的处理逻辑比较直观，有些消息处理逻辑较为复杂，下面先简要介绍集群成员在收到不同消息的后的处理逻辑，然后，介绍集群成员周期性执行的三个操作。值得注意的是，消息允许被压缩和加密。 tcp 消息处理逻辑首先阐述通过 tcp 来交互的三种消息处理逻辑。需要注意的是，这些消息都是直接通过连接响应对端，并不会以 gossip 的方式广播到集群，也不会执行 piggyback 操作。 userMsg当集群成员通过 tcp 连接收到 userMsg 时，其处理逻辑很简单，在解码消息后，直接回调上层应用的 hook 方法。显然，这是一种单向消息，即 userMsg 类型的消息的处理过程并不属于协议范围，相反，它是属于上层应用程序借助协议库实现的成员之间的单向传输消息。 pushPullMsg当集群成员通过 tcp 连接收到 pushPullMsg 时，其会执行一个标准的 pull -&gt; push -&gt; merge 消息以同集群中一个随机选中的成员进行节点状态全量交换同步。具体地，其首先从连接中读取远程节点状态，所谓节点状态，即该节点本地的集群成员视图。然后，将本地状态发送到对端，同时也可发送上层应用程序委托的附属数据，最后，执行远程节点状态和本地节点状态的合并操作，在合并过程中，一一同步处理远程节点视图中包含的集群成员的状态，即对于 alive、left、dead 以及 suspect 状态的节点，节点会当作自身收到对应类型的消息，并执行对应消息类型的 handler，具体在后面阐述。类似地，在合并操作的同时，同样会调用上层应用的针对集群成员状态出现 merge 操作所对应的 hook 接口。 pingMsg当集群成员通过 tcp 连接收到 ping 时，其直接返回一个 ack 消息。 udp 消息处理逻辑其次，阐述通过 udp 来交互的十种消息的处理逻辑。 compoundMsgcompoundMsg 是一种复合消息类型，所谓复合类型消息，即其容纳有其它消息或消息集合。引入 compoundMsg 的目的是充分利用消息传输带宽，同时加速集群成员状态的收敛。当集群成员通过 tcp 连接收到 pushPullMsg 时，它首先解析出其中包含的单个基础消息，然后，再执行消息处理 handler 的逻辑。 compressMsgcompressMsg 同样是一种复合消息类型，其作用同 compoundMsg 类似。且处理过程也类似，即首先解压缩出其中包含基础消息，然后回到消息处理 handler 的逻辑。 pingMsg当集群成员通过 udp 连接收到 pingMsg 时，其直接返回一个 ack 消息。但需要注意的是，此 ack 消息不会直接通过 udp 发送给对方，这同通过 tcp 连接收到的 pingMsg 不同，它会执行一个额外操作：构建一个 compoundMsg，并从排队缓存的广播消息集合中取出若干个消息，以尽可能使得此 compoundMsg 接近 udp 消息的额外网络包大小，最后才将消息发送给对端。 indirectPingMsg当集群成员需要依靠其它集群成员对目标成员的状态的进行判定时，其会向若干个随机的集群成员发送 indirectPingMsg，根据它们的检测结果来间接判断目标成员的状态。具体地，当集群成员通过 udp 连接收到 indirectPingMsg 时，它会向消息中指定的目标成员发送一个 pingMsg，当目标成员在规定的超时时限内返回了 ackMsg，则其同样发送一个 ackMsg 给 indirectPingMsg 消息的发送者，否则，发送一个 nackMsg。同样，此响应的 ackMsg 或 nackMsg 也会执行 piggyback 操作。 ackRespMsg当集群成员通过 udp 连接收到 ackRespMsg 时，它会执行此消息对应的 ack handler，然后将此 handler 移除。 nackRespMsg类似的，当集群成员通过 udp 连接收到 nackRespMsg 时，它会执行此消息对应的 nack handler，然后将此 handler 移除。 aliveMsgaliveMsg 的发送时机包含三种情况：当一个节点通过协议库加入集群时，会向集群中广播 aliveMsg，另外，当一个节点收到针对自身的 suspect 消息时，其会广播 aliveMsg，最后，当集群中成员进行全量状态交换同步时，针对同步节点保存的 alive 状态节点，节点会当作自身收到了 alilveMsg。 当集群成员收到 aliveMsg 时，它首先回调上层应用的 Alive hook，然后，判断 aliveMsg 中包含的节点是否已经存在于本地，若不存在，则将其随机插入到自身的集群成员视图列表中，否则，判断 aliveMsg 中节点元信息（如 ip 地址）是否和本地保存的对应节点的元信息冲突，若产生冲突，则需进一步判断节点是否允许更新其元信息，显然，只有节点处于 left 或者 dead 状态，且协议被配置为允许节点回收，节点才会更新 aliveMsg 中包含的节点在本地存储的元信息和状态，否则，回调上层应用的 Conflict hook，然后直接中止后续处理流程。接下来，节点清除对应节点的 suspicion 定时器（因为收到了 aliveMsg，明确节点是存活的）。同时，若发现此 aliveMsg 正是针对节点自身，且并不是节点自身在启动加入集群时发出的，则节点可能需要执行 refute 操作，即向集群中广播 aliveMsg，以公告自己仍然处于存活的状态。相反，若发现此 aliveMsg 同自身无关，或者即使此消息同自身相关，但也并非在节点启动加入集群时发出的，此时直接将此 aliveMsg 广播到集群中。这两种情况的最终处理结果虽然都是向集群中广播 aliveMsg，其区别在于，在前一种情况下，节点可能会更新自身的 incarnation 计数器，同时，考虑到节点驳斥了其它成员的怀疑，因此需要显式地增加节点自身的 local health 值。 先创建 tcp 连接处理监听并启动对应的 tcp 消息的处理循环，在 tcp 消息 handler 中，依次从连接中读取消息，然后根据消息类型（userMsg/pushPullMsg/pingMsg）针对性处理。 若是 userMsg，则直接读取，然后回调上层应用程序的处理逻辑； 若是 pushPullMsg 类型，则执行一个标准的 pull -&gt; push -&gt; merge 消息 操作，即随机选择一个集群成员，进行全量的节点状态的同步操作； 最后若是 pingMsg 类型，则对应响应一个 ack 消息。 创建 udp 网络包处理监听并启动对应的 udp 消息处理循环，在 udp 消息 handler 中，同样根据消息的类型（compundMsg/compressMsg/pingMsg/indirectMsg/ackRespMsg/nackRespMsg）进行针对性处理。 若为 compundMsg 则解析此复合消息，并循环取出其中携带的基本消息，然后再进入消息处理 handler； 若为 compressMsg 则解密消息取出其加密的基本消息，同样再进入消息处理 handler； 若为 pingMsg，则回复一个 ack 消息 Lifeguard 的优化关联协议库和上层应用至此，本文相关内容已经阐述完毕，本文的重点在于通过解析函数和方法（值类型和指针类型接收者）调用的汇编过程，以阐述值类型接收者和指针类型接收者的方法被调用时的具体原理，以及方法调用和函数调用的关系。 本文参考代码在这里。 参考文献[1]. https://go101.org/article/method.html[2]. function-and-method-calls]]></content>
      <categories>
        <category>分布式系统</category>
        <category>组成员协议</category>
      </categories>
      <tags>
        <tag>memberlist</tag>
        <tag>failure detection</tag>
        <tag>membership</tag>
        <tag>gossip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go 函数和方法运行时调用解析]]></title>
    <url>%2F2020%2F06%2F04%2Fgo-%E5%87%BD%E6%95%B0%E5%92%8C%E6%96%B9%E6%B3%95%E8%BF%90%E8%A1%8C%E6%97%B6%E8%B0%83%E7%94%A8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Go 中函数是一等公民，即可以把函数当作值来使用，Go 中的函数灵活且强大。而除了可以使用函数之外，Go 也支持面向对象语言所支持的方法的概念，Go 允许我们为任何类型定义方法。函数和方法都表示对过程的抽象。从概念上看，函数接受一些参数作为输入然后处理之后产生一些输出。且对于相同的输入，函数总是产生相同的输出。换言之，函数不依赖于状态。而 Go 中的方法则类似于附加到特定类型上的函数，它定义类型的行为，并且一般需要使用该类型的状态。但若从实现上来看，二者没有本质区别，方法只是一个带有额外接收者参数的函数。本文从底层来探究函数和方法在运行时如何被调用。 本文并不详细介绍 Go 中的函数和方法的基本用法（可以参考这里和这里），及它们的基本特性。本文通过一些示例程序，以及它们的运行时调用原理以简单对比函数和方法的调用。这需要你了解一些基础的 Go 汇编知识，可以参考这里或这里。具体地，本文首先通过一些示例程序来切入，然后分析函数及方法在运行时调用原理。 一个示例程序从一个简单的示例程序切入，以阐述函数和方法（值类型接收者和指针类型接收者）调用。 1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport "fmt"//go:noinlinefunc Add(a int32, b int32) int32 &#123; return a + b&#125;type Calculator struct &#123; val int32&#125;//go:noinlinefunc (c Calculator) Value() int32 &#123; return c.val&#125;//go:noinlinefunc (c *Calculator) SetValue(val int32) &#123; c.val = val&#125;//go:noinlinefunc (c *Calculator) Add(incr int32) &#123; c.val += incr&#125;// no usedfunc main() &#123; Add(10, 21) // 0. 函数调用 cal := Calculator&#123;val: 0&#125; cal.Value() // 1. 值类型变量调用值类型接收者的方法 (&amp;cal).Value() // 2. 指针类型变量调用值类型接收者的方法 (&amp;cal).SetValue(10) // 3. 指针类型变量调用指针类型接收者的方法 cal.SetValue(10) // 4. 值类型变量调用指针类型接收者的方法 (*Calculator).SetValue(&amp;cal, 10) // 5. 直接调用方法所对应的一个函数以调用对应的方法 fmt.Println(Calculator.Value(cal)) // 10&#125; 上面的代码段逻辑比较简单，也没有复杂的语法，但对于初学者可能觉得有点奇怪。具体地，我们创建了一个结构变量 cal，一方面，在【1】处使用它调用值类型接收者（receiver type）的方法main.Calculator.Value，这是显而易见的，但我们在【2】处使用一个指针类型变量（即 &amp;cal）来调用一个值类型接收者的方法Value（毕竟，类型 *Calculator 并没有拥有一个Value方法），这在编译和运行时都没有问题。另一方面，在【3】处使用它调用指针类型接收者的方法main.(*Calculator).SetValue，这同样显而易见，但我们在【4】处使用一个值类型变量来调用一个指针类型接收者的方法SetValue，这种语法同样在编译和运行时也没有问题。而在【5】处，我们以一种奇怪的方式来调用指针类型接收者方法SetValue（但它似乎更像是一种函数调用）。 我们直接说结论，然后在下一小节通过观察其编译后的汇编输出来观察其运行时真正的调用原理。 当使用一个指针类型的变量来调用值类型接收者的方法时，实际上编译器会将指针类型变量进行解引用为对应类型的值，然后使用对应的值类型变量来调用值类型接收者的方法。具体地，编译器会将(&amp;cal).SetValue自动改写成cal.SetValue。 当使用一个值类型的变量来调用指针类型接收者类型的方法时，这实际上是编译器的语法糖，且此语法糖只针对可寻址的值类型的变量有效。具体地，编译器会将cal.SetValue自动替换成(&amp;cal).SetValue。 编译器会为每一个方法声明自动隐式声明一个相对应的函数。具体地，编译器会为方法SetValue(int)生成一个函数(*Calculator).SetValue(*Calculator, int)，其中函数体中的内容即为对应方法体中的内容，且需要注意的是，其中(*Calculator).Value为函数名称，函数参数列表中第一个参数为对应类型的指针变量，函数体的内容即为方法体的内容。而Value()方法也是类似的。这正印证前文所说的任何方法都是一个带有额外接收者参数的函数这一表述。 函数和方法的运行时调用原理我们使用命令go tool compile -S method_cases.go，注意，我们没有采用-l -N来禁止编译器优化（事实上，其输出的汇编内容类似，只是多了一些额外的指令）。 函数调用过程作为对比，首先看一下Add(10, 21)函数调用过程，其调用过程比较简单。 1234560x0000 00000 (method_cases.go:29) TEXT &quot;&quot;.main(SB), ABIInternal, $96-0;; ...省略了除函数/方法调用外的所有代码...;; 调用函数 main.Add0x002f 00047 (method_cases.go:30) MOVQ $90194313226, AX ;; 将 (10, 21) 移动到 ..0x0039 00057 (method_cases.go:30) MOVQ AX, (SP) ;; ..栈顶（即参数 #1 和参数 #2）0x003d 00061 (method_cases.go:30) CALL &quot;&quot;.Add(SB) ;; 调用函数 main.Add 可以发现，整个函数调用的过程很直接，考虑到 Go 的函数（方法）调用都是传值，换言之，其主要是通过将函数调用所需的参数拷贝到栈上（若有返回值，则调用方同样需准备好返回值在栈上空间），换言之，参数和返回值被存储在调用方的栈帧中，最后，函数调用被翻译成直接跳转指令，目标是.text 段的全局函数符号。 结构体变量创建过程接下来，简单了解 Calculator 结构变量的创建初始化过程。实际上，struct 类型变量的创建方式有多种，可通过字面量创建对应的值类型变量（示例程序中即是），也可以通过 new 来创建其零值的指针变量，当然，也可直接创建对应类型的零值值类型变量。 1234567;; cal := Calculator&#123;val: 0&#125;;; 使用 runtime.newobject 创建 Calculator 结构的指针变量0x0034 00052 (method_cases.go:32) LEAQ type.&quot;&quot;.Calculator(SB), AX ;; 将 Calculator 结构的类型描述符(_type)的指针移动到..0x003b 00059 (method_cases.go:32) MOVQ AX, (SP) ;; ..栈顶（即 runtime.newobject 的参数）0x003f 00063 (method_cases.go:32) CALL runtime.newobject(SB) ;; 调用函数 runtime.newobject0x0044 00068 (method_cases.go:32) MOVQ 8(SP), AX ;; 将分配好的内存对象拷贝到...0x0049 00073 (method_cases.go:32) MOVQ AX, &quot;&quot;.&amp;cal+64(SP) ;; ..cal 变量中(相对于伪SP的104偏移位置) 整个过程比较简单，但需要注意的是，并不是针对所有的结构类型的字面量的创建操作，编译器都会使用runtime.newobject函数来显式为其分配内存。换言之，被创建的变量有可能在栈上也有可能在堆中。我们通过如下代码进行测试。 1234567func main() &#123; Add(10, 21) cal := Calculator&#123;val: 1&#125; cal.Value() (&amp;cal).Value()&#125; 即将第一小节中的 main 方法进行精简，这导致编译器直接在栈上创建了对应结构变量。下面是结构创建对应的汇编输出。 123;; 这里直接在栈上创建一个结构变量0x0030 00048 (method_cases.go:31) MOVL $0, &quot;&quot;.cal+20(SP)0x0038 00056 (method_cases.go:31) MOVL $1, &quot;&quot;.cal+20(SP) 显然，生成的汇编没有什么地方有对 type.&quot;&quot;.Calculator(SB) 的引用，编译器并没有在堆中显式为结构变量分配内存，而是直接将变量初始化在栈上。更进一步地，我们看后面两行方法调用的输出（读者可以先了解正常情况下，方法调用被编译后的汇编代码，即接下来的两个小节的内容，最后再来了解此部分内容）。 1234;; cal.Value() 调用过程;; 直接在栈上创建了 cal 变量的拷贝，以调用方法 main.Calculator.Value0x0040 00064 (method_cases_1.go:32) MOVL $1, (SP)0x0047 00071 (method_cases_1.go:32) CALL &quot;&quot;.Calculator.Value(SB) 可以发现，编译器在这里做一些优化，并没有拷贝对应的接收者变量，尽管拷贝的目标地址是我们调用对应方法需要传入的 receiver 所处栈地址位置。这是因为 receiver 是值类型，且编译器能够通过静态分析推测出其值，这种情况下编译器认为不需要对其从它原来的位置进行拷贝了。 相反，只需简单地在栈上创建一个新的和 cal 变量相等的值即可。 我们继续观察(&amp;cal).Value的汇编输出。它同上一个方法调用不一样，它将接收者变量显式拷贝到栈上。 123456;; (&amp;cal).Value() 调用过程;; 这里没有新建 receiver 变量的拷贝，但其直接拷贝值类型的 receiver 变量，来调用方法 main.Calculator.Value,;; 而未调用方法 main.(*Calculator).Value0x004c 00076 (method_cases_1.go:33) MOVL &quot;&quot;.cal+20(SP), AX0x0050 00080 (method_cases_1.go:33) MOVL AX, (SP)0x0053 00083 (method_cases_1.go:33) CALL &quot;&quot;.Calculator.Value(SB) 调用值类型接收者的方法下面阐述值类型接收者的方法的调用过程。这根据调用者为值类型还是指针类型分为两种情况，对应示例程序的【1】和【2】。其中，调用者为值类型的方法调用过程如下。 12345;; cal.Value();; 值类型 receiver 调用方法 main.Calculator.Value0x004e 00078 (method_cases.go:33) MOVL (AX), CX ;; 解引用 cal 变量的，并将其拷贝到..0x0050 00080 (method_cases.go:33) MOVL CX, (SP) ;; ..栈顶（即参数）0x0053 00083 (method_cases.go:33) CALL &quot;&quot;.Calculator.Value(SB) ;; 调用函数 main.Calculator.Value 考虑到Value方法没有显式参数列表，因此，其调用过程只是将对应值类型的 receiver 拷贝到调用栈再直接调用目标方法即可。 作为对比，调用者为指针类型的方法调用过程如下。 123456;; (&amp;cal).Value();; 指针类型 receiver 调用方法 main.Calculator.Value0x0058 00088 (method_cases.go:34) MOVQ &quot;&quot;.&amp;cal+64(SP), AX ;; 将 cal 变量的指针（地址）拷贝到 AX0x005d 00093 (method_cases.go:34) MOVL (AX), CX ;; cal 指针变量解引用，同时拷贝到..0x005f 00095 (method_cases.go:34) MOVL CX, (SP) ;; ..栈顶（即参数）0x0062 00098 (method_cases.go:34) CALL &quot;&quot;.Calculator.Value(SB) ;; 调用函数 main.Calculator.Value 可以发现，当使用一个指针类型的变量来调用值类型接收者的方法时，编译器首先将此指针类型变量解引用，再使用解引用后的值来调用值类型接收者的方法。这符合上文给出的结论。而且，其同样是将值类型的 receiver 拷贝到调用栈，再跳转到被调用方法。 但事实上，对于使用指针类型的变量来调用值类型接收者的方法，还有另一种调用方式。采用上述调用方式的原因在于，编译器分析发现 receiver 变量并没有逃逸到堆上。换言之，若 receiver 在栈上，且 receiver 本身很小，则此时只需较少的汇编指令就可以将其先解引用再将其拷贝到栈上，然后直接调用对应的方法。 然而，若 receiver 逃逸到堆上，则编译器会采用另一种方式来调用。具体地，编译器会为每一个值类型接收者的方法生成一个对应的指针类型接收者的方法（这些方法因为不存在于源代码中，因此编译器会将其标记为 &lt;autogenerated&gt;），然后在生成的指针类型接收者方法中先拷贝对应的参数，同时，保证此指针类型接收者不为 nil，最后调用原来的值类型接收者的方法（准确而言，调用的是编译器为此值类型接收者的方法生成的对应的函数，具体参阅『方法的调用即则为对应类型的隐式函数的调用』）。下面是编译器为值类型接收者的Value方法生成的指针类型的Value方法的详细注释的汇编代码。 12345678910111213141516171819202122;; (*Calculator).Value 函数声明内容0x0000 00000 (&lt;autogenerated&gt;:1) TEXT &quot;&quot;.(*Calculator).Value(SB), DUPOK|WRAPPER|ABIInternal, $24-16;; ...省略了除方法调用外的所有代码...0x0026 00038 (&lt;autogenerated&gt;:1) MOVQ &quot;&quot;..this+32(SP), AX ;; 判断 cal 指针变量..0x002b 00043 (&lt;autogenerated&gt;:1) TESTQ AX, AX ;; ..是否为 nil0x002e 00046 (&lt;autogenerated&gt;:1) JEQ 76 ;; 若为 nil，则跳转到 76 行，panic0x0030 00048 (&lt;autogenerated&gt;:1) MOVL (AX), AX ;; 解引用 cal 指针变量..0x0032 00050 (&lt;autogenerated&gt;:1) MOVL AX, (SP) ;; ..同时，将其拷贝到栈顶（main.Calculator.Value的参数#1）;; ...拷贝其它参数（若方法包含有其它参数的情况）...;; 调用方法（函数） main.Calculator.Value，同时拷贝返回值，然后返回0x0035 00053 (&lt;autogenerated&gt;:1) CALL &quot;&quot;.Calculator.Value(SB)0x003a 00058 (&lt;autogenerated&gt;:1) MOVL 8(SP), AX 0x003e 00062 (&lt;autogenerated&gt;:1) MOVL AX, &quot;&quot;.~r0+40(SP) 0x0042 00066 (&lt;autogenerated&gt;:1) MOVQ 16(SP), BP0x0047 00071 (&lt;autogenerated&gt;:1) ADDQ $24, SP0x004b 00075 (&lt;autogenerated&gt;:1) RET;; 若指针接收者类型（pointer receiver）变量为 nil，则直接调用 runtime.panicwrap0x004c 00076 (&lt;autogenerated&gt;:1) CALL runtime.panicwrap(SB);; ...省略了除方法调用外的所有代码... 调用指针类型接收者的方法类似地，下文开始阐述指针类型接收者的方法调用过程。同样根据调用者的类型分为两种情况，对应示例程度的【3】和【4】。首先，看一下调用者为指针类型的方法调用过程。 123456;; (&amp;cal).SetValue(10);; 指针类型 receiver 调用方法 main.(*Calculator).SetValue0x0067 00103 (method_cases.go:36) MOVQ &quot;&quot;.&amp;cal+64(SP), AX ;; 将 cal 变量的指针（地址）拷贝到 AX0x006c 00108 (method_cases.go:36) MOVQ AX, (SP) ;; 对 cal 指针变量拷贝到栈顶0x0070 00112 (method_cases.go:36) MOVL $10, 8(SP) ;; 为方法传入参数 10 （第二个参数）0x0078 00120 (method_cases.go:36) CALL &quot;&quot;.(*Calculator).SetValue(SB);; 调用 main.(*Calculator).SetValue 同值类型变量调用值类型接收者的方法类似，只不过此时拷贝到调用栈上的是对应的指针变量，且同样是栈中参数列表中的第一个位置。我们重点看一下值类型的调用者。 123456;; cal.SetValue(10)，即值类型 receiver 调用方法 main.(*Calculator).SetValue;; 同 (&amp;cal).SetValue(10) 的汇编输出类似0x007d 00125 (method_cases.go:37) MOVQ "".&amp;cal+64(SP), AX0x0082 00130 (method_cases.go:37) MOVQ AX, (SP)0x0086 00134 (method_cases.go:37) MOVL $10, 8(SP)0x008e 00142 (method_cases.go:37) CALL "".(*Calculator).SetValue(SB) 显然，它和使用指针类型的变量的调用方法的汇编输出完全一样，即仍然将对应指针类型的变量拷贝到调用栈上。换言之，这种语法只是编译器的语法糖。 方法的调用即则为对应类型的隐式函数的调用最后，我们重点看一下示例程序的【5】的汇编输出。 123456;; (*Calculator).SetValue(&amp;cal, 10) 的调用过程;; 同 (&amp;cal).SetValue(10) 的汇编输出类似0x0093 00147 (method_cases.go:38) MOVQ "".&amp;cal+64(SP), AX0x0098 00152 (method_cases.go:38) MOVQ AX, (SP)0x009c 00156 (method_cases.go:38) MOVL $10, 8(SP)0x00a4 00164 (method_cases.go:38) CALL "".(*Calculator).SetValue(SB) 事实上，(*Calculator).SetValue(&amp;cal, 10)的调用方式和前面使用值类型或者指针类型来调用指针类型的接收者的方法的作用是一样的，而且，它们的编译后的汇编代码也是一样的。而且，观察一下这三种调用方式最终调用对应的SetValue函数。 1230x0078 00120 (method_cases.go:36) CALL &quot;&quot;.(*Calculator).SetValue(SB);; 调用 main.(*Calculator).SetValue0x008e 00142 (method_cases.go:37) CALL &quot;&quot;.(*Calculator).SetValue(SB)0x00a4 00164 (method_cases.go:38) CALL &quot;&quot;.(*Calculator).SetValue(SB) 可以发现函数调用指令 CALL 后面跟的目标是同一个.text 段的全局函数符号。换言之，所有的方法调用最终都会被改写成编译器为其隐式声明的函数调用。其中，所有这些隐式声明的函数的第一个参数为对应方法的接收者（值或者指针类型），其余的参数同方法的参数列表一样，而其函数体即为对应方法体的内容。需要注意的是，考虑到(*Calculator).SetValue实际上并非一个合法的函数名称，换言之，我们不能在源代码中显式声明具有类似名称的函数，只有编译器才能创建。而且，当编译器为每一个方法创建对应的隐式函数之后，那些方法的方法体都会被改写成调用对应的函数。 因此，结合『调用值类型接收者的方法』小节最后所阐述的内容，可以发现，对于每一个值类型接收者的方法，编译器会为其生成一个指针类型接收者的方法和这两个方法所对应的两个隐式声明的函数。 最后，值得注意的是，关于函数及方法使用的最佳实践，相信读者大致了解。但关于值类型和指针类型接收者使用的最佳实践呢？官方提供了一份指南。总的来说，我们需要关注的几点是： 显然，当需要修改接收者变量时，必须使用指针类型接收者； 值类型的接收者的方法调用需要拷贝对应的值变量，因此，较大的值变量的开销不容忽略； 过多的指针类型接收者可能会导致垃圾回收器负担过重； 对于 map、chan 以及函数类型的接收者，需要使用值类型； 如果接收者包含 sync 标准库包中的类型，则应该使用指针类型接收者； 如果不能确定使用何种类型，则使用指针类型接收者。 至此，本文相关内容已经阐述完毕，本文的重点在于通过解析函数和方法（值类型和指针类型接收者）调用的汇编过程，以阐述值类型接收者和指针类型接收者的方法被调用时的具体原理，以及方法调用和函数调用的关系。 本文参考代码在这里。 参考文献[1]. https://go101.org/article/method.html[2]. function-and-method-calls]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go map 原理剖析]]></title>
    <url>%2F2020%2F05%2F26%2Fgo-map-%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Go 语言提供了 map 内置数据类型，map 在实际开发中应用较为广泛。但在使用 Go map 时难免会有一些疑问，比如 map 底层是如何实现的、map 是如何支持泛型的、 map 中查询不存在的键为什么返回对应值类型的零值、map 遍历为什么没有保证顺序、 map 的元素为什么不能直接寻址以及 map 检测多个 go routine 并发读写的原理是什么等等。因此，本文通过阐述 go runtime 中 map 的实现原理来解决这些疑问。 注意，本文并非 map 的使用或介绍教程，因此你需要具备一定基础知识，可以从这里和这里了解。另外，本文剖析的 Go 源码的版本是 go1.12 ，但事实上相近的不同版本差距并不大，读者可以阅读源码以更加细致地了解。具体地，本文首先通过介绍 Go map 源码中一些关键的结构声明和字段以阐述 map 底层大致是如何实现的，然后再结合源码介绍 map 的几个典型的操作，如创建、查询、扩容以及删除和遍历操作实现的基本原理。事实上，若读者熟悉 Java 的 HashMap 或者 C++ 的 unorderedMap 的源码，那也能快速上手 Go map 实现原理，其大致是类似的，只不过因数据结构不同，使得具体实现有些区别。另外，需要提醒的是，本文篇幅非常长，因此，若读者没有足够的时间，则建议阅读第一小节『map 基本实现原理』以对 Go map 的大致了解，它基本是足够的。其次，每一个小节（相当于 map 的一种操作）的开头部分都阐述了对应操作的大致实现原理，这可以使得读者能够不深入源码而对实现的大致原理有更进一步的了解。 map 基本实现原理在使用 map 编程时，你是否思考过 map 是如何实现泛型的——键类型支持几乎所有类型（除了 slice、map 和 function 等），而值类型支持所有类型。但到 Go1.4 为止，Go 未提供泛型支持。因此你可能会觉得 map 的签名包含 interface{} 类型，实际上并不是。而且，不同于 Java 和 C++ 所实现的 map 的泛型支持方式，Java 中的 HashMap 所支持的泛型仅能添加对象类型（基本类型需要装箱），并且未将类型信息保留到运行期（泛型擦除），而 C++ 中的 unorderedMap 则实现了真正的泛型，换言之，每一种键值类型的 map 通过代码生成技术都会被编译成不同的类型（因此，在可执行二进制文件中充斥着大量的类型信息）。而 Go 则集成了二者的优点。具体地，它没有 C++ 所面临的类型信息膨胀问题，因为它为不同键值类型的 map 只生成不同的 maptype 结构，而且也不会有类型限制（或者频繁装箱和拆箱而遭受性能损失）。 map 实现原理概述Go map 由编译器和 runtime 协作实现。概括而言，包括如下两点： 关于数据结构。map 使用数组+链表的实现方式（Java HashMap 采用数组+链表/红黑树，而 C++ unorderedMap 采用数组+链表），即采用拉链法来解决 key 的冲突问题，其中数组位置被称为 bucket，且初始 bucket 数量为 2^B（即大于用户指定的容量的最小的 2 的指数幂）。另外使用 cell（源码中没有明确这个称呼）来容纳具体键值对信息 ，且一个 bucket 只能装载 8（bucketCnt） 个 cell，当 bucket 中没有更多空间容纳添加的键值对时，则在此 bucket 后动态挂一个称为 overflow 的 bucket，以此类推。其中 bucket 的映射算法使用 key 的 hash 值的低 B 位的值，而 cell 的映射算法使用 key 的 hash 值的高 8 位的值。 关于扩容。扩容的目的是提高检索、插入和删除的效率，而扩容手段采用的是渐近式扩容或者增量扩容（原理和实现类似于 redis 的渐近式 rehash ），即每次执行插入或修改以及删除操作时最多迁移 2 个 bucket。一方面，当整个 map 中包含过多数量的键值对时，即当前键值对数量超过负载因子 6.5（loadFactor），导致此种情形出现的原因是创建的 bucket 数量过少（元素过多），因此将现有容量（即 bucket 的数量）扩大为原来的一倍，在这种情况下，旧的 bucket 中的元素根据其 key 的 hash 值的第 B+1 位是否为 0，以决定其被迁移到扩容后的 bucket 数组的前半部分还是后半部分；另一方面，当整个 map 包含过多 overflow bucket 时，导致此种情形出现的原因是频繁的插入和删除操作创建大量的 overflow bucket，因此开辟一个新的 bucket ，将旧的 bucket 元素以相同的映射方式迁移重组到新的 bucket，通过将 overflow bucket 中的键值对整合到 bucket 中，使得 bucket 中的键值对排列更紧凑，以节省空间并提高 bucket 利用率。事实上，因为采用拉链法，因此过多 overflow bucket 使得元素查找效率急剧下降（不能保证接近O(1)的复杂度）。 map 实现涉及的关键数据结构结合源码来看，map 的 runtime 实现主要及到 hmap 和 bmap 两个数据结构，hmap 代表 map header 数据结构，具体如下所示。 1234567891011121314151617181920type hmap struct &#123; count int // map 键值对数目。必须处于第一个位置，以使得 len() 函数可以正确获取其长度 flags uint8 // map 的状态，如正在被某个迭代器遍历，或者被某个 go routine 写入 B uint8 // 容量为 2^B，但只能容纳 loadFactor(6.5) * 2^B 个元素 noverflow uint16 // overflow buckets 的大致数量 hash0 uint32 // hash 种子，用于计算 key 的 hash 值 buckets unsafe.Pointer // 指向 buckets 数组，大小为 2^B，若 count==0，则其为 nil. oldbuckets unsafe.Pointer // 扩容时使用，oldbuckets 指向旧的 bucket 数组，新的 buckets 长度是 oldbuckets 的两倍 nevacuate uintptr // 扩容时使用，元素迁移进度，小于此索引的 buckets 表示已迁移完成 extra *mapextra // 可选字段，当 key 和 value 可以被 inline 时，会使用此字段&#125;// mapextra 包含一些没有包含在 map 中的字段type mapextra struct &#123; overflow *[]*bmap // 指向一个 bmap 地址切片的指针，表示 overflow bucket oldoverflow *[]*bmap // 同上，扩容时使用，表示旧的 overflow bucket nextOverflow *bmap // 指向空闲的 overflow bucket 的指针&#125; hmap 定义很清晰，但需说明的是，mapextra 源码中注释说若 map 的 key 和 value 都不包含指针，并且可以被 inline（不大于 128 字节），则可使用 mapextra 来存储 overflow bucket，以避免 GC 扫描整个 map。但考虑到 bmap 有一个 overflow 的指针字段（下文），因此就把 overflow 移动到 hmap.mapextra.overflow以及hmap.mapextra.oldoverflow 字段。换言之，mapextra.overflow 包含的是 hmap.buckets的 overflow buckets，而mapextra.oldoverflow则包含的是hmap.odlbuckets的 overflow buckets。 另外，bmap 表示 bucket 数据结构，即数组元素的表示，其在 runtime 中的定义如下。 12345type bmap struct &#123; // 存储此 bucket 中的 key 的 top hash 值（高8位） // 并且，若 tophash[0] &lt; minTopHash，则表示此 bucket 正在被迁移（处于扩容状态） tophash [bucketCnt]uint8&#125; 事实上，bmap 在编译期会通过反射给它增加几个字段，因此，它真正的结构如下所示。 1234567type bmap struct &#123; tophash [bucketCnt]uint8 keys [8]keytype // 存储在此 bucket 中 8 个连续的 key values [8]valuetype // 存储在此 bucket 中 8 个连续的 value padding uniptr // 内存对齐字段（可选） overflow uniptr // overflow bucket 指针&#125; bmap 结构也容易理解，值得注意的是，它的 key/value 的存储的方式，并不是交叉存储，而是分开存储，显然这种存储方式可以压缩 padding 的大小。因此，bmap 的内存结构大致如下所示。其中 hobH 表示元素 key 值的 hash 值的高 8 位。 1234567891011+-------------------------------------------------------+| hobH | hobH | hobH | hobH | hobH | hobH | hobH | hobH |+-------------------------------------------------------+| key[8] |+-------------------------------------------------------+| val[8] |+-------------------------------------------------------+| padding(optional) |+-------------------------------------------------------+| *overflow |+-------------------------------------------------------+ 现在，我们似乎还未找到和 map 支持泛型相关的代码。可以看一下创建 map 的函数原型。 1func makemap(t *maptype, hint int, h *hmap) *hmap 可以看到 maptype 类型的参数 t 被传入创建函数。事实上，maptype 存储了关于此 map 的 key 和 value 的详细信息，对于每一个不同键值对类型的 map，在编译时期会创建一个对应的 maptype。其定义如下。 12345678910type maptype struct &#123; typ _type key *_type // key 的内部类型 elem *_type // value 的内部类型 bucket *_type // bucket(bmap) 的内部类型 keysize uint8 // key 的大小 valuesize uint8 // value 的大小 bucketsize uint16 // bucket 的大小 flags uint32 // key 或 value 的状态，比如 key 存储值还是指针值&#125; 如上所示，每个被创建的 maptype 都包含关于从 key 到 elem 的 map 属性的详细信息。其中 _type 类型可称为类型描述符。 12345678910111213type _type struct &#123; size uintptr ptrdata uintptr hash uint32 tflag tflag align uint8 fieldalign uint8 kind uint8 alg *typeAlg gcdata *byte str nameOff ptrToThis typeOff&#125; _type 类型（类型描述符）包含其所代表类型的详细信息，如大小等。另外，我们可通过其包含的 typeAlg 字段来比较两个类型的值是否相等（equal 函数），以及取得此类型的值的 hash 值（hash 函数）。typeAlg 的结构定义如下所示。 123456type typeAlg struct &#123; // (ptr to object, seed) -&gt; hash hash func(unsafe.Pointer, uintptr) uintptr // (ptr to object A, ptr to object B) -&gt; ==? equal func(unsafe.Pointer, unsafe.Pointer) bool&#125; map 实现涉及的关键常量和 util 函数至此，读者应该清楚 Go map 泛型支持的实现原理了。在正式阐述 map 中各具体操作的逻辑前，为了帮助理解，我们先简单介绍一些关键常量以及一些工具类函数和方法。 12345678910111213141516171819202122232425262728293031323334353637const ( // bucket 包含的 cell 的数量 1 &lt;&lt; 3 = 8 bucketCntBits = 3 bucketCnt = 1 &lt;&lt; bucketCntBits // map 的负载因子，默认是 6.5. // 即当 map 中包含的元素大于 (loadFactorFum/loadFactorDen) * 2^B 时，则进行扩容 loadFactorNum = 13 loadFactorDen = 2 // 能够 inline 的 key 和 value 的大小 maxKeySize = 128 maxValueSize = 128 // dataOffset 即为 bmap 结构的大小，但包含对齐的字节。 dataOffset = unsafe.Offsetof(struct &#123; b bmap v int64 &#125;&#123;&#125;.v) // tophash 的一些特殊取值。 emptyRest = 0 // cell 是空的，而且在此 cell 后面不存在容纳元素的 cell. emptyOne = 1 // cell 是空的 evacuatedX = 2 // 此 cell 所对应的 key/value 被迁移到新的 bucket 数组的前半部分： hash&amp;bucketCnt==0 evacuatedY = 3 // 此 cell 所对应的 key/value 被迁移到新的 bucket 数组的后半部分： hash&amp;bucketCnt==1 evacuatedEmpty = 4 // cell 是空的, 而且此 bucket 对应的 cell 已被迁移 minTopHash = 5 // 处于正常状态的 bucket 的最小的 tophash 值。若计算的值小于此值，则直接加上此值作为新的 topHash // hmap.flags 的状态取值 iterator = 1 // 有迭代器正在使用（遍历） hmap.buckets oldIterator = 2 // 有迭代器正在使用（遍历） hmap.oldbuckets hashWriting = 4 // 有迭代器正在写入或修改 hmap。此字段用于防止 go routine 并发读写 map sameSizeGrow = 8 // 当前 map 正执行等量扩容操作。即此时扩容的原因是 overflow buckets 过多 // 迭代 map 时使用的哨兵 bucket ID，表示当前迭代的 bucket 未处于扩容状态，或者即使处于扩容状态，也已经迁移完毕 noCheck = 1&lt;&lt;(8*sys.PtrSize) - 1) 12345678910111213141516171819202122232425262728293031323334// 返回 bucket 数目。如 B = 4，则返回 10000func bucketShift(b uint8) uintptr &#123; // ... return uintptr(1) &lt;&lt; b&#125;// 返回 bucket 数目的掩码值。如 B = 4，则返回 1111func bucketMask(b uint8) uintptr &#123; return bucketShift(b) - 1&#125;// 返回指定 key 的 hash 值所对应的 tophash，即获取 hash 值的高 8 位值func tophash(hash uintptr) uint8 &#123; top := uint8(hash &gt;&gt; (sys.PtrSize*8 - 8)) if top &lt; minTopHash &#123; top += minTopHash &#125; return top&#125;// 返回此 bucket 是否已经被迁移完毕func evacuated(b *bmap) bool &#123; h := b.tophash[0] return h &gt; emptyOne &amp;&amp; h &lt; minTopHash&#125;// 返回此 maptype 对应类型值的 overflow 指针func (b *bmap) overflow(t *maptype) *bmap &#123; return *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize))&#125;// 设置 maptype 对应类型值的 overflow 的指针func (b *bmap) setoverflow(t *maptype, ovf *bmap) &#123; *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize)) = ovf&#125;// 返回 bucket 中存储 key 的起始偏移位置func (b *bmap) keys() unsafe.Pointer &#123; return add(unsafe.Pointer(b), dataOffset)&#125; map 创建函数map 创建逻辑比较简单。对应的函数为 makmap，首先判断用户传入的大小值是否合法，然后创建 hmap 并初始化 map，接下来计算 hmap.B 的大小，即大于传入的 map 大小的最小的 2 的整数幂，最后为 hmap 创建 bmap，即分配一段连续的内存空间来存储元素。其源码如下。 123456789101112131415161718192021222324252627282930func makemap(t *maptype, hint int, h *hmap) *hmap &#123; // t 存储 map 类型信息， hint 表示用户创建的 map 的大小 // h 若不为 nil，则直接在 h 中创建新的 map，且若 h.buckets 也不为 nil，则其指向的 bucket 即为第一个 bucket 地址 // 1. 判断 hint 大小是否合法，不能超过最大分配内存，也不能溢出 mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem &gt; maxAlloc &#123; hint = 0 &#125; // 2. 初始化 hmap，并确定其 hash 种子的值 if h == nil &#123; h = new(hmap) &#125; h.hash0 = fastrand() // 3. 计算 B 的值，即大于传入的 map 大小的最小的 2 的整数幂 B := uint8(0) for overLoadFactor(hint, B) &#123; B++ &#125; h.B = B // 4. 若 B=0，则延迟分配 bucket，否则直接开辟内存空间 if h.B != 0 &#123; var nextOverflow *bmap h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil &#123; h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow &#125; &#125; return h&#125; 值得注意的是，makemap 函数返回值是 *hmap 指针类型（不同于 slice，其返回的是 slice 结构体），因此，当将 hmap 作为函数参数时，在函数内部对参数 map 的修改会反映到原始的 map 变量，而不需要像 slice 一样要返回修改后的 slice。 map 元素查询map 的元素查询关键点在于 bucket 的映射方法以及 cell 的映射方法。对应的函数为 mapaccess1，其大致过程如下：首先判断若 map 为空或者大小为 0，则直接返回 map 元素值类型的零值。然后，检测是否存在 go routine 并发读写 map 的情况。接下来的执行过程大致为先定位 bucket，再定位 cell。具体地，先获取当前查询的 key 的 hash 值，然后根据 hash 值的低 B 位的值计算 key 所映射的 bucket，同时，计算当前 key 的 tophash 值用于后续映射 bucket 中的 cell。接下来，执行两层 for 循环，外层循环遍历当前 bucket 链（因为可能存在 overflow bucket），内层循环遍历 bucket 的 8 个 cell ，根据 tophash 值定位对应的 cell。需要注意的是，map 元素查询过程可能和 map 扩容操作的元素迁移过程重叠，此时，若旧的 bucket 未迁移完毕，则需要到对应的旧的 bucket 中执行上述的两层 for 循环。 在介绍 map 元素查询源码逻辑前，先简单介绍几个频繁使用的 bucket 定位以及 key 和 value 定位的计算方式。 1b := (*bmap)(add(h.buckets, (hash&amp;m)*uintptr(t.bucketsize))) 上面为 bucket 定位方式。其中 m 为 map 的掩码值，hash 为 key 的 hash 值，因此 m&amp;hash可以计算出对应的 bucket 在数组中的索引，也即需要跳过的 bucket 的数目，最后加上 hmap 中 buckets 指针的偏移，可得到 key 所映射的 bucket 的地址。 1k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) 上面为 key 定位方式。其中 dataOffset 为 bucket 中 key 的起始偏移，因此 dataOffset+i*uintptr(t.keysize)可以跳过指定数量的 key，最后通过加上当前 bucket 的地址偏移以获取对应的 key 所映射的 cell 的地址。 1v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) 上面为 key 所关联的 value 定位方式。显然，其定位过程同 key 类似，只不过需要加上位于其前面的 8 个 key 占用的地址空间。 最后，在 map 中通过指定 key 查找 value 的方法 mapaccess1 的源码解析如下，为了更清楚阐述，删除了部分不太相关的逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123; // 编译器将用户程序的 v := map[key] 映射到此函数调用 // t 存储 map 类型信息， h 表示被查询的 map，而 key 表示查询的元素的 key 值 // ... // 1. 若 map 为空，则返回对应元素值类型的零值 if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.key.alg.hash(key, 0) // see issue 23734 &#125; return unsafe.Pointer(&amp;zeroVal[0]) &#125; // 2. 禁止 go routine 并发读写 map if h.flags&amp;hashWriting != 0 &#123; throw("concurrent map read and map write") &#125; // 3. 获取 key 对应的 typeAlg 字段，以用于计算其 hash 值以及判断两个 key 是否相同 alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) // 4. 计算 hmap.B 的掩码值，然后定位到对应的 bucket m := bucketMask(h.B) b := (*bmap)(add(h.buckets, (hash&amp;m)*uintptr(t.bucketsize))) // 5. 若 map 正在执行扩容过程，则进一步判断是否为等容量扩容， // 若非等量扩容（增大为原容量的两倍），则更新上述 m 的值为原来的一半， // 同时，定位查询 key 在旧的 bucket 数组中对应的 bucket， // 最后，若此对应的 bucket 未迁移完成，则后续将在此 bucket 中检索 key 对应的元素值 if c := h.oldbuckets; c != nil &#123; if !h.sameSizeGrow() &#123; m &gt;&gt;= 1 &#125; oldb := (*bmap)(add(c, (hash&amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) &#123; b = oldb &#125; &#125; // 6. 计算当前 key 的 tophash 值 top := tophash(hash) // 7. 在指定的 bucket 链（若包含有 overflow bucket）上根据 tophash 值查找对应的 keybucketloop: // 7.1 外层循环遍历 bucket 链上的每个 bucket， // 其中 overflow(t) 表示获取下一个 overflow bucket for ; b != nil; b = b.overflow(t) &#123; // 7.2 内层循环遍历 bucket 包含的每个 cell for i := uintptr(0); i &lt; bucketCnt; i++ &#123; // 7.3 若 cell 存储的 tophash 值和当前 key 的 tophash 不同， // 并且为 emptyRest，则表明当前的 bucket 已经被迁移完成，则直接中止循环 // 否则，继续查看下一个 cell if b.tophash[i] != top &#123; if b.tophash[i] == emptyRest &#123; break bucketloop &#125; continue &#125; // 7.4 获取当前的 cell 中存储的 key，且若 key 为指针，则解引用取值 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; // 7.5 若当前 cell 的 key 值与查询的 key 值相等， // 则计算当前 cell 的 key 所关联的 value 所存储的地址 // 类似地，若 value 为指针，则解引用取值，最后返回此 value if alg.equal(key, k) &#123; v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) if t.indirectvalue() &#123; v = *((*unsafe.Pointer)(v)) &#125; return v &#125; &#125; &#125; // 7.6 否则，说明此 key 在当前 map 的所有 bucket 中未找到，返回 map 元素值类型的零值 return unsafe.Pointer(&amp;zeroVal[0])&#125; 最后，还有两点需要补充说明。首先，从源码中看，map 为源码中两种不同的获取指定 key 所对应 value 的方式实现了两个方法 mapaccess1 和 mapaccess2，其实现逻辑大致类似，只不过 mapaccess2 对应的是源码中v, ok := map[key]的访问操作；其次，为了提升执行效率，对于具体的 key 类型，编译器将查找、插入、删除操作所对应的函数用替换为更具体函数。比如，对于 key 为 uint32 类型，其调用 src/runtime/hashmap_fast.go 文件中的 mapaccess1_fast32(t maptype, h hmap, key uint32) unsafe.Pointer 函数。 map 元素插入或更新map 元素插入或更新的实现逻辑和 map 元素查询逻辑有很多重叠的地方。对应函数为 mapassign，其大致执行逻辑为：首先做一些预备工作，比如判断当前 map 是否为 nil 或者容量为 0，是否存在 go routine 并发写，以及计算当前 key 的 hash 值，标记当前 map 处于写入状态等。接下来的三层（也可以认为是两层）循环是整个函数的核心，最外层循环主要考虑到当前 map 是否需要执行扩容操作，因为一旦执行扩容操作，则先前 bucket 中的 key 分布信息会失效，同时，在最外层循环中还会执行定位当前 key 所对应的 bucket 索引操作，而且，若当前确实处于扩容状态，则协助迁移最多两个 bucket。然后，进入到里面的两层循环，此两层循环的逻辑同 mapaccess1 函数的两层循环逻辑类似，第一层遍历 bucket 链以定位具体的 bucket，而第二层遍历每个 bucket 的 8 个 cell，查询当前 key 是否在之前就已经插入过，若是，则更新对应 key 在 tophash 数组中的索引地址、key 存储在 cell 中的地址以及 key 所关联的 value 的地址，最后统一赋值。最后一个部分包含一些收尾工作，比如重置当前 map 的写状态，并返回 key 所关联的 value 的地址，最后由汇编指令将对应的值存储到此指针所指的内存地址。在 map 中通过插入或更新键值对的函数 mapassign 的源码如下，同样，为了更清楚阐述，删除了部分不太相关的逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123; // 编译器将用户程序的 map[key] = value 映射到此函数调用。 // t 存储 map 类型信息， h 为对应的 map，而 key 表示被插入或更新元素的 key 值。 // 那么对应 value 值是如何传入的呢？事实上，赋值的最后一步是由编译器额外生成的汇编指令来完成的。 // mapassign 返回存储对应值的指针，汇编指令将对应的值存储到此指针所指的内存地址。 // 1. 若 h == nil，则直接 panic。 var m map[keytype]valuetype 这种情形下 m == nil if h == nil &#123; panic(plainError("assignment to entry in nil map")) &#125; // ... // 2. 同样禁止 go routine 并发写 map if h.flags&amp;hashWriting != 0 &#123; throw("concurrent map writes") &#125; // 3. 获取 key 对应的 typeAlg 字段，以用于计算其 hash 值以及判断两个 key 是否相同 alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) // 4. 标记当前 go routine 正在写 map h.flags ^= hashWriting // 5. 若 buckets 为空，即创建 map 时指定容量为 0： m := make(map[keytype]valuetype, 0) // 则直接创建容量为 1 的 map if h.buckets == nil &#123; h.buckets = newobject(t.bucket) // newarray(t.bucket, 1) &#125; // 6. again 标签表示由于执行了扩容操作导致 key 的分布信息失效， // 因此需要重新走一遍 key 的整个定位过程。again: // 7. 计算当前 key 所映射的 bucket 索引值 bucket := hash &amp; bucketMask(h.B) // 8. 判断当前 map 是否处于扩容状态（包含两种扩容情况）， // 若是，则先执行 bucket 迁移操作，再执行后续逻辑。 // 因为 map 扩容时元素迁移是渐近式的，每次插入或修改操作最多迁移两个 bucket if h.growing() &#123; growWork(t, h, bucket) &#125; // 9. 定位 key 所对应的 bucket 的地址，同时计算出 key 的 tophash 值 b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize))) top := tophash(hash) // inserti 指向 key 的 hash 值在 tophash 数组所处的位置 var inserti *uint8 // insertk 指向 key 所处的 cell 的位置 var insertk unsafe.Pointer // val 指向 key 关联的 value 所处的的位置 var val unsafe.Pointer // 10. bucketloop 仍旧为两层循环，外层循环遍历 bucket 链，内层循环遍历每个 bucket 的 8 个 cell， // 在其中查找同当前 key 相同的 cell，若查询成功，返回 key 所关联的 value 的地址，跳转到 done， // 否则，直接跳出循环，此时 inserti、insertk 以及 val 都为空值。bucketloop: for &#123; for i := uintptr(0); i &lt; bucketCnt; i++ &#123; // 11. 内层循环。循环遍历当前 bucket 的 8 个 cell，查找当前 key 是否存在。 // 若通过 tophash 找到一个空位 cell， // 则记录对应的空位索引地址、key 所存放的 cell 地址，以及关联的 value 的存放地址。 // 否则，若发现当前遍历的 tophash 值为 emptyRest，表明此 bucket 及其后续的元素已经被迁移， // 因此，直接退出遍历当前 bucket 的循环。 // 否则，当前遍历的 tophash 值同 key 的 tophash 值不相等，且不为空，也不为 emptyRest 时， // 则继续遍历下一个 tophash 值。 if b.tophash[i] != top &#123; if isEmpty(b.tophash[i]) &amp;&amp; inserti == nil &#123; inserti = &amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) val = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) &#125; if b.tophash[i] == emptyRest &#123; break bucketloop &#125; continue &#125; // 12. 找到同当前 key 的 tophash 相等的 cell，即当前 key 在之前有可能已经被插入过， // 但还需比较两个 key 值是否相同（因为两个不同的 key 其 tophash 有可能相同） // 若 key 值不同，则继续遍历下一个 key。 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; if !alg.equal(key, k) &#123; continue &#125; // 13. 否则，可以肯定当前 key 之前已插入过，此次操作为更新操作。 // 则将 key 所对应的值拷贝到对应的 cell， // 然后，计算 key 所关联的 value 的地址，跳转到 done，表明此次操作已经完毕 if t.needkeyupdate() &#123; typedmemmove(t.key, k, key) &#125; val = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) goto done &#125; // 14.继续遍历下一个 overflow bucket，直至遍历完所有的 overflow bucket // 则跳出循环。 ovf := b.overflow(t) if ovf == nil &#123; break &#125; b = ovf &#125; // 15. 若程序执行到这里，说明未能找到对应的 key，表明此次操作为插入操作，需要添加一个 key/value 对， // 但在正式插入之前，需要先检测是否需要扩容。 // 具体地，若 map 当前未执行扩容操作，但满足任一扩容条件（增量扩容和等量扩容）， // 则执行预扩容操作（即扩容的准备工作），hashGrow 完成分配新的 buckets 工作， // 并将旧的 buckets 挂到 oldbuckets 字段。 // 最后，重新执行步骤 7-14，因为扩容后，所有 key 的分布位置都发生了变化， // 因此需要重新走一次之前整个的查找定位 key 的过程。 if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again // Growing the table invalidates everything, so try again &#125; // 16. 表明当前 map 不需要扩容（或者已经扩容完毕），此次操作为插入操作， // 且在当前 bucket 中未能找到存储 key 的 cell。 // 因此，需要创建一个 overflow bucket，然后将此 key 插入到第一个 tophash 位置， // 同时计算对应的 key 插入的 cell 位置，和 value 插入的位置。 if inserti == nil &#123; // all current buckets are full, allocate a new one. newb := h.newoverflow(t, b) inserti = &amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) val = add(insertk, bucketCnt*uintptr(t.keysize)) &#125; // 17. 真正执行存储 key 和 value 以及 tophash 值的动作， // 但需要考虑 key 和 value 是否为指针的情况， // 当插入完毕后，更新当前 map 的元素的数量 count。 if t.indirectkey() &#123; kmem := newobject(t.key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem &#125; if t.indirectvalue() &#123; vmem := newobject(t.elem) *(*unsafe.Pointer)(val) = vmem &#125; typedmemmove(t.key, insertk, key) *inserti = top h.count++ // 18. 程序执行到这里，表示当前 key/value 已经插入成功， // 但有可能在插入过程序执行了扩容以及创建 overflow bucket 的动作。done: // 19. 再次检测是否存在 go routine 并发写， // 然后，重置当前 map 的写状态， // 最后，返回当前 key 所关联的 value 存储的地址 if h.flags&amp;hashWriting == 0 &#123; throw("concurrent map writes") &#125; h.flags &amp;^= hashWriting if t.indirectvalue() &#123; val = *((*unsafe.Pointer)(val)) &#125; return val&#125; 最后，同 map 元素查询操作类似，mapassign 同样有一系列的函数，其根据 key 具体类型，编译器将其优化为相应的快速函数。 map 扩容函数考虑到在 map 元素插入或更新，以及 map 元素的删除操作中都涉及到 map 扩容逻辑（bucket 渐近式迁移过程），因此在阐述 map 元素的删除操作之前，先重点了解 map 的扩容原理。下文从三个方面阐述 map 的扩容原理，首先介绍触发 map 扩容的条件，其次阐述扩容的准备工作（即 hashGrow 函数的逻辑），最后重点阐述扩容的具体逻辑（即 growWork 和 evacuate 的逻辑）。 map 扩容的触发条件在 mapassign 函数中，在最外层循环包含了检测当前 map 的扩容触发条件是否成立的代码逻辑。具体如下所示。 1234if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again&#125; 其中，h.growing()方法简单地根据当前 map 的 oldbuckets 是否为空以判断当前 map 是否处于扩容状态。 123func (h *hmap) growing() bool &#123; return h.oldbuckets != nil&#125; 而overLoadFactor()函数则判断当添加当前元素后，是否触发增量扩容（扩大为原容量的两倍）。所谓增量扩容是由当前 map 包含过多的元素导致的，换言之，bucket 数量过少，因此需要增加 bucket 的数量。 123func overLoadFactor(count int, B uint8) bool &#123; return count &gt; bucketCnt &amp;&amp; uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen)&#125; 最后的tooManyOverflowBuckets()函数则判断是否触发了等量扩容。所谓等量扩容是由于当前 map 包含过多的 overflow bucket 导致的，而过多的 oveflow bucket 实际上是相对于 bucket 数量而言的，其准确的描述为：map 中包含的 overflow bucket 的数量和 bucket 的数量基本持平，则说明 overflow bucket 中的 cell 非常稀疏（因为若非常紧凑，则早已触发了增量扩容）。因此，等量扩容的目的是将 overflow bucket 中的元素尽可能迁移到 bucket 中，以提高 bucket 的利用率，也间接提高了元素的查询效率。 123456789// 等量扩容的判断条件。在此情况下，overflow buckets 的使用通常是稀疏的，否则早就执行扩容操作了（超过 loadFactor）func tooManyOverflowBuckets(noverflow uint16, B uint8) bool &#123; if B &gt; 15 &#123; B = 15 &#125; // 当 bucket 数量不大于 1&lt;&lt;15 时，则触发条件为 overflow bucket 的数量是否超过 bucket 的数量； // 否则，其触发条件为 overflow bucket 的数量是否超过 1&lt;&lt; 15。 return noverflow &gt;= uint16(1)&lt;&lt;(B&amp;15)&#125; 上面的解释可能让读者有些疑惑，为何当 bucket 数量大于 1&lt;&lt;15 时，还是判断 overflow bucket 的数量（noverflow）是否大于 1&lt;&lt;15 以确定是否需要执行等量扩容。源码中注释提示我们需要结合incrnoverflow()函数来理解。 123456789101112131415161718192021func (h *hmap) incrnoverflow() &#123; // 因为 noverflow 为 uint16 类型，因此需要限制 h.B 的大小。 // 当 bucket 数量不大于 1&lt;&lt;15 时，则每次都会直接递增 noverflow 的值， // 因此，此时 noverflow 的值是一个准确的值。所以，当 noverflow &gt; 1&lt;&lt;15 时， // 会触发等量扩容。 if h.B &lt; 16 &#123; h.noverflow++ return &#125; // 当 bucket 数量大于 1&lt;&lt;15 时，每次调用 h.incrnoverflow()， // 不一定会增加 noverflow 的大小，实际上它是以 1/(1&lt;&lt;(h.B-15)) 概率递增 h.noverflow， // 换言之，结合 tooManyOverflowBuckets() 方法中的判断是否触发等量扩容的条件来看， // 当 bucket 数量大于 1&lt;&lt;15 时，只要 noverflow 的值超过 1&lt;&lt;15，则说明需要执行等量扩容。 // 这个近似的操作是合理的。 mask := uint32(1)&lt;&lt;(h.B-15) - 1 // Example: if h.B == 18, then mask == 7, // and fastrand &amp; 7 == 0 with probability 1/8. if fastrand()&amp;mask == 0 &#123; h.noverflow++ &#125;&#125; map 扩容的准备工作hashGrow()函数完成 map 扩容的准备工作。其操作内容大致包括两个方面，首先它会申请新的 bucket 空间，同时更新 oldbucket。其次，更新（转移）迭代器的状态，同时初始化 bucket 的迁移进度。 1234567891011121314151617181920212223242526272829303132333435363738394041424344func hashGrow(t *maptype, h *hmap) &#123; // bigger 为增量扩容的扩大因子，即扩大为原来的两倍 // 1. 判断是否触发了增量扩容，若触发的是等量扩容，则重置 bigger 为 0， // 并标记当前是处于等量扩容状态。 // 然后，保存当前的 bucket 指针，同时开辟新的 bucket 空间。 // 需要说明的是，无论是等量扩容还是增量扩容，都需要重新开辟 bucket 数组空间， // 只是开辟的大小不同。 bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) &#123; bigger = 0 h.flags |= sameSizeGrow &#125; oldbuckets := h.buckets newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) // 2. &amp;^ 为按位置 0 操作符。 z := x &amp;^ y, 它将 y 中的 1 bit 清零，否则保持和 x 相同。 // 简单而言，在 buckets 转移到 oldBuckets 下之后，此操作转移对应的迭代器的标志位。 flags := h.flags &amp;^ (iterator | oldIterator) if h.flags&amp;iterator != 0 &#123; flags |= oldIterator &#125; // 3. 提交 grow 操作，即更新 map 使其真正处于扩容状态。 // 同时切换 overflow 和 nextOverflow 指针 h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 // 初始化扩容进度（ bucket 迁移进度为 0） h.noverflow = 0 // 重置 overflow bucket 数量 if h.extra != nil &amp;&amp; h.extra.overflow != nil &#123; if h.extra.oldoverflow != nil &#123; throw("oldoverflow is not nil") &#125; h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil &#125; if nextOverflow != nil &#123; if h.extra == nil &#123; h.extra = new(mapextra) &#125; h.extra.nextOverflow = nextOverflow &#125; // the actual copying of the hash table data is done incrementally // by growWork() and evacuate().&#125; map 渐近式扩容原理map 元素的插入或修改，以及删除操作都可能执行到 map 的渐近式扩容操作。比如上一小节的 mapassign 函数中存在如下逻辑。 123456789101112if h.growing() &#123; growWork(t, h, bucket)&#125;func growWork(t *maptype, h *hmap, bucket uintptr) &#123; // 1. 再一次确认此次扩容需要迁移的 bucket 索引，然后执行 evacuate 进行 bucket 迁移 evacuate(t, h, bucket&amp;h.oldbucketmask()) // 2. 若当前还是处于扩容状态，即当前还剩有 bucket 未迁移完成， // 则再迁移一个 bucket if h.growing() &#123; evacuate(t, h, h.nevacuate) &#125;&#125; growWork 函数的操作比较简单，它最多执行两个 bucket （包括每个 bucket 后所挂的 overflow bucket）的迁移过程。真正的 bucket 迁移的逻辑位于函数 evacuate 中。同样，在展示其源码注释逻辑时，先概述整个 bucket 迁移过程。 整个 bucket 迁移过程大到处包含四个步骤。首先，获取需要被迁移的 bucket 的地址。其次，若当前 bucket 未被迁移，则依据当前扩容的类型，并根据被迁移的 bucket 在旧的 bucket 数组中的索引，定位其在新的 bucket 数组中插入前半段或者后半段对应的目标 bucket 地址，换言之，后续只需将被迁移的旧的 bucket 中的所有元素逐一插入到新的 bucket 中即可。具体同样是通过两层循环实现，外层循环遍历当前旧的 bucket 链中的每一个 bucket，而内层循环遍历每个 bucket 中的 8 个 cell。具体地，在内层循环遍历过程中，一旦发现当前 cell 的 tophash 为 evacuatedEmpty，则表明其已被迁移。另外，若此次扩容为增量扩容，则还要判断当前 bucket 的元素是迁移到新的 bucket 数组的前半段还是后半段，确定好之后，就可以将原 bucket 中的 key 和 value 存储到对应的目标 bucket 中，然后继续迁移下一个元素。同时，若目标 bucket 没有更多空间容纳被迁移的元素时，则同样在目标 bucket 后挂一个 overflow bucket。第三个步骤执行清除 bucket 操作，即当当前 bucket 已迁移完毕后，且若当前被迁移的 bucket 未被 go routine 使用，则清空旧的 bucket 所存储的 cell 空间。最后，更新 bucket 迁移进度，若所有旧的 bucket 都迁移完成，则清空 map 中包含的旧的 bucket 数组指针和 overflow 指针 。map 渐近式扩容的函数 evacuate 的源码如下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154// 被迁移到新的数组中的目标 bucket 结构type evacDst struct &#123; b *bmap i int k unsafe.Pointer v unsafe.Pointer&#125;func evacuate(t *maptype, h *hmap, oldbucket uintptr) &#123; // evacuate 迁移 oldbucket 索引处的 bucket // 1. 计算需要迁移的 bucket (oldbucket索引) 的地址偏移，同时计算出旧的 bucket 的容量 b := (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) newbit := h.noldbuckets() // 2. 若当前 bucket 未被迁移，则开始执行迁移的逻辑。 // 具体是判断当前 bucket 的首个 tophash 值是否介于 emptyOne 和 minTopHash 中间， // 即处于迁移完毕的状态。 if !evacuated(b) &#123; // 3. evacDst 表示迁移的目标 bucket 结构，因此，x 和 y 分别表示迁移到新的 bucket 数组中的前半段还是后半段。 // x.b 计算出当前被迁移的 bucket b，其对应的在新的 bucket 数组中处于前半段的地址 // x.k 计算出当前被迁移的 bucket b，其存储的 key 对应的在新的 bucket 数组中处于前半段的 cell 地址 // x.v 计算出当前被迁移的 bucket b，其存储的 key 关联的 value 对应的在新的 bucket 数组中处于前半段的地址 // 上述三个计算是关联的，递进式计算求值。 var xy [2]evacDst x := &amp;xy[0] x.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) x.k = add(unsafe.Pointer(x.b), dataOffset) x.v = add(x.k, bucketCnt*uintptr(t.keysize)) // 4. 通过 h.flags 字段判断是否是增量扩容， // 若是，则需计算当前被迁移的 bucket b 放置到新的 bucket 数组后半段的具体情况。 // 类似地，计算 evacDst 包含的三个字段的值。 if !h.sameSizeGrow() &#123; y := &amp;xy[1] y.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) y.k = add(unsafe.Pointer(y.b), dataOffset) y.v = add(y.k, bucketCnt*uintptr(t.keysize)) &#125; // 5. 同样是两层循环，外层循环遍历当前需要被迁移的 bucket b 以及挂在其后的 overflow bucket // 内层循环遍历每个 bucket 的 8 个 cell for ; b != nil; b = b.overflow(t) &#123; // 6. 计算当前 bucket 存储 key 和 value 的起始地址偏移 k := add(unsafe.Pointer(b), dataOffset) v := add(k, bucketCnt*uintptr(t.keysize)) // 7. 遍历当前 bucket 的 8 个 cell for i := 0; i &lt; bucketCnt; i, k, v = i+1, add(k, uintptr(t.keysize)), add(v, uintptr(t.valuesize)) &#123; // 8. 获取对应 cell 的 key 的 tophash， // 若为 empty，则表示其已被迁移，则将其 tophash 标记为 evacuatedEmpty，继续遍历下一个 cell // 同时若当前 key 为指针类型，对当前 key 进行解引用，获取对应的 key 值 top := b.tophash[i] if isEmpty(top) &#123; b.tophash[i] = evacuatedEmpty continue &#125; // 未迁移的 cell 只可能是 empty 或是正常的 tophash（大于 minTopHash） if top &lt; minTopHash &#123; throw("bad map state") &#125; k2 := k if t.indirectkey() &#123; k2 = *((*unsafe.Pointer)(k2)) &#125; // 9. useY 表示当前 bucket 被迁移到新的 bucket 的后半段 // 若当前执行增量扩容，则首先计算 key 的 hash 值。 var useY uint8 if !h.sameSizeGrow() &#123; hash := t.key.alg.hash(k2, uintptr(h.hash0)) if h.flags&amp;iterator != 0 &amp;&amp; !t.reflexivekey() &amp;&amp; !t.key.alg.equal(k2, k2) &#123; // 9.1 需要注意的一种情况是：当扩容的同时存在 go routine 对 map 进行迭代， // 同时，发现相同的 key 值，计算得出不同的 hash 值，这种情况只在 NaN 才出现。 // 因为，所有的 NaN 值都不同。 // 对于这种 key 可以随意对其目标进行分配，而且 NaN 的 tophash 也没有意义， // 但还是给它计算一个随机的 tophash // 同时，公平地把这些 key(NaN) 均匀分布到前半段和后半段 useY = top &amp; 1 top = tophash(hash) &#125; else &#123; // 9.2 通过 hash&amp;newbit 是否为 0，来决定当前 key 被迁移到新的 bucket 数组的前半段还是后半段。 // newbit 为旧的 map 容量大小。 // 事实上这是一个 trick，比如对于旧容量为 16 的情况，即 h.B = 4, // bucket 的映射算法为 maskBucket &amp; hash，即取决于 1111 &amp; hash 值， // 当容量扩大一倍后，可通过其第 B 位为 0 还是 1 来判断迁移到新的 bucket 数组的前半段还是后半段。 if hash&amp;newbit != 0 &#123; useY = 1 &#125; &#125; &#125; if evacuatedX+1 != evacuatedY || evacuatedX^1 != evacuatedY &#123; throw("bad evacuatedN") &#125; // 10. 设置迁移完成的 bucket cell 的状态为 evacuatedX 或 evacuatedY， // 分别表示迁移到新的 bucket 数组中的前半段或者后半段。 b.tophash[i] = evacuatedX + useY // evacuatedX + 1 == evacuatedY dst := &amp;xy[useY] // evacuation destination // 11. 若当前的 tophash 的位置索引超过了 8，则表明当前的 bucket 已经存储满了， // 需要在其后挂上新的 overflow bucket，同时初始化其属性值。 if dst.i == bucketCnt &#123; dst.b = h.newoverflow(t, dst.b) dst.i = 0 dst.k = add(unsafe.Pointer(dst.b), dataOffset) dst.v = add(dst.k, bucketCnt*uintptr(t.keysize)) &#125; // 12. 设置目标 bucket 指定索引处的 tophash 值， // 类似地，若 key 或 value 存储指针值，则将原 key/value 复制到新位置。 // 最后，更新目标迁移 bucket 的属性值，以为后续元素的迁移做准备 dst.b.tophash[dst.i&amp;(bucketCnt-1)] = top if t.indirectkey() &#123; *(*unsafe.Pointer)(dst.k) = k2 // copy pointer &#125; else &#123; typedmemmove(t.key, dst.k, k) // copy value &#125; if t.indirectvalue() &#123; *(*unsafe.Pointer)(dst.v) = *(*unsafe.Pointer)(v) &#125; else &#123; typedmemmove(t.elem, dst.v, v) &#125; dst.i++ dst.k = add(dst.k, uintptr(t.keysize)) dst.v = add(dst.v, uintptr(t.valuesize)) &#125; &#125; // 13. 若没有 go routine 在使用旧的 buckets，则将旧的 buckets 清除掉，减轻 gc 压力 // 即清除掉此 bucket 所存储的 cell(key 和 value) 的空间 if h.flags&amp;oldIterator == 0 &amp;&amp; t.bucket.kind&amp;kindNoPointers == 0 &#123; b := add(h.oldbuckets, oldbucket*uintptr(t.bucketsize)) ptr := add(b, dataOffset) n := uintptr(t.bucketsize) - dataOffset memclrHasPointers(ptr, n) &#125; &#125; // 14. 最后，若此次迁移的 bucket 正好是当前进度值，则更新扩容进度 if oldbucket == h.nevacuate &#123; advanceEvacuationMark(h, t, newbit) &#125;&#125;func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr) &#123; // 1. 更新 bucket 迁移进度 h.nevacuate++ // 2. 尝试往后最多看 1024 个 bucket，在这些 bucket 中找一个还没有被迁移的 bucket，作为当前迁移进度 stop := h.nevacuate + 1024 if stop &gt; newbit &#123; stop = newbit &#125; for h.nevacuate != stop &amp;&amp; bucketEvacuated(t, h, h.nevacuate) &#123; h.nevacuate++ &#125; // 3. 若当前 bucket 迁移进度表示已经扩容完成，则清空旧的 bucket 数组指针， // 以及 overflow bucket 指针，同时，清除表示当前正在进行扩容的标志位。 if h.nevacuate == newbit &#123; // newbit == # of oldbuckets h.oldbuckets = nil if h.extra != nil &#123; h.extra.oldoverflow = nil &#125; h.flags &amp;^= sameSizeGrow &#125;&#125; map 元素删除map 元素的删除函数为 mapdelete，若读者熟悉前面的 map 操作，则 map 元素删除操作也是容易理解的，基本过程和 mapassign 类似，同样会检测是否需要涉及协助迁移 bucket 的操作。具体地，先做一些判断工作，然后根据 key 来定位 bucket 和 cell。然后将对应的 key 和 value 清除掉，同时将 bucket 对应索引位置的 tophash 设置为 emptyOne。最后，更新 map 包含的元素数量，恢复未写状态等。主要区别有两点：其一，找到 cell 之后，需要将对应的 key 和 value 给清除，对应代码如下。 12345678910111213if t.indirectkey() &#123; *(*unsafe.Pointer)(k) = nil&#125; else if t.key.kind&amp;kindNoPointers == 0 &#123; memclrHasPointers(k, t.key.size)&#125;v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize))if t.indirectvalue() &#123; *(*unsafe.Pointer)(v) = nil&#125; else if t.elem.kind&amp;kindNoPointers == 0 &#123; memclrHasPointers(v, t.elem.size)&#125; else &#123; memclrNoHeapPointers(v, t.elem.size)&#125; 其二，考虑到删除一个元素后，当前 bucket 的 cell 可能已全部被清空，甚至当前 bucket （overflow bucket）的前面的 bucket 也为空，则需要依次将这些 bucket 的状态设置为 emptyRest。具体操作逻辑比较简单，这里就不展示相关代码了。最后，同 map 元素的查询、插入或修改类似，map 元素的删除也会根据 key 的具体类型，将其优化成更具体的函数。 map 元素遍历map 元素遍历包含两个关键点。即 map 迭代顺序的原理，以及 map 迭代操作和 map 扩容操作的并发执行过程。 map 的迭代不保证顺序。这在源码中体现为对于某一次迭代操作，其随机初始化迭代初始的 bucket 索引，同时，随机初始化对应的 bucket 中的 cell 的索引。因此，绝大多数情况下，map 的迭代操作是从中间 bucket 索引位置开始的，往后遍历（地址增长方向），当遍历到 bucket 数组末尾时，则会调转方向，重新到 bucket 数组的起始位置开始遍历，直至遍历到开始生成的随机 bucket 索引的位置，说明 bucket 数组已经遍历完成； map 的迭代操作可能同扩容操作并发执行。对于增量扩容而言，bucket 还有可能被迁移到新的 bucket 数组中，也有可能还处于旧的 bucket 数组。这给 map 的迭代操作带来了较大的复杂性。具体而言，当 map 当前迭代的 bucket 处于旧的 bucket 数组时（还未被迁移），则迭代器会指向旧的 bucket 数组中对应 bucket 索引，然后，遍历对应 bucket 包含的 cell，同时，只会输出（返回）那些将被迁移到当前 bucket 的元素，这是因为对于增量扩容的情形，旧的 bucket 中的元素有可能被迁移到新的 bucket 数组中的前半段或者后半段。当将位于旧的 bucket 数组中的对应的 bucket （包括挂在其后的 overflow bucket）所有的 cell 遍历完成后，仍旧会返回到原来的（新的）bucket 数组中，然后，更新当前迭代的 bucket 索引，继续往后遍历。 map 元素的遍历在源码中对应两个函数，其中 mapiterinit 执行迭代器结构的初始化操作，同时调用 mapnext 以获取当前迭代器指向的元素的 key 和 value，同时更新迭代器的状态，指向下一个元素，持续遍历直至遍历完所有元素。mapiterinit 函数的源码如下，删除了不太相关的逻辑。 12345678910111213141516171819202122232425262728293031323334func mapiterinit(t *maptype, h *hmap, it *hiter) &#123; // ... // 1. 若当前 map 为 nil 或长度为0，则直接返回 if h == nil || h.count == 0 &#123; return &#125; // ... it.t = t it.h = h // 2. 将 map 当前的状态复制到迭代器 it.B = h.B it.buckets = h.buckets if t.bucket.kind&amp;kindNoPointers != 0 &#123; h.createOverflow() it.overflow = h.extra.overflow it.oldoverflow = h.extra.oldoverflow &#125; // 3. 这就是每次迭代同一个 map 将打印不同的元素顺序的一个原因。 // 其每次推迭代时，会随机从一个 bucket 索引开始，针对此 bucket 还会随机从其中的一个 cell 索引开始 r := uintptr(fastrand()) if h.B &gt; 31-bucketCntBits &#123; r += uintptr(fastrand()) &lt;&lt; 31 &#125; it.startBucket = r &amp; bucketMask(h.B) it.offset = uint8(r &gt;&gt; h.B &amp; (bucketCnt - 1)) // 4. 设置当前指向的 bucket 索引（初始时，即为随机抽到的 bucket 索引） it.bucket = it.startBucket // 5. 设置当前正处于迭代 map 的过程，多个迭代操作可以并发进行 if old := h.flags; old&amp;(iterator|oldIterator) != iterator|oldIterator &#123; atomic.Or8(&amp;h.flags, iterator|oldIterator) &#125; // 6. 获取当前迭代器指向的元素的 key 和 value，同时更新迭代器的状态，指向下一个元素 mapiternext(it)&#125; mapiternext 函数源码如下，同样删除了部分不太相关的逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139func mapiternext(it *hiter) &#123; // 1. 首先获取迭代器的一些状态值，同时禁止 map 的并发迭代和写入 h := it.h // ... if h.flags&amp;hashWriting != 0 &#123; throw("concurrent map iteration and map write") &#125; t := it.t bucket := it.bucket b := it.bptr i := it.i checkBucket := it.checkBucket alg := t.key.alg // 2. next 标签表示一个 bucket 的迭代过程， // 即当一个 bucket 所有的 cell 遍历完成后，会继续遍历下一个 overflow bucket， // 然后返回到此处，再执行元素的整个定位逻辑。next: // 3. 若 b 为 nil，则表示当前正是迭代的开始（还未遍历任何元素） if b == nil &#123; // 3.1 这个条件表明迭代指针返回到了起始迭代的位置，即整个迭代过程已经完成了。 // it.wrapped 表示已经从头开始遍历了，因为，最开始是从中间的 bucket 开始遍历的。 // 此时，标记 key 和 value 为 nil，表明迭代结束。 if bucket == it.startBucket &amp;&amp; it.wrapped &#123; // end of iteration it.key = nil it.value = nil return &#125; // 3.2 这个条件表明 map 当前正在执行扩容操作。 // 此时，若当前迭代指针所指向的 bucket 未被迁移完成，则需要到旧的 bucket 执行迭代遍历， // 同时，只输出（返回）那些将被迁移到当前（新的） bucket 的那些元素。 // 具体地，它计算出当前遍历的 bucket 在旧的 bucket 数组中的索引 oldbucket， // 然后，计算出对应的旧的 bucket 地址偏移。 // 接下来，通过当前 bucket 的第一个 tophash 值的状态，以判断当前 bucket 是否已经迁移完成。 // 若未迁移完成，则设置 checkBucket 为当前正在迭代的旧的 bucket， // 否则，表示当前 bucket 已经迁移完毕，则还是在新的 bucket 数组中定位到对应的 bucket 地址， // 同时，将 checkBucket 为 noCheck，表示当前迭代的 bucket 不受扩容的影响。 if h.growing() &amp;&amp; it.B == h.B &#123; oldbucket := bucket &amp; it.h.oldbucketmask() b = (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) if !evacuated(b) &#123; checkBucket = bucket &#125; else &#123; b = (*bmap)(add(it.buckets, bucket*uintptr(t.bucketsize))) checkBucket = noCheck &#125; // 3.3 否则，表明当前 map 未执行扩容操作，则直接计算对应的 bucket 地址即可， // 同样，将 checkBucket 为 noCheck，表示当前迭代的 bucket 不受扩容的影响。 &#125; else &#123; b = (*bmap)(add(it.buckets, bucket*uintptr(t.bucketsize))) checkBucket = noCheck &#125; // 3.4 更新当前迭代的 bucket 索引，若当前迭代的 bucket 索引等于 bucket 的数量减一， // 则表示从中间开始遍历 bucket，已经遍历到末尾了，因此重置 bucket 索引，同时设置 wrapped。 // 即表示下一次迭代时，需要从头开始遍历前一部分的 bucket 元素。 // 最后，在每次迭代 bucket 的开始，初始化当前迭代的 cell 的数目 i（不是 cell 的索引）。 bucket++ if bucket == bucketShift(it.B) &#123; bucket = 0 it.wrapped = true &#125; i = 0 &#125; // 4. 设置好当前迭代的 bucket 索引后，就开始循环遍历此 bucket 中的 cell // 注意此循环中的 i 在每次迭代时会递增，而当迭代一个新的 bucket 后，会清零。 for ; i &lt; bucketCnt; i++ &#123; // 4.1 计算当前迭代遍历的 cell 的索引。 &amp; (bucketCnt - 1) 目的是避免溢出。 // 若当前迭代的 cell 的索引处的 tophash 值为空，或者为 evacuatedEmpty， // 表明当前 cell 没有元素，或者当前 cell 被迁移了。 // 因此，继续遍历下一个 cell。 offi := (i + it.offset) &amp; (bucketCnt - 1) if isEmpty(b.tophash[offi]) || b.tophash[offi] == evacuatedEmpty &#123; continue &#125; // 4.2 通过 cell 的索引，计算出对应的 key 和 value， // 同时，若 key 为指针，则解引用。 k := add(unsafe.Pointer(b), dataOffset+uintptr(offi)*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+uintptr(offi)*uintptr(t.valuesize)) // 4.3 此条件表明迭代的 map 正在执行扩容操作，而且是增量扩容。 // 且此时的 bucket 地址对应的是旧的 bucket 地址。 if checkBucket != noCheck &amp;&amp; !h.sameSizeGrow() &#123; // 4.3.1 正常情况下（除了 key 为 NaN）都会走这里。 // 同时，正如前面所说，只遍历出那些将会迁移到新的 bucket 中的元素。 if t.reflexivekey() || alg.equal(k, k) &#123; hash := alg.hash(k, uintptr(h.hash0)) if hash&amp;bucketMask(it.B) != checkBucket &#123; continue &#125; &#125; else &#123; // 4.3.2 否则，表明此 key 为 NaN，因此，正如在 evacuate 方法中所阐述的， // 对于 NaN 元素，在扩容过程中，以随机（50%）的概率被迁移到新的数组的前半段和后半段。 // 因此，这里和之前的逻辑相对应，看其 tophash 的最低位是否为 1， // 若是，则迁移到后半段，否则迁移到前半段。 if checkBucket&gt;&gt;(it.B-1) != uintptr(b.tophash[offi]&amp;1) &#123; continue &#125; &#125; &#125; // 4.4 进入到此条件表明 key 为正常的未被迁移的元素（包括 NaN） // 则直接设置迭代器的 key 和 value 即可。 if (b.tophash[offi] != evacuatedX &amp;&amp; b.tophash[offi] != evacuatedY) || !(t.reflexivekey() || alg.equal(k, k)) &#123; it.key = k if t.indirectvalue() &#123; v = *((*unsafe.Pointer)(v)) &#125; it.value = v &#125; else &#123; // 4.5 执行到此处表明对应的元素已经被迁移，更新甚至删除了， // 因此调用 mapaccessK 来深度获取对应的 key 和 value rk, rv := mapaccessK(t, h, k) if rk == nil &#123; continue // key has been deleted &#125; it.key = rk it.value = rv &#125; // 4.6 更新迭代器当前迭代的 bucket 索引，同时递增当前 bucket 中被遍历的 cell 的数目。 // 设置 checkBucket 的值，因为下一次迭代过程，可能仍旧遍历的是扩容过程中未被迁移的 bucket。 // 然后返回。 it.bucket = bucket if it.bptr != b &#123; // avoid unnecessary write barrier; see issue 14921 it.bptr = b &#125; it.i = i + 1 it.checkBucket = checkBucket return &#125; // 4.7 若执行到这里，表明当前迭代的 bucket 的所有 cell 已经遍历完毕。 // 因此，需要继续遍历下一个 overflow bucket， // 同时，重置 i 为 0，表示当前迭代的 bucket 已遍历的 cell 数目 为 0。 // 随即跳转到 next 标签处。 b = b.overflow(t) i = 0 goto next&#125; 至此，Go map 的几个典型操作的执行逻辑已经阐述完毕。 简单小结，本文围绕 map 介绍了七个方面的内容，其中重点为『map 基本实现原理』、『map 元素查询』以及『map 扩容函数』。七个方面的内容小结如下。 『map 基本实现原理』从 map 的数据结构和扩容原理两个层面概述 map 实现原理，同时，将 Go map 同 Java 的 HashMap 以及 C++ 的 unorderedMap 进行了原理上简单对比，最后介绍了一些关键的数据结构、常量以及 util 函数，这有助于理解后续介绍的 map 的各操作的逻辑。若读者不能读完全文，仅了解此小节的内容也能够了解 map 的大致实现原理； 『map 创建函数』简单介绍了 map 的创建过程，其返回的是 hmap 类型的指针； 『map 元素查询』详细介绍了 map 执行指定元素 key 的查询逻辑，在理解了 bucket、key 以及 value 通过指针地址定位的操作，以及 bucket 和 cell 的映射方法之后，其重点在于遍历两层循环以查找指定的 key 所对应的 value； 『map 元素插入或更新』相较于 map 元素的查询逻辑会更复杂，虽然其也包括两层循环来查找指定的 key 和 value，但考虑到 map 元素插入或更新可能会执行 map 的渐进式扩容操作，因此，在此方法中其会协助迁移至多两个 bucket，一旦执行了迁移操作，则需要重新执行 key 的整个定位过程； 『map 扩容函数』是个较为复杂的过程。因此，将其拆解为 map 扩容的触发条件、map 扩容的准备工作以及 map 渐进式扩容原理这三个部分来阐述。其重点在于渐进式扩容的过程，考虑到对于增量扩容的情况，原 bucket 数组中指定 bucket 元素可能被迁移到新开辟的 bucket 数组的前半段或者后半段； 『map 元素删除』相关逻辑比较简单，同 map 元素插入或更新类似，只不过多了一个清理过程，即将空的 bucket 设置为 emptyRest 状态。 『map 元素遍历』主要包括迭代器初始化以及获取当前迭代器指向元素的 key 和 value，同时更新迭代器的状态以指向下一个元素。其中，获取迭代器指向元素的逻辑较为复杂，因为它也需要考虑迭代操作和扩容操作的并发执行，在这种情形下，对于增量扩容，且当前迭代器指向的 bucket 还未迁移完毕，则需要进入到旧的 bucket 数组中指定 bucket 索引处遍历所有的 cell，并返回那些将会被迁移到对应的新的 bucket 索引的元素。 最后，相信对于本文开头提出的那些疑问，读者心中已经有了解答。『参考文献』部分列出文中涉及的资料出处。 参考文献[1]. https://github.com/golang/go/blob/release-branch.go1.12/src/runtime/map.go[2]. 深度解密Go语言之map[3]. How the Go runtime implements maps efficiently (without generics)]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go slice 最佳实践]]></title>
    <url>%2F2020%2F05%2F23%2Fgo-slice-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[Go 语言切片提供了一种方便、有效且强大的用于处理序列数据的方式。切片和数组都是 Go 内置的数据类型。简单而言，切片类似于动态数组。考虑到 Go 中数组类型和其长度绑定，属于编译期数据类型，操作不够灵活，这导致数组的使用范围非常有限，可以说数组在 Go 中最大的应用就是为切片保存存储空间。相反，切片的应用场景则较广泛，但也较为复杂，切片的运用和程序性能息息相关。因此，本文通过一些示例程序揭示和解决切片使用中相对令人困惑的地方，最重要的是，通过一些 benchmark 阐述和论证切片编程的典型最佳实践。 本文并非切片（slice） 的介绍或使用教程，因此假设你已具备切片和数组的基本知识，同时了解大致底层实现，比如熟悉 slice 的基本操作、清楚slice 的编程使用，而且能基本了解 slice 内部实现原理以及 append 和 copy 的实现机制。了解上述内容后，相信你对切片已经有比较深刻的认识。这些对于掌握切片编程的最佳实践至关重要。本文以 slice 为核心，阐述切片原理中令人隐蔽和棘手地方，同时，通过 benchmark 来论证切片编程的最佳实践。另外，本文所有实验均在 Ubuntu 18.04 x86_64，go1.14.3 linux/amd64 的环境下测试通过。为了节省篇幅，删除了文中示例代码的注释，所有示例代码可以在这里找到。 在阐述重点内容前，先了解关于切片的一些细节，这有助于理解切片是如何工作的，启示我们应该如何准确且高效地使用切片。 当我们在谈论 slice 时，实际上我们指的是 slice header，它包括长度，容量，底层数据的地址三个字段。这说明在 slice 的赋值和函数参数传递操作复制的是 slice header； slice 用于描述和其自身分开存储的数组的连续数据部分。换言之，数组代表 slice 底层数据存储，而 slice 描述数组的一部分； 接上一条，Go 中只有值传递，即显式复制被传递的参数，而没有引用传递。另外，在 Go 中谈论引用类型的概念是不合适的，将其称之为指针持有者类型（pointer holder）会更合适； 当 slice 作为参数传递时，可以改变 slice 的元素，但不能改变 slice 本身。因此，若需要改变 slice 本身，可以将改变后的 slice 返回给调用方，也可以将 slice 的指针作为参数传递； 数组类型和其长度绑定，换言之，相同元素类型但不同长度的数组（包括数组指针）属于不同数组类型，不能相互赋值。而元素类型相同的 slice 肯定是同一种类型； 多个 slice 可能共享同一个底层数组，因此更改其中一个 slice 的底层数组，可能影响其他切片的状态； append 操作在切片容量不够的情况下，会执行扩容操作，扩容会改变元素原来的位置。即， append 操作不一定改变底层数组，因此，append 操作得到的 slice 和原来的 slice 有可能共享底层数组； slice 未提供专门的内置函数用于扩展 slice 容量，append 本质是追加元素而非扩展容量，扩展切片容量是 append 的副作用； slice 的扩容策略包括两个步骤，一是将容量扩充为原 slice 容量的 2 倍或 1.25 倍，二是内存对齐操作。 nil 切片&amp;空切片Go 包含两种特殊状态的切片，即 nil 切片和空切片。这两种特殊状态的 slice 有时候会带来一些棘手的问题，因此，我们我们通过下面简单示例程序来了解它们，更详细的阐述在这里。 12345678910111213141516171819202122232425func TestSpecialStateOfSlice(t *testing.T) &#123; t.Run("TestSSOS", func(t *testing.T) &#123; var s1 []int // nil slice var s2 = []int&#123;&#125; // empty slice var s3 = make([]int, 0) // empty slice var s4 = *new([]int) // nil slice fmt.Println(len(s1), len(s2), len(s3), len(s4)) // 0 0 0 0 fmt.Println(cap(s1), cap(s2), cap(s3), cap(s4)) // 0 0 0 0 fmt.Println(s1, s2, s3, s4) // [] [] [] [] fmt.Println(s1 == nil, s2 == nil, s3 == nil, s4 == nil) // true false false true var a1 = *(*[3]int)(unsafe.Pointer(&amp;s1)) // var a1 = *(*reflect.SliceHeader)(unsafe.Pointer(&amp;s1)) var a2 = *(*[3]int)(unsafe.Pointer(&amp;s2)) var a3 = *(*[3]int)(unsafe.Pointer(&amp;s3)) var a4 = *(*[3]int)(unsafe.Pointer(&amp;s4)) fmt.Println(a1) // [0 0 0] fmt.Println(a2) // [6717704 0 0] fmt.Println(a3) // [6717704 0 0] fmt.Println(a4) // [0 0 0] &#125;)&#125; 上面的代码中，首先需要理解四种初始化的确切含义。其中，内置 new 操作被用来为一个任何类型的值开辟内存并返回一个存储此值的指针， 用 new 开辟出来的值均为零值。因此，new 对于创建 slice (map) 没有太大价值。而 make 操作可用来创建 slice (以及map 和 channel)，并且被创建的 slice 中所有元素值均被初始化为零值。从上面的输出可以看出 nil 切片和零切片的打印内容没有什么差异，但空切片底层数组指向的是一个既定地址——zerobase 。空切片和 nil 比较的结果为 false。且官方建议，为了避免混淆空切片和 nil 切片，尽可能使用 nil 切片。 切片遍历Go 为我们提供 for range操作来遍历容器类型（slice、map和数组）的变量，相比于传统的for循环，for range循环操作有一些让人困惑的地方，同时，Go也在某些场合对for range操作进行了优化。因此，充分理解for range操作能提高容器迭代效率。下面，我们首先阐述for range循环中一些相对难以理解的地方，然后，通过一个 benchmark 来论证for range的最佳实践，最后，我们通过一个 benchmark 以简单比较数组和 slice 的for range操作效率。 range 循环操作的细节我们首先通过下面的代码段了解for range操作的一些“奇怪”特征（输出显示在对应语句旁边）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081type MyInt intfunc TestRangeOfContainer(t *testing.T) &#123; t.Run("TestROC1", func(t *testing.T) &#123; arr := []MyInt&#123;1, 2, 3&#125; for _, v := range arr &#123; arr = append(arr, v) &#125; fmt.Println(arr) // [1 2 3 1 2 3] &#125;) printSeperateLine("TestROC2") t.Run("TestROC2", func(t *testing.T) &#123; arr := []MyInt&#123;1, 2, 3&#125; newArr := []*MyInt&#123;&#125; for _, v := range arr &#123; newArr = append(newArr, &amp;v) &#125; for _, v := range newArr &#123; fmt.Print(*v, " ") // [3 3 3] &#125; fmt.Println() &#125;) t.Run("TestROC3", func(t *testing.T) &#123; type Person struct &#123; name string age MyInt &#125; persons := [2]Person&#123;&#123;"Alice", 28&#125;, &#123;"Bob", 25&#125;&#125; for i, p := range persons &#123; // 0 &#123;Alice 28&#125; // 1 &#123;Bob 25&#125; fmt.Println(i, p) // fail to update the element of original array // because of its duplicate &lt;persons&gt; is provided. persons[1].name = "Jack" // fail to update the field of original array // because of its being a element in its duplicate. p.age = 31 &#125; fmt.Println("persons:", &amp;persons) // persons: &amp;[&#123;Alice 28&#125; &#123;Jack 25&#125;] &#125;) t.Run("TestROC4", func(t *testing.T) &#123; type Person struct &#123; name string age MyInt &#125; persons := [2]Person&#123;&#123;"Alice", 28&#125;, &#123;"Bob", 25&#125;&#125; pp := &amp;persons for i, p := range pp &#123; // 0 &#123;Alice 28&#125; // 1 &#123;Jack 25&#125; fmt.Println(i, p) // this modification has effects on the iteration. pp[1].name = "Jack" // fail to update the field of original array pointer. p.age = 31 &#125; fmt.Println("persons:", &amp;persons) // 1 &#123;Jack 25&#125; persons: &amp;[&#123;Alice 28&#125; &#123;Jack 25&#125;] &#125;) t.Run("TestROC5", func(t *testing.T) &#123; type Person struct &#123; name string age MyInt &#125; persons := []Person&#123;&#123;"Alice", 28&#125;, &#123;"Bob", 25&#125;&#125; for i, p := range persons &#123; // 0 &#123;Alice 28&#125; // 1 &#123;Jack 25&#125; fmt.Println(i, p) // this modification has effects on the iteration. persons[1].name = "Jack" // fail to update the field of original slice. p.age = 31 &#125; fmt.Println("persons:", &amp;persons) // persons: &amp;[&#123;Alice 28&#125; &#123;Jack 25&#125;] &#125;)&#125; 我们简单解释上述测试的结果： TestROC1测试用例中，若 range 操作所遍历的 slice 同初始化声明的 slice 为同一个变量时，则此循环将不会终止。换言之，range 操作所遍历的对象并非原有的 slice 变量； TestROC2测试用例中，若 range 循环提取值 v 为不同（地址的）变量，则后一个 range 循环将打印出原有 slice 的元素列表。换言之，range 循环操作提取的元素 v，其实是同一个变量（地址不变），循环过程只是将 slice 的元素值拷到到此变量； TestROC3测试用例中，循环中对原有数组的更新未能起作用，进一步说明 range 操作所遍历的对象并非原有的数组变量，另外，对原有数组元素的更新未能成功，进一步说明 range 循环操作提取的元素并非原有数组中的元素； TestROC4测试用例中，循环中成功更新原有数组指针，同时结合TestROC3，说明 range 操作所遍历的数组指针是原有数组指针的浅拷贝，同样，对原有数组指针中元素未能更新成功，说明 range 循环操作提取元素并非原有数组指针所指向元素； TestROC5测试用例的结果所得出的结论同TestROC5类似，只不过此容器变量为 slice 类型。 综上所述，可得出如下三个结论： range 操作遍历的容器变量是原有容器变量的一个（匿名）副本。 且只有容器的直接部分（sizeof 函数所计算部分）被复制。数组是值类型，因此，range 操作的是数组的完整拷贝，而 slice 是指针持有者类型，因此，range 操作的 slice 副本，其和原有 slice 共享底层存储； range 操作中的每个循环步，会将容器副本中的一个键和值元素对复制给循环变量，因此，对循环变量的修改不会体现到原容器中，这也说明 range 循环元素元素尺寸较大的数组会带来较大的性能开销； range 操作所遍历的键和值将被赋值给同一对循环变量实例。 range 循环的最佳实践上一小节提到，对具有较大元素尺寸的数组应用 range 操作会导致较低的循环效率，下面通过一个 benchmark 来简单论证。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748type MyInt intconst ( arraySize = 1 &lt;&lt; 15 arrayD2Size = 1 &lt;&lt; 5)func BenchmarkRangeOfArray(b *testing.B) &#123; b.Run("BenchmarkROA1", func(b *testing.B) &#123; arr := [arraySize][arrayD2Size]MyInt&#123;&#125; for i := 0; i &lt; b.N; i++ &#123; for i, v := range arr &#123; _, _ = i, v &#125; &#125; &#125;) b.Run("BenchmarkROA2", func(b *testing.B) &#123; arr := [arraySize][arrayD2Size]MyInt&#123;&#125; parr := &amp;arr for i := 0; i &lt; b.N; i++ &#123; for i, v := range parr &#123; _, _ = i, v &#125; &#125; &#125;) b.Run("BenchmarkROA3", func(b *testing.B) &#123; arr := [arraySize][arrayD2Size]MyInt&#123;&#125; for i := 0; i &lt; b.N; i++ &#123; for i, v := range arr[:] &#123; _, _ = i, v &#125; &#125; &#125;) b.Run("BenchmarkROA4", func(b *testing.B) &#123; arr := [arraySize][arrayD2Size]MyInt&#123;&#125; for i := 0; i &lt; b.N; i++ &#123; for i := range arr &#123; _ = i &#125; &#125; &#125;) b.Run("BenchmarkROA5", func(b *testing.B) &#123; arr := [arraySize][arrayD2Size]MyInt&#123;&#125; for i := 0; i &lt; b.N; i++ &#123; for j := 0; j &lt; len(arr); j++ &#123; _, _ = j, arr[j] &#125; &#125; &#125;)&#125; 我们执行go test -benchmem -run=^$ -bench=^BenchmarkRangeOfArray$ -count 1来执行上面的 benchmark ，其在笔者机器上的测试结果如下。 12345BenchmarkRangeOfArray/BenchmarkROA1 670 1531845 ns/op 0 B/op 0 allocs/opBenchmarkRangeOfArray/BenchmarkROA2 3410 366578 ns/op 0 B/op 0 allocs/opBenchmarkRangeOfArray/BenchmarkROA3 3391 360236 ns/op 0 B/op 0 allocs/opBenchmarkRangeOfArray/BenchmarkROA4 91201 12965 ns/op 0 B/op 0 allocs/opBenchmarkRangeOfArray/BenchmarkROA5 98376 12702 ns/op 0 B/op 0 allocs/op 从测试结果，可得出如下结论： 当 range 直接遍历具有大尺寸的数组元素时，其遍历效率远低于遍历其对应的指针和切片； range 操作遍历数组指针和切片的效率接近，但都低于只提供循环索引变量的情况，同时也低于传统的 for 循环的遍历操作。这说明若在循环中不需要使用到循环变量，则省略它们是好的实践； 当我们将arrayD2Size设置为 0 时（即不需要拷贝循环变量），我们发现所有测试结果都非常接近。另外，读者若有兴趣，可以变化arrayD2Size的大小以观察其对 range 遍历效率的影响。 range 操作的 memclr 优化所谓的 memclr 优化指的是，当我们使用 range 遍历操作来清空一个容器时，即将容器中每个元素赋值为其对应类型的零值字面量，则编译器会将整个循环优化成一个 memclr 调用，这显著提升清空容器的执行效率。下面我们通过一个简单的 benchmark 来比较 memclr 优化和使用传统的 for 循环来清空容器的效率比较。 12345678910111213141516171819202122232425262728293031323334353637type MyInt intconst ( expLimit = 26 incrUint = 2 initExp = 6)func memclr(s []MyInt) &#123; for i := range s &#123; s[i] = 0 &#125;&#125;func memsetLoop(s []MyInt, v MyInt) &#123; for i := 0; i &lt; len(s); i++ &#123; s[i] = v &#125;&#125;func BenchmarkClearSlice(b *testing.B) &#123; for j := 0; initExp+j*incrUint &lt; expLimit; j++ &#123; sliceSize := 1 &lt;&lt; uint(initExp+j*incrUint) b.Run("BenchmarkCS"+strconv.Itoa(j), func(b *testing.B) &#123; sli := make([]MyInt, sliceSize) b.ResetTimer() for i := 0; i &lt; b.N; i++ &#123; memclr(sli) &#125; &#125;) b.Run("BenchmarkCS"+strconv.Itoa(j), func(b *testing.B) &#123; sli := make([]MyInt, sliceSize) b.ResetTimer() for i := 0; i &lt; b.N; i++ &#123; memsetLoop(sli, 0) &#125; &#125;) &#125;&#125; 然后，执行测试命令go test -benchmem -run=^$ -bench=^BenchmarkClearSlice$ -count 1，得到的结果如下： 1234567891011121314151617181920BenchmarkClearSlice/BenchmarkCS-1-64 92657308 11.7 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-64 33621434 34.5 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-1-256 39937538 30.1 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-256 11074920 109 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-1-1024 11525629 104 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-1024 2989976 417 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-1-4096 1785872 653 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-4096 617702 1647 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-1-16384 263352 4584 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-16384 150302 6888 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-1-65536 43773 27638 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-65536 38641 32088 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-1-262144 10000 108601 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-262144 10000 122657 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-1-1048576 2304 484845 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-1048576 2294 537908 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-1-4194304 549 2092379 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-4194304 220 5122413 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-1-16777216 141 8401282 ns/op 0 B/op 0 allocs/opBenchmarkClearSlice/BenchmarkCS-2-16777216 62 20834414 ns/op 0 B/op 0 allocs/op 上面的测试结果论证了 memclr 优化的有效性。同时，我们还可以使用 pprof 工具来具体查看程序中memclr方法和memsetLoop各自具体的 cpu 耗时，具体的使用方法这里就不展开了。读者还可以使用这个测试程序，首先构建它，然后执行程序，它会生成相应的cpu profile，最后执行 pprof 命令来查看生成的报告 go tool pprof memclr_pprof_case -cpuprofile /tmp/profile442700233/cpu.pprof，然后使用 top 命令查看程序中最耗时的那些函数，还可以使用list/disasm &lt;function name&gt;来查看具体耗时。pprof 执行结果如下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849File: memclr_pprof_caseType: cpuTime: May 22, 2020 at 7:10pm (CST)Duration: 12.36s, Total samples = 11.58s (93.71%)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) top 5Showing nodes accounting for 11560ms, 99.83% of 11580ms totalDropped 15 nodes (cum &lt;= 57.90ms)Showing top 5 nodes out of 13 flat flat% sum% cum cum% 8000ms 69.08% 69.08% 8000ms 69.08% main.memsetLoop 3420ms 29.53% 98.62% 3420ms 29.53% runtime.memclrNoHeapPointers 140ms 1.21% 99.83% 140ms 1.21% runtime.futex 0 0% 99.83% 11420ms 98.62% main.main 0 0% 99.83% 3410ms 29.45% main.memclr(pprof) list main.memesetLoopTotal: 11.58s(pprof) list main.memsetLoopTotal: 11.58sROUTINE ======================== main.memsetLoop in /home/ubuntu/workSpaces/go/src/github.com/qqzeng/go-code-snippet/slice-best-practice/memclr_pprof_case.go 8s 8s (flat, cum) 69.08% of Total . . 17: for i := range s &#123; . . 18: s[i] = 0 . . 19: &#125; . . 20:&#125; . . 21:func memsetLoop(s []MyInt, v MyInt) &#123; 6.41s 6.41s 22: for i := 0; i &lt; len(s); i++ &#123; 1.59s 1.59s 23: s[i] = v . . 24: &#125; . . 25:&#125; . . 26: . . 27:func testMemclr() &#123; . . 28: sli := make([]MyInt, arraySize)(pprof) list main.memclrTotal: 11.58sROUTINE ======================== main.memclr in /home/ubuntu/workSpaces/go/src/github.com/qqzeng/go-code-snippet/slice-best-practice/memclr_pprof_case.go 0 3.41s (flat, cum) 29.45% of Total . . 12: arraySize = 1 &lt;&lt; 26 . . 13: loopLimit = 100 . . 14:) . . 15: . . 16:func memclr(s []MyInt) &#123; . 3.41s 17: for i := range s &#123; . . 18: s[i] = 0 . . 19: &#125; . . 20:&#125; . . 21:func memsetLoop(s []MyInt, v MyInt) &#123; . . 22: for i := 0; i &lt; len(s); i++ &#123;(pprof) pprof 给出的结果同我们的 benchmark 接近，memsetLoop的耗时是memclr的两倍左右。 数组和切片的测试比较最后，我们对不同容量的数组和切片进行简单的性能测试。这个测试的主题和 range 操作没有太大联系，只是我们使用 range 循环来为数组和切片进行赋值。benchmark 程序如下，且为了节省篇幅，省略了更大容量数组的测试程序，全部代码可参考这里。 123456789101112131415161718192021222324252627282930313233343536373839404142434445type MyInt intconst ( incrUint = 2)func BenchmarkBasicSlice(b *testing.B) &#123; const incrUint = 3 const expLimit = 28 for j := 0; initExp+j*incrUint &lt; expLimit; j++ &#123; sliceSize := 1 &lt;&lt; uint(initExp+j*incrUint) b.Run("BenchmarkS-"+strconv.Itoa(sliceSize), func(b *testing.B) &#123; for i := 0; i &lt; b.N; i++ &#123; s := make([]MyInt, sliceSize) for i, v := range s &#123; s[i] = MyInt(1 + i) _ = v &#125; &#125; &#125;) &#125;&#125;func BenchmarkBasicArray(b *testing.B) &#123; const incrUint = 3 const ( arraySize6 = 1 &lt;&lt; 6 arraySize9 = 1 &lt;&lt; (6 + incrUint) arraySize12 = 1 &lt;&lt; (6 + incrUint*2) arraySize15 = 1 &lt;&lt; (6 + incrUint*3) arraySize18 = 1 &lt;&lt; (6 + incrUint*4) arraySize21 = 1 &lt;&lt; (6 + incrUint*5) arraySize24 = 1 &lt;&lt; (6 + incrUint*6) arraySize27 = 1 &lt;&lt; (6 + incrUint*7) ) b.Run("BenchmarkA-"+strconv.Itoa(arraySize6), func(b *testing.B) &#123; for i := 0; i &lt; b.N; i++ &#123; a := [arraySize6]MyInt&#123;&#125; for i, v := range a &#123; a[i] = MyInt(1 + i) _ = v &#125; &#125; &#125;) // ...&#125; 执行命令go test -benchmem -run=^$ -bench=^BenchmarkBasic* -count 1，测试结果如下所示。 1234567891011121314151617BenchmarkSlice/BenchmarkS-64 8226044 131 ns/op 512 B/op 1 allocs/opBenchmarkSlice/BenchmarkS-512 1420550 857 ns/op 4096 B/op 1 allocs/opBenchmarkSlice/BenchmarkS-4096 225555 5459 ns/op 32768 B/op 1 allocs/opBenchmarkSlice/BenchmarkS-32768 30286 42308 ns/op 262144 B/op 1 allocs/opBenchmarkSlice/BenchmarkS-262144 3770 331089 ns/op 2097152 B/op 1 allocs/opBenchmarkSlice/BenchmarkS-2097152 236 4803495 ns/op 16777216 B/op 1 allocs/opBenchmarkSlice/BenchmarkS-16777216 40 28614153 ns/op 134217728 B/op 1 allocs/opBenchmarkSlice/BenchmarkS-134217728 2 8294252180 ns/op 1073741824 B/op 1 allocs/opBenchmarkArray/BenchmarkA-64 25120772 46.8 ns/op 0 B/op 0 allocs/opBenchmarkArray/BenchmarkA-512 6202855 195 ns/op 0 B/op 0 allocs/opBenchmarkArray/BenchmarkA-4096 739294 1564 ns/op 0 B/op 0 allocs/opBenchmarkArray/BenchmarkA-32768 101179 12290 ns/op 0 B/op 0 allocs/opBenchmarkArray/BenchmarkA-262144 12030 100248 ns/op 0 B/op 0 allocs/opBenchmarkArray/BenchmarkA-2097152 273 4581004 ns/op 16777216 B/op 1 allocs/opBenchmarkArray/BenchmarkA-16777216 39 29075298 ns/op 134217728 B/op 1 allocs/opBenchmarkArray/BenchmarkA-134217728 2 8289524371 ns/op 1073741824 B/op 1 allocs/op 从测试结果来看，在容量较小时（小于 1 &lt;&lt; 21时，并非准确阈值），因为数组在编译时期已经分配好存储空间，而 slice 则需在运行时动态分配，因此，数组的效率要显著优于 slice 的效率（一倍左右），而当设置非常大的容量之后，二者的速度基本接近，但在笔者机器上运行多次，发现仍然是数组效率占优。读者不妨自己动手试试看。 切片的扩容在阐述 slice 的克隆、插入和删除相关的最佳实践之前，先简单介绍切片扩容操作，前面了解到切片并未提供显式的扩容接口，其中 append 操作可以按照一定的策略自动扩容，切片类似于动态数组，频繁的扩容必然影响程序效率。前文简单介绍了 slice 扩容策略，读者可通过自己阅读源码，或者参考这里以了解具体的扩容策略。 切片扩容的细节先简单了解因为切片的扩容操作导致的一些微妙的问题。我们看如下两段代码。 123456789101112131415161718192021222324252627func TestGrowOfsli(t *testing.T) &#123; t.Run("TestGOS-1", func(t *testing.T) &#123; sli := []int&#123;10, 20, 30, 40&#125; newSli := append(sli, 50) fmt.Printf("Before update sli = %v, Pointer = %p, len = %d, cap = %d\n", sli, &amp;sli, len(sli), cap(sli)) fmt.Printf("Before update newSli = %v, Pointer = %p, len = %d, cap = %d\n", newSli, &amp;newSli, len(newSli), cap(newSli)) newSli[1] += 10 fmt.Printf("After update sli = %v, Pointer = %p, len = %d, cap = %d\n", sli, &amp;sli, len(sli), cap(sli)) fmt.Printf("After update newSli = %v, Pointer = %p, len = %d, cap = %d\n", newSli, &amp;newSli, len(newSli), cap(newSli)) &#125;) t.Run("TestGOS-2", func(t *testing.T) &#123; array := [4]int&#123;10, 20, 30, 40&#125; sli := array[0:2] newSli := append(sli, 50) var pArrayOfSli = (*[3]int)(unsafe.Pointer(&amp;array)) var pArrOfnewSli = (*[3]int)(unsafe.Pointer(&amp;array)) fmt.Printf("Before sli = %v, Pointer Slice = %p, Pointer Array Of Slice = %p, Pointer Array= %p, len = %d, cap = %d\n", sli, &amp;sli, pArrayOfSli, &amp;array, len(sli), cap(sli)) fmt.Printf("Before newSli = %v, Pointer Slice = %p, Pointer Array Of NewSlice = %p, Pointer Array= %p, len = %d, cap = %d\n", newSli, &amp;newSli, pArrOfnewSli, &amp;array, len(newSli), cap(newSli)) newSli[1] += 10 fmt.Printf("After sli = %v, Pointer = %p, len = %d, cap = %d\n", sli, &amp;sli, len(sli), cap(sli)) fmt.Printf("After newSli = %v, Pointer = %p, len = %d, cap = %d\n", newSli, &amp;newSli, len(newSli), cap(newSli)) fmt.Printf("After array = %v\n", array) &#125;)&#125; 执行命令go test -run=^TestGrowOfsli$，测试结果如下所示。 12345678910Before append sli = [10 20 30 40], Pointer = 0xc00000c0e0, len = 4, cap = 4Before append newSli = [10 20 30 40 50], Pointer = 0xc00000c100, len = 5, cap = 8After append sli = [10 20 30 40], Pointer = 0xc00000c0e0, len = 4, cap = 4After append newSli = [10 30 30 40 50], Pointer = 0xc00000c100, len = 5, cap = 8Before sli = [10 20], Pointer Slice = 0xc00000c1c0, Pointer Array Of Slice = 0xc00001c220, Pointer Array= 0xc00001c220, len = 2, cap = 4Before newSli = [10 20 50], Pointer Slice = 0xc00000c1e0, Pointer Array Of NewSlice = 0xc00001c220, Pointer Array= 0xc00001c220, len = 3, cap = 4After sli = [10 30], Pointer = 0xc00000c1c0, len = 2, cap = 4After newSli = [10 30 50], Pointer = 0xc00000c1e0, len = 3, cap = 4After array = [10 30 50 40] 从TestGOS-1的输出结果来看：一方面，此时的 append 操作重新分配底层存储，即新的切片和原切片没有共享底层数组，另一方面，append 操作扩容后的新容量为 8(=2*4)，因此，在修改其中一个切片的元素，另一个切片的的元素并未受到相应影响。 从TestGOS-2的输出结果来看：一方面，此时的 append 操作未重新分配底层存储，两个切片共享底层数组，值得注意的是，此时修改其中一个切片的元素，另一个切片以及底层的数组的元素值也同样被更新。 综上所述，切片的 append 操作产生的结果切片不一定会重新分配底层存储，因此，对结果切片的更新操作也不一定会影响同其共享的切片，以及被共享的底层数组。这需要我们在编程时额外注意。 切片扩容的测试另外，我们通过一个简单的 benchmark 测试下频繁扩容的具体影响，代码如下。 12345678910111213141516171819202122232425262728293031func BenchmarkGrowSlice(b *testing.B) &#123; const innerLoops = 100 const preAllocSize = innerLoops * 5 b.Run("BenchmarkGS-1", func(b *testing.B) &#123; for i := 0; i &lt; b.N; i++ &#123; var s []int for j := 0; j &lt; innerLoops; j++ &#123; s = append(s, []int&#123;j, j + 1, j + 2, j + 3, j + 4&#125;...) &#125; &#125; &#125;) b.Run("BenchmarkGS-2", func(b *testing.B) &#123; b.ResetTimer() for i := 0; i &lt; b.N; i++ &#123; s := make([]int, 0, preAllocSize) for j := 0; j &lt; innerLoops; j++ &#123; s = append(s, []int&#123;j, j + 1, j + 2, j + 3, j + 4&#125;...) &#125; // fmt.Println(cap(s)) &#125; &#125;) b.Run("BenchmarkGS-3", func(b *testing.B) &#123; for i := 0; i &lt; b.N; i++ &#123; s := make([]int, preAllocSize) n := 0 for j := 0; j &lt; innerLoops; j++ &#123; n += copy(s[n:], []int&#123;j, j + 1, j + 2, j + 3, j + 4&#125;) &#125; &#125; &#125;)&#125; 三个测试用例的不同之处在于内存分配以及为 slice 添加元素的方式。执行测试命令go test -benchmem -run=^$ -bench=^BenchmarkGrowSlice$ -count 1，测试结果如下。 123BenchmarkGrowSlice/BenchmarkGS-1 373908 3237 ns/op 12240 B/op 8 allocs/opBenchmarkGrowSlice/BenchmarkGS-2 1000000 1172 ns/op 0 B/op 0 allocs/opBenchmarkGrowSlice/BenchmarkGS-3 1107466 1048 ns/op 0 B/op 0 allocs/op 因为BenchmarkGS-1未提前分配内存，而使用 append 操作自动扩容。因此其在添加大量数据的场景下，会因频繁扩容而效率低下。而BenchmarkGS-2提前分配足够的内存，同时使用 append 操作来添加元素，因此执行添加元素操作时，实际上无需额外分配空间，这使得它的效率要远高于未提前分配内存且使用 append 操作的方式。最后的BenchmarkGS-3和 append 操作的效率类似。因此，可得出结论：实际编程中，对于 append 和 copy 操作，一般而言， append 是应用于未知所需容量大小的场景，而 copy 则是预先知道所需容量，因此，添加元素时只需拷贝元素即可。但即使 append 操作能够自动扩容，若知道所需容量的大小，提前分配足够的初始容量是一种好的实践方式，若不能确定所需要容量大小，分配一个经验值的容量大小也有利于减少 append 扩容操作的次数，提高程序效率。 下文通过一些简单的 benchmark 以介绍切片克隆、删除以及插入切片元素的实践技巧。这部分内容来源于这里，并对它们扩展及测试，以形成对各种操作执行效率的基本认识。这部分详细参考代码在这里。 克隆切片克隆切片元素可以使用 append 或 copy 操作，下面展示了切片克隆的三种具体实现方式。 12345678910111213141516type MyInt intfunc Clone(ori []MyInt) []MyInt &#123; oriClone := append(ori[:0:0], ori...) return oriClone&#125;// if ori is nil, return a non-nil slice.func Clone2(ori []MyInt) []MyInt &#123; oriClone := make([]MyInt, len(ori)) copy(oriClone, ori) return oriClone&#125;// returns nil even if the source slice a is a non-nil empty slice.func Clone3(ori []MyInt) []MyInt &#123; oriClone := append([]MyInt(nil), ori...) return oriClone&#125; 下面是以上三种方式对应的测试程序。 123456789101112131415161718192021222324252627282930const ( expLimit = 26 incrUint = 2 initExp = 6)func benchmarkCloneSlice(b *testing.B, f func(b *testing.B, sz int, cloner func(ori []MyInt) (result []MyInt))) &#123; for j := 0; initExp+j*incrUint &lt; expLimit; j++ &#123; sliceSize := 1 &lt;&lt; uint(initExp+j*incrUint) b.Run("BenchmarkCS-1-"+strconv.Itoa(sliceSize), func(b *testing.B) &#123; f(b, sliceSize, Clone) &#125;) b.Run("BenchmarkCS-2-"+strconv.Itoa(sliceSize), func(b *testing.B) &#123; f(b, sliceSize, Clone2) &#125;) b.Run("BenchmarkCS-3-"+strconv.Itoa(sliceSize), func(b *testing.B) &#123; f(b, sliceSize, Clone3) &#125;) &#125;&#125;func BenchmarkCloneSlice(b *testing.B) &#123; benchmarkCloneSlice(b, func(b *testing.B, sz int, cloner func(ori []MyInt) (result []MyInt)) &#123; sli := make([]MyInt, sz) b.ResetTimer() for i := 0; i &lt; b.N; i++ &#123; _ = cloner(sli) &#125; &#125;)&#125; 执行测试命令go test -benchmem -run=^$ -bench=^BenchmarkCloneSlice$ -count 1，笔者机器上的测试结果如下。 123456789101112131415161718192021222324252627282930BenchmarkCloneSlice/BenchmarkCS-1-64 8705222 126 ns/op 512 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-64 11212414 111 ns/op 512 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-64 11065206 112 ns/op 512 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-1-256 3613802 338 ns/op 2048 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-256 3305574 367 ns/op 2048 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-256 3419361 358 ns/op 2048 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-1-1024 928869 1291 ns/op 8192 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-1024 893323 1671 ns/op 8192 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-1024 774730 1323 ns/op 8192 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-1-4096 274794 4509 ns/op 32768 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-4096 222540 6111 ns/op 32768 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-4096 267927 4381 ns/op 32768 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-1-16384 72402 17123 ns/op 131072 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-16384 50442 21794 ns/op 131072 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-16384 73422 16813 ns/op 131072 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-1-65536 17569 68165 ns/op 524288 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-65536 12440 97966 ns/op 524288 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-65536 17532 68458 ns/op 524288 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-1-262144 3733 302609 ns/op 2097152 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-262144 1802 667451 ns/op 2097152 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-262144 4084 306389 ns/op 2097152 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-1-1048576 1363 882149 ns/op 8388608 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-1048576 468 2566684 ns/op 8388608 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-1048576 1442 860908 ns/op 8388608 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-1-4194304 237 4942850 ns/op 33554432 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-4194304 170 6988974 ns/op 33554432 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-4194304 235 4960112 ns/op 33554432 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-1-16777216 60 19953652 ns/op 134217728 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-2-16777216 43 27613843 ns/op 134217728 B/op 1 allocs/opBenchmarkCloneSlice/BenchmarkCS-3-16777216 61 19484353 ns/op 134217728 B/op 1 allocs/op 从以上性能测试结果来看，第一种和第三种方式的克隆效率接近，都要优于第二种克隆方式。但值得注意的是，第二种克隆方式克隆 nil 切片会得到一个 non-nil 切片，而第三种克隆方式克隆一个空切片会得到一个 nil 切片，这可能不是你需要的，因此，在实际使用时需作额外判断。综上，第一种克隆方式是最简单有效的，它直接从已有切片派生一个 0 容量的新切片。 删除切片元素删除切片元素的一个关键点在于是否需要维持剩余元素的相对顺序，这对执行效率有较大影响。另外，删除切片元素包括删除单个和删除连续范围的切片元素。但事实上，删除单个切片元素是删除连续范围切片元素的一种特殊情况，二者的实现和性能表现都类似。因此这里只展示删除连续范围切片元素的测试结果。 删除单个切片元素删除连续范围切片元素下面展示删除连续范围切片元素的三种具体实现方式，其中方式一和二维持剩余切片元素的相对顺序。 123456789101112131415161718type MyInt intfunc DelRangeInOrder(s []MyInt, from int, to int) []MyInt &#123; tmp := append(s[:from], s[to:]...) return tmp&#125;func DelRangeInOrder2(s []MyInt, from int, to int) []MyInt &#123; tmp := s[:from+copy(s[from:], s[to:])] return tmp&#125;func DelRangeOutOfOrder(s []MyInt, from int, to int) []MyInt &#123; if n := to - from; len(s)-to &lt; n &#123; copy(s[from:to], s[to:]) &#125; else &#123; copy(s[from:to], s[len(s)-n:]) &#125; tmp := s[:len(s)-(to-from)] return tmp&#125; 下面是以上三种删除方式对应的测试程序。 123456789101112131415161718192021222324252627282930313233343536373839404142const ( expLimit = 26 incrUint = 2 initExp = 6)func generateInterval(r *rand.Rand, max int) (int, int) &#123; delIndexL := r.Intn(max) delIndexH := r.Intn(max) if delIndexL &gt; delIndexH &#123; return delIndexH, delIndexL &#125; return delIndexL, delIndexH&#125;func benchmarkDeleteRangeOfSlice(b *testing.B, f func(b *testing.B, sz int, deleter func(sli []MyInt, dl int, dh int) (result []MyInt))) &#123; for j := 0; initExp+j*incrUint &lt; expLimit; j++ &#123; sliceSize := 1 &lt;&lt; uint(initExp+j*incrUint) b.Run("BenchmarkDROS-1-"+strconv.Itoa(sliceSize), func(b *testing.B) &#123; f(b, sliceSize, DelRangeInOrder) &#125;) b.Run("BenchmarkDROS-2-"+strconv.Itoa(sliceSize), func(b *testing.B) &#123; f(b, sliceSize, DelRangeInOrder2) &#125;) b.Run("BenchmarkDROS-3-"+strconv.Itoa(sliceSize), func(b *testing.B) &#123; f(b, sliceSize, DelRangeOutOfOrder) &#125;) &#125;&#125;func BenchmarkDeleteRangeOfSlice(b *testing.B) &#123; benchmarkDeleteRangeOfSlice(b, func(b *testing.B, sz int, deleter func(sli []MyInt, dl int, dh int) (result []MyInt)) &#123; sli := make([]MyInt, sz) r := rand.New(rand.NewSource(time.Now().UnixNano())) b.ResetTimer() for i := 0; i &lt; b.N; i++ &#123; dl, dh := generateInterval(r, sz) _ = deleter(sli, dl, dh) &#125; &#125;)&#125; 执行测试命令go test -benchmem -run=^$ -bench=^BenchmarkDeleteRangeOfSlice$ -count 1，笔者机器上的测试结果如下。 123456789101112131415161718192021222324252627282930BenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-64 19558353 61.2 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-64 18577694 63.6 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-64 17482377 63.7 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-256 16634200 67.6 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-256 17841384 69.6 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-256 17188262 70.2 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-1024 12792736 95.7 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-1024 12631670 92.7 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-1024 15232003 79.5 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-4096 5089140 241 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-4096 5129475 235 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-4096 7656724 159 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-16384 619933 1913 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-16384 630433 1940 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-16384 1000000 1042 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-65536 99871 11927 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-65536 103848 11668 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-65536 198235 6153 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-262144 13483 88173 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-262144 13726 90386 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-262144 42987 28412 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-1048576 2991 350443 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-1048576 3769 347662 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-1048576 8601 133422 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-4194304 789 1515745 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-4194304 826 1612763 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-4194304 2442 530678 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-1-16777216 220 5430883 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-2-16777216 190 6285572 ns/op 0 B/op 0 allocs/opBenchmarkDeleteRangeOfSlice2/BenchmarkDROS-3-16777216 384 2738002 ns/op 0 B/op 0 allocs/op 性能测试结果符合我们的猜想，不用维持剩余切片元素的相对顺序能显著提升性能。注意，三种方式删除 nil 或空切片会 panic。 插入切片插入切片元素同删除切片元素相对应，但不同的是，实际编程中，插入切片元素的实践一般指将一个切片所有元素插入到另一个切片。虽然也有将零散元素插入到切片的情况。但这里只展示将一个切片插入到另一个切片指定位置的测试结果。下面是两种插入方式，它们的根本区别在于一种是一次性分配足够的内存，同时结合 copy 操作，而另一种则直接使用 append 操作。 1234567891011121314151617181920type MyInt intfunc InsertSlice(s []MyInt, elements []MyInt, i int) []MyInt &#123; s = append(s[:i], append(elements, s[i:]...)...) return s&#125;// More efficient but tediousfunc InsertSlice2(s []MyInt, elements []MyInt, i int) []MyInt &#123; if cap(s)-len(s) &gt;= len(elements) &#123; s = s[:len(s)+len(elements)] copy(s[i+len(elements):], s[i:]) copy(s[i:], elements) &#125; else &#123; x := make([]MyInt, 0, len(elements)+len(s)) x = append(x, s[:i]...) x = append(x, elements...) x = append(x, s[i:]...) s = x &#125; return s&#125; 下面是以上两种插入方式对应的测试程序。 12345678910111213141516171819202122232425262728293031const ( expLimit = 26 incrUint = 2 initExp = 6)func BenchmarkInsertSlice(b *testing.B) &#123; for j := 0; initExp+j*incrUint &lt; expLimit; j++ &#123; sliceSize := 1 &lt;&lt; uint(initExp+j*incrUint) b.Run("BenchmarkIS-1-"+strconv.Itoa(sliceSize), func(b *testing.B) &#123; sli := make([]MyInt, sliceSize) sli2 := make([]MyInt, sliceSize/2) r := rand.New(rand.NewSource(time.Now().UnixNano())) b.ResetTimer() for i := 0; i &lt; b.N; i++ &#123; insertIndex := r.Intn(sliceSize) _ = InsertSlice(sli, sli2, insertIndex) &#125; &#125;) b.Run("BenchmarkIS-2-"+strconv.Itoa(sliceSize), func(b *testing.B) &#123; sli := make([]MyInt, sliceSize) sli2 := make([]MyInt, sliceSize/2) r := rand.New(rand.NewSource(time.Now().UnixNano())) b.ResetTimer() for i := 0; i &lt; b.N; i++ &#123; insertIndex := r.Intn(sliceSize) _ = InsertSlice2(sli, sli2, insertIndex) &#125; &#125;) &#125;&#125; 执行测试命令go test -benchmem -run=^$ -bench=^BenchmarkInsertSlice$ -count 1，笔者机器上的测试结果如下。 1234567891011121314151617181920BenchmarkInsertSlice/BenchmarkIS-1-64 2187957 511 ns/op 1615 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-64 4695094 228 ns/op 768 B/op 1 allocs/opBenchmarkInsertSlice/BenchmarkIS-1-256 819127 1337 ns/op 6487 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-256 2078091 513 ns/op 3072 B/op 1 allocs/opBenchmarkInsertSlice/BenchmarkIS-1-1024 301641 4187 ns/op 25864 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-1024 647520 1836 ns/op 12288 B/op 1 allocs/opBenchmarkInsertSlice/BenchmarkIS-1-4096 75786 14610 ns/op 96167 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-4096 137834 8866 ns/op 49152 B/op 1 allocs/opBenchmarkInsertSlice/BenchmarkIS-1-16384 25744 47725 ns/op 348537 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-16384 34798 36605 ns/op 196608 B/op 1 allocs/opBenchmarkInsertSlice/BenchmarkIS-1-65536 6430 175863 ns/op 1372954 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-65536 8308 139352 ns/op 786432 B/op 1 allocs/opBenchmarkInsertSlice/BenchmarkIS-1-262144 1411 899223 ns/op 5491595 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-262144 1154 967825 ns/op 3145728 B/op 1 allocs/opBenchmarkInsertSlice/BenchmarkIS-1-1048576 351 3325487 ns/op 21887040 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-1048576 313 3942896 ns/op 12582912 B/op 1 allocs/opBenchmarkInsertSlice/BenchmarkIS-1-4194304 85 13348969 ns/op 86763014 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-4194304 98 11056438 ns/op 50331648 B/op 1 allocs/opBenchmarkInsertSlice/BenchmarkIS-1-16777216 21 57495901 ns/op 348690919 B/op 2 allocs/opBenchmarkInsertSlice/BenchmarkIS-2-16777216 28 41363541 ns/op 201326592 B/op 1 allocs/op 性能测试结果（注意内存次数分配）依旧证明了一次性分配足够的内存能显著提升性能。 至此，关于 slice 的一些容易让人困惑的细节以及一些典型操作的最佳实践两个部分已经阐述完毕。 简单小结，本文围绕 slice 介绍了两个方面的内容： 关于理解 slice 原理的一些关键点，这些点也相对容易让人困惑。比如 nil 切片和空切片，以及切片的扩容操作； 实际编程使用的涉及 slice 各种操作的最佳实践，包括 for range遍历切片、切片扩容、切片的克隆、往一个切片中插入另一个切片，删除切片的元素。 但值得注意的是，本文提供的 benchmark 都是最简单的情况，因此可能得出的结论并不具有普适性，即实际应用中不能一概而论。但通过这些测试结果，得出的几个基本性结论是值得思考的。有兴趣的读者可以亲自实践并拓展。『参考文献』部分列出文中涉及的资料。 文中实践源码在这里。 参考文献[1]. https://blog.golang.org/slices-intro[2]. https://blog.golang.org/slices[3]. https://go101.org/article[4]. https://juejin.im/post/5bea58df6fb9a049f153bca8[5]. go slice growslice[6]. https://blog.golang.org/pprof[7]. https://github.com/golang/go/wiki/SliceTricks]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go build 命令基本工作过程]]></title>
    <url>%2F2020%2F05%2F19%2Fgo-build-%E5%91%BD%E4%BB%A4%E5%9F%BA%E6%9C%AC%E5%B7%A5%E4%BD%9C%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Go 是一种支持原生并发、带垃圾回收同时编译速度快的语言，这一定程度上归功于其实现了语言自身的 runtime，这类似于 C 运行时 libc。正如官方文档所说，Go 是一种快速的、静态类型的编译语言，以至于让人感觉其就像一种动态类型的解释语言。本文从原理上阐述go build命令具体工作过程，即阐述 Go 项目如何从由语言源码构成的代码文件转换成机器码组成的可执行二进制文件。简单而言，逻辑上，build 命令编译用户指定的导入路径所对应的应用程序包及其依赖包，但并不安装编译的结果。而从执行步骤上看，build 命令执行过程包含编译（compile）和链接（link）两个步骤。 本文假设你已具备 Go 语言入门知识，比如如何搭建 Go 开发环境，同时创建一个 Go 项目并顺利运行起来，在这过程中，你会了解 GOPATH、GOBIN 等重要环境变量的含义，同时也清楚 Go 项目源码的标准组织结构，具体可参考这里。另外，本文并非是语言使用或介绍教程，换言之，本文对于所阐述的内容并不会面面俱到，比如，本文不会详细介绍 build 命令各选项的含义及具体用法，因此你需要提前或同时使用 man 指令或者官方文档来详细了解相关命令。另外，本文所有实验均在 Ubuntu 18.04 x86_64，go1.12 linux/amd64 的环境下测试通过。 以一个实例来剖析 go build 命令执行过程我们以一个简单测试项目 helloworld 来阐述go build命令的具体工作过程，项目组织结构如下所示，我们需要知道的和本文相关的代码逻辑仅仅是main.go引入了 uitl 包。具体地，helloworld 包含程序启动执行入口 main 包源码文件main.go以及 util 包下的工具类源码文件slice_lib.go及其对应的单元测试源码文件slice_lib_test.go，事实上，它们分别对应 Go 项目中所包含的命令源码、库源码、测试源码三类源代码文件。 12345helloworld/├── main.go└── util ├── slice_lib.go └── slice_lib_test.go 然后，我们进入到项目要目录下，执行go build -x -work main.go命令来构建（注意，官方并没有将 build 命令称之为构建，这里我们不准确地将编译和链接两个步骤称为构建，下文也是类似）此项目。其中，-x 选项表示打印构建过程所执行的命令，而 -work 选项表示保留在构建过程中创建的临时文件及目录，以方便我们了解构建的细节，build 命令的更多选项可参考这里，或者使用 man 命令。另外，build 命令后也可以跟隶属单个包的源码文件列表。同时，build 会忽略测试源码文件。最后，当编译包含单个 main 包源码文件时，它以 build 命令后附加的源码文件列表中第一个文件的名称来命名其构建生成的二进制可执行文件，并输出到当前执行命令所在的目录。注意，若 build 后未跟任何源文件或包名（或项目名），则其生成的二进制可执行文件的名称为对应的包名（或项目名）。build 命令输出结果如下所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960ubuntu@VM-0-14-ubuntu:~/workSpaces/go/src/github.com/qqzeng/helloworld$ go build -x -work main.goWORK=/tmp/go-build634787878mkdir -p $WORK/b029/cat &gt;$WORK/b029/importcfg &lt;&lt; 'EOF' # internal# import configEOFcd /home/ubuntu/workSpaces/go/src/github.com/qqzeng/helloworld/util/usr/local/go/pkg/tool/linux_amd64/compile -o $WORK/b029/_pkg_.a -trimpath $WORK/b029 -p \github.com/qqzeng/helloworld/util -complete -buildid T0vxP0OAL4NQLqv8Z5hZ/T0vxP0OAL4NQLqv8Z5hZ -goversion \go1.12 -D "" -importcfg $WORK/b029/importcfg -pack ./slice_lib.go/usr/local/go/pkg/tool/linux_amd64/buildid -w $WORK/b029/_pkg_.a # internalcp $WORK/b029/_pkg_.a /home/ubuntu/.cache/go-build/00/00c7955e923d12630ae21b9150bf5cd83ad34d446a411992f2818bd1953a03e2-d # internalmkdir -p $WORK/b001/cat &gt;$WORK/b001/importcfg &lt;&lt; 'EOF' # internal# import configpackagefile fmt=/usr/local/go/pkg/linux_amd64/fmt.apackagefile github.com/qqzeng/helloworld/util=$WORK/b029/_pkg_.apackagefile runtime=/usr/local/go/pkg/linux_amd64/runtime.aEOFcd /home/ubuntu/workSpaces/go/src/github.com/qqzeng/helloworld/usr/local/go/pkg/tool/linux_amd64/compile -o $WORK/b001/_pkg_.a -trimpath $WORK/b001 -p main -complete - \buildid NQfuV250impAGJ9_g3cW/NQfuV250impAGJ9_g3cW -goversion go1.12 -D \_/home/ubuntu/workSpaces/go/src/github.com/qqzeng/helloworld -importcfg $WORK/b001/importcfg -pack ./main.go/usr/local/go/pkg/tool/linux_amd64/buildid -w $WORK/b001/_pkg_.a # internalcp $WORK/b001/_pkg_.a /home/ubuntu/.cache/go-build/63/63457873f0a007f7287e97b43e05b31477c7f8d1bb0008df9ceaaf10ba9a665a-d # internalcat &gt;$WORK/b001/importcfg.link &lt;&lt; 'EOF' # internalpackagefile command-line-arguments=$WORK/b001/_pkg_.apackagefile fmt=/usr/local/go/pkg/linux_amd64/fmt.apackagefile github.com/qqzeng/helloworld/util=$WORK/b029/_pkg_.apackagefile runtime=/usr/local/go/pkg/linux_amd64/runtime.apackagefile errors=/usr/local/go/pkg/linux_amd64/errors.apackagefile internal/fmtsort=/usr/local/go/pkg/linux_amd64/internal/fmtsort.apackagefile io=/usr/local/go/pkg/linux_amd64/io.apackagefile math=/usr/local/go/pkg/linux_amd64/math.apackagefile os=/usr/local/go/pkg/linux_amd64/os.apackagefile reflect=/usr/local/go/pkg/linux_amd64/reflect.apackagefile strconv=/usr/local/go/pkg/linux_amd64/strconv.apackagefile sync=/usr/local/go/pkg/linux_amd64/sync.apackagefile unicode/utf8=/usr/local/go/pkg/linux_amd64/unicode/utf8.apackagefile internal/bytealg=/usr/local/go/pkg/linux_amd64/internal/bytealg.apackagefile internal/cpu=/usr/local/go/pkg/linux_amd64/internal/cpu.apackagefile runtime/internal/atomic=/usr/local/go/pkg/linux_amd64/runtime/internal/atomic.apackagefile runtime/internal/math=/usr/local/go/pkg/linux_amd64/runtime/internal/math.apackagefile runtime/internal/sys=/usr/local/go/pkg/linux_amd64/runtime/internal/sys.apackagefile sort=/usr/local/go/pkg/linux_amd64/sort.apackagefile sync/atomic=/usr/local/go/pkg/linux_amd64/sync/atomic.apackagefile math/bits=/usr/local/go/pkg/linux_amd64/math/bits.apackagefile internal/poll=/usr/local/go/pkg/linux_amd64/internal/poll.apackagefile internal/syscall/unix=/usr/local/go/pkg/linux_amd64/internal/syscall/unix.apackagefile internal/testlog=/usr/local/go/pkg/linux_amd64/internal/testlog.apackagefile syscall=/usr/local/go/pkg/linux_amd64/syscall.apackagefile time=/usr/local/go/pkg/linux_amd64/time.apackagefile unicode=/usr/local/go/pkg/linux_amd64/unicode.apackagefile internal/race=/usr/local/go/pkg/linux_amd64/internal/race.aEOFmkdir -p $WORK/b001/exe/cd ./usr/local/go/pkg/tool/linux_amd64/link -o $WORK/b001/exe/a.out -importcfg $WORK/b001/importcfg.link -buildmode=exe -buildid=vCNW2zZHTRjAfLGbu0Gw/NQfuV250impAGJ9_g3cW/WU9YqLv9eR4s95poEhRu/vCNW2zZHTRjAfLGbu0Gw -extld=gcc $WORK/b001/_pkg_.a/usr/local/go/pkg/tool/linux_amd64/buildid -w $WORK/b001/exe/a.out # internalmv $WORK/b001/exe/a.out main 下面我们逐步骤阐述 build 命令工作过程。第2-3行创建了临时工作目录 /tmp/go-build545340510/，同时在这个目录下创建了目录b029以用于存放 util 包所对应的源码文件编译后的静态链接文件（归档文件）。然后在4-6行导入编译步骤依赖的静态链接文件，即所编译的源文件引入的依赖库所对应的静态链接文件，因为 util 包没有依赖任何库源码文件，因此这里并没有任何配置内容。 12WORK=/tmp/go-build634787878mkdir -p $WORK/b029/ 123b029├── importcfg└── _pkg_.a 接下来，在第7行进入 util 包所在的目录后，第8-10行使用 compile 命令工具执行一个编译命令以编译库源码文件 slice_util.go，具体地，使用 -o 选项指定编译生成的静态链接文件名_pkg_.a，-p 选项指定编译的源码包，而 -buildid 指定此次构建的 ID，最后 -pack 选项表示编译过程中绕过中间对象文件（后缀为 .o 的文件），而直接生成一个存档文件（静态链接文件），若不指定此选项，则会首先生成以 .o 为后缀的中间对象文件，然后将各对象文件打包到一个存档文件，compile 命令及其详细选项可参考这里。使得一提的是，使用go tool compile -N -l -S main.go可以得到程序的汇编代码。最后，将生成的静态链接文件重命名后拷贝到一个临时的 cache 目录/home/ubuntu/.cache/go-build/00/...。 123456cd /home/ubuntu/workSpaces/go/src/github.com/qqzeng/helloworld/util/usr/local/go/pkg/tool/linux_amd64/compile -o $WORK/b029/_pkg_.a -trimpath $WORK/b029 -p \github.com/qqzeng/helloworld/util -complete -buildid T0vxP0OAL4NQLqv8Z5hZ/T0vxP0OAL4NQLqv8Z5hZ -goversion \go1.12 -D "" -importcfg $WORK/b029/importcfg -pack ./slice_lib.go/usr/local/go/pkg/tool/linux_amd64/buildid -w $WORK/b029/_pkg_.a # internalcp $WORK/b029/_pkg_.a /home/ubuntu/.cache/go-build/00/00c7955e923d12630ae21b9150bf5cd83ad34d446a411992f2818bd1953a03e2-d # internal 第13-25行则针对main.go执行上述类似的步骤。首先它创建b001来存储中间文件，同时在编译main.go命令源码文件之前，导入了其所依赖的三个静态链接文件，即 fmt、util 和 runtime。最后同样将生成的静态链接文件重命名后拷贝到相应的临时的 cache 目录/home/ubuntu/.cache/go-build/63/...。 12345b001├── exe├── importcfg├── importcfg.link└── _pkg_.a 123456789cat &gt;$WORK/b029/importcfg &lt;&lt; 'EOF' # internal# import configEOFcd /home/ubuntu/workSpaces/go/src/github.com/qqzeng/helloworld/util/usr/local/go/pkg/tool/linux_amd64/compile -o $WORK/b029/_pkg_.a -trimpath $WORK/b029 -p \github.com/qqzeng/helloworld/util -complete -buildid T0vxP0OAL4NQLqv8Z5hZ/T0vxP0OAL4NQLqv8Z5hZ -goversion \go1.12 -D "" -importcfg $WORK/b029/importcfg -pack ./slice_lib.go/usr/local/go/pkg/tool/linux_amd64/buildid -w $WORK/b029/_pkg_.a # internalcp $WORK/b029/_pkg_.a /home/ubuntu/.cache/go-build/00/00c7955e923d12630ae21b9150bf5cd83ad34d446a411992f2818bd1953a03e2-d # internal 然后，在26-55行导入了链接阶段所链接的静态链接文件列表。值得注意的是，一方面其自动为命令源码文件生成一个虚拟包 command-line-arguments，另外，还导入了标准库的一些其它静态链接文件。 12345678cat &gt;$WORK/b001/importcfg.link &lt;&lt; 'EOF' # internalpackagefile command-line-arguments=$WORK/b001/_pkg_.apackagefile fmt=/usr/local/go/pkg/linux_amd64/fmt.apackagefile github.com/qqzeng/helloworld/util=$WORK/b029/_pkg_.apackagefile runtime=/usr/local/go/pkg/linux_amd64/runtime.a# ...packagefile internal/race=/usr/local/go/pkg/linux_amd64/internal/race.aEOF 最后，第56行在b001目录下创建了目录exe用于存放链接后的二进制可执行文件，然后在此目录下使用 link 工具执行链接命令，同时通过选项 -extld=external linker（默认为 gcc 或 clang） 来链接 main 包及其依赖的静态链接文件。Go 语言可采用 static linking 或者 external linking 两种链接方式，关于链接的更多内容不在本文的介绍范围。最后将可执行的二进制文件重命名并移动到执行 build 命令的目录。关于 link 命令及其详细选项可参考这里。 12345mkdir -p $WORK/b001/exe/cd ./usr/local/go/pkg/tool/linux_amd64/link -o $WORK/b001/exe/a.out -importcfg $WORK/b001/importcfg.link -buildmode=exe -buildid=vCNW2zZHTRjAfLGbu0Gw/NQfuV250impAGJ9_g3cW/WU9YqLv9eR4s95poEhRu/vCNW2zZHTRjAfLGbu0Gw -extld=gcc $WORK/b001/_pkg_.a/usr/local/go/pkg/tool/linux_amd64/buildid -w $WORK/b001/exe/a.out # internalmv $WORK/b001/exe/a.out main 至此，整个 build 过程已经分析完毕。 go build 的更多细节关于 go build 命令，最后再补充几点。 go build命令在执行时，通常会先递归寻找 main.go 所依赖的包，及其依赖的依赖，直至底顶层的包，同时如果发现有循环依赖，则直接退出。因此，若机器包含有多个逻辑核，则编译代码包的顺序可能会存在一些不确定性，但其会保证被依赖代码包 A 要先于当前包 B（B依赖A）被编译[1]； 前面提到在编译各个包后，compile 命令工具会将编译后的静态链接文件缓存到目录/home/ubuntu/.cache/go-build/..，因此，当我们再次执行 build 命令时（前提是删除之前生成的二进制文件），会发现 build 过程省略了 compile 步骤（若未删除之前生成的二进制文件，则还会省略 link 步骤，即 build 没有任何影响）。具体而言，对于那些被缓存的包所对应的静态链接文件，其在临时目录/tmp/go-build*/下不会生成对应的目录和文件； build 命令执行过程中同缓存相关的还有一个选项 -i，若开启此设置，则会将库源码文件编译生成的链接文件按源码文件中目录组织结构存放到$GOPATH/pkg/${GOOS}_${GOARCH}/目录。因此，当再次执行 build 命令时，同样不会再次编译对应的库源码包文件； 这篇文章没有涉及go install和go run两个命令，因为它们都基于go build命令，因此相对简单。其中 install 命令在 build 的基础上，还会安装编译后的结果文件到指定目录，准确而言是将结果文件（静态链接文件或者可执行文件）存放到相应的目录，这包括两个部分：其一是将各库源码包文件所对应的静态链接文件存放到$GOPATH/pkg/${GOOS}_${GOARCH}/目录，同时将生成的可执行二进制文件存放到$GOPATH/bin/目录（若设置了$GOBIN，则安装到$GOBIN目录）。一般而言，build 命令生成的可执行文件对于应用程序的分发、部署和测试很有帮助，而 install 命令则可以方便你在系统任意地方访问你构建的应用程序。而 run 命令则是在 build 命令的基础上执行二进制文件，并输出运行结果。 本文以一个简单是项目实例阐述go build命令的基本工作流程，并没有涉及过多原理性的知识点。换言之，本文的作用更多地在于使用一个实例来串联其中所涉及的 go tool 命令工具的介绍和使用。因此，读者亲自了解更多的内容并实践才能理解并掌握它们。『参考文献』部分给出了一些详细资料，文中给出的链接基本是官方文档内容[2]。 文中实践源码在这里。 参考文献[1]. https://github.com/hyper0x/go_command_tutorial[2]. https://golang.org/cmd/go/]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Log-Structured File System]]></title>
    <url>%2F2020%2F01%2F09%2FThe-Log-Structured-File-System%2F</url>
    <content type="text"><![CDATA[前面关于文件系统的三篇文章，分别总结了简单文件系统(vsfs)、快速文件系统(FFS)和保证文件系统（以简单文件系统示例）的崩溃恢复一致性(crash consistency)的两种主要解决方案：fsck和journaling。但已有文件系统的文件访问效率仍然远低于磁盘的peek bandwidth，因此伯克利研究人员 John Ousterhout 和他的学生 Mendel Rosenblum 研发了log-structured file system(LFS)，LFS专注于磁盘写操作性能的提升，使其尽量接近于磁盘的peek bandwidth，特别是对于那些存在大量小文件写入和更新的应用场景。顾名思义，LFS是一种日志类型文件系统，磁盘上存储全部是日志，日志包含元数据信息和文件数据块。具体而言，当文件系统发起写操作时，LFS首先将更新的相关数据块（包括元数据信息）以segment为单位缓存在主存，然后按顺序写入磁盘的一块连续区域。LFS不会覆盖或删除磁盘上已有数据，相反它将更新后的数据块（包括元数据）按顺序追加写入到空闲磁盘区域。通过将文件更新带来的随机写操作转化为顺序写segment到磁盘的方式，最大化了写操作效率，因此显著提升了文件系统的性能。从论文中提供的具体数据来看，它将旧文件系统的磁盘访问效率仅有 5% 到 10% 的peek bandwidth提升到 65% 到 75%。 众所周知，vsfs存在严重的磁盘访问效率问题，具体原因是它将磁盘上相关数据结构（如inode和data block，以及同一目录下的文件）分开存储，导致文件访问时，累计大量寻道和旋转操作，严重降低了磁盘吞吐量。而FFS则正是基于文件数据结构访问的空间局部性(space locality)，创造性地引入了cylinder group(block group)，以将文件inode及其关联的data block，以及同一目录下的文件存放于同一block group，避免大量寻道和旋转延时，但考虑到访问相关文件或目录时，它们并不是完全直接相邻，换言之，仍存在一些短的寻道和旋转开销。典型地，FFS创建一个文件需要 5 次写操作。另外，虽然jounraling模式通过顺序写circular log在保证系统能顺利从崩溃中恢复的同时，显著提升磁盘效率，但其存在的问题是，异步的checkpoint操作（将文件数据块和元数据信息真正写入磁盘）仍然存在定位开销。总而言之，已有文件系统的文件访问效率仍然远低于磁盘的peek bandwidth。 FFS的研发存在几个前提或动机：一方面，在文件系统第一篇文章中强调过，磁盘随机写操作速度比顺序写的速度慢大约两个数量级，而且随着技术不断发展，磁盘传输效率越来越高，但其旋转和寻道开销并没有相应地减少，因此二者之间的差距越拉越大；另一方面，同样随着技术成熟，系统主存也不断增长，换言之，原本因不能放入内存而带来的额外磁盘开销的情况已不复存在，主存通过缓存磁盘内容能提供较高的文件读取效率，在这种情况下，文件访问操作由文件的写操作主导；第三，如上所述，已有文件系统在很多应用场景下的性能表现糟糕。这三个前提或动机催生LFS的诞生。 本篇文章同样隶属于总结或启发性的文章，文章内容来源于书本 Operating Systems Three Easy Pieces [1]、LFS原论文[2]，以及一些课程参考资料[3]。因此读者若有兴趣，建议阅读原论文。后文从三个方面来详细阐述LFS，即LFS主要包含的三个部分：一是设计恰当数据结构，以保证高效地检索写入到磁盘的日志；第二，磁盘无用块的回收以备重复用，即垃圾回收(garbage collection)，这一部分相对更为复杂。最后是lfs实现的崩溃恢复(crash recovery)策略。总而言之，LFS思想是简单的，且容易理解，但仅仅了解这些并不够，更需要了解其成为一个完善的文件系统所必须具备的功能——crash recovery，这也是LFS的相当重要的一部分贡献。 从日志中高效检索数据这一部分主题为如何构建日志位于磁盘的详细布局，从而允许我们获得高效的文件检索速率。我们以一种解决抛出问题的方式来阐述（希望这种方式能够让读者不仅能够知道日志构造细节，也能够理解日志如此设计的原因，因为这能帮助我们更多）。 一个简单磁盘日志结构设计首先，既然是将对文件的更新或写入操作转化为日志条目，再顺序写入到磁盘的一段连续空闲区域。那么，这个转化过程具体又是如何进行的？简单地，从一个实例入手，对于创建一个新文件，我们可以在磁盘上顺序写入类似于下图的日志块：它同时将文件的数据块 $D$ 写入到磁盘的$A_{0}$地址处，并将索引数据块$D$的 inode写入到紧跟着$D$后面的位置（注意实际上 inode会比data block小很多）。 如何设计 segment 的大小？但能想象得到，仅仅将日志数据顺序写入磁盘并不能完全保证文件更新操作非常高效，这还取决于你每次顺序写入磁盘的日志大小，换言之，必须一次性写入较大块数据到磁盘才能最大程度均摊磁盘的定位开销。为了实现这一目的，我们引入segment的概念（实际上，segment的概念同样有利于crash recovery的实现，后文会详述）。具体而言，文件系统先将文件更新相关的数据块转化成日志条目并以segment为单位缓存于主存，通过设置合适的segment大小（几 $MB$，论文中是 512$KB$或1$1MB$），能够使得写操作非常高效。我们可以简单进行如下计算。其中$D$表示写入数据块大小，$R_{effective}$表示磁盘实际写速率，$T_{position}$表示一次写操作带来的定位开销（寻道延时和旋转延时），$R_{peak}$表示磁盘的peek bandwidth，最后$F$为预计的磁盘实际写速率和磁盘peek bandwidth的比值。 R_{effective} = \frac{D}{T_{position}+D/R_{peak}} = F * R_{peak} \\ D = F * R_{peak} * (T_{position} + D/R_{peak}) \\ D = (F * R_{peak} * T_{position}) + (F * R_{peak} * D/R_{peak}) \\ D = \frac{F}{1-F} * R_{peak} * T_{position}从以上公式可知，假设定位开销为 10 ms，peek bandwidth为 100 $MB/s$，预计的 $F=90\%$， 那么可计算得，$D=9MB$。同样可计算出当实际的写速率达到peek bandwidth的 $95\%$甚至$99\%$，$D$的理论大小。由此可见，通过设置合适的$D$的大小取值，可使得磁盘实际写速率达到预计值。 Inode Map回到日志结构设计问题，我们在第一小节中按要求设计了最简单的日志结构原型。在vsfs中，inode以数组形式组织并且被写入到磁盘固定位置，如此一来，通过inode编号和inode区域首地址来检索指定inode容易计算得到。类似地，在FFS中，也可通过近似方法计算（仅仅是引入block group）。但这种通过简单计算方式获取inode在LFS却不起作用，考虑到LFS将所有文件的inode和data block分散存储在磁盘上，且无法简单确定最新版本inode地址。 LFS引入inode map来解决此问题。inode map (imap)的作用即作为inode和inode number的中转结构。顾名思义，它大体是一个字典数据结构，其通过inode number来索引对应最新版本的inode地址。但毫无疑问，imap必须持久化以应对系统崩溃情况，因此LFS选择将imap同样作为元信息存储在紧跟着inode的位置，换言之，每一次inode的更新，也会写入一个相应更新后的imap块（可以理解，之所以如此设计是为了尽可能避免磁盘的定位开销）。因此，当写入一个文件时，其写入磁盘日志的内容大概如下图所示。另外，需要强调的是，为了保证检索效率，imap会被缓存到主存，因此当通过一个inode number检索inode时，会先从内存的imap中检索对应inode地址，此后操作的开销同其它文件系统的检索开销相同。 Checkpoint Region虽然通过imap可以方便索引到inode，但问题是如何方便且快速索引到imap呢（可以发现imap并没有固定地址）？因此，我们不得不引入一个具备固定地址的数据结构，通过此数据结构来索引imap，是的，LFS由此设计了Checkpoint Region (CR)。具体而言，CR包含直接索引最新版本的imap指针（其实还存储其它内容，后文阐述），且论文中将CR被设置成以 30s 的时间间隔更新到磁盘，换言之，因更新CR所造成的写操作开销可以接受。因此引入CR后，整个磁盘日志布局相应更新如下图。 事实上，为了应对崩溃恢复，CR设计得更为复杂。但为了逻辑阐述清晰，将相关内容放到后文再阐述。 目录如何处理？众所周知，在所有文件系统中，目录都被当成一种特殊的文件来对待，在LFS中也不例外，一个包含目录更新的简单日志结构如下图。因为目录仅仅是类似于(name, inode number)简单列表。换言之，目录也是通过imap来间接索引，和文件的inode索引没有区别。但正是因为引入了imap才避免了文件更新所存在的recursive update problem[4]，此问题存在于那些不对文件数据块就地更新的文件系统。简单而言，当我们更新一个文件时，事实上文件所在的目录（具体是目录所关联的data block）也需要被更新，类似地，一旦文件所在目录被更新，其父目录同样需要被更新，如此循环，直至根目录。imap的引入可避免此问题，因为此时只需要更新对应的imap即可，文件所在目录只包含了文件的inode number，并不会被级联更新，真正需要更新的只有imap，因此规避了recursive update problem。 至此，关于LFS为保证能高效地从日志中检索数据所做的工作（设计相应数据结构）已阐述完毕。事实上，还有其它用于garbage collection和crash recovery的数据结构未阐述，具体可见如下表（出自原论文）。 最后，阐述LFS从磁盘上读取指定文件的过程以小结此部分内容。首先我们必须从磁盘指定位置读取CR的内容，然后通过CR存储的关联imap指针以读取整个最新版本的imap到内存中。至此，预备工作已经完毕。此时若需根据inode number从磁盘检索相应的inode，LFS首先从缓存的imap中根据inode number查找对应的最新版本的inode地址，以inode地址读取data block的过程同其它文件系统的索引过程类似，即通过direct pointer或者indirect pointer等结构来读取。因此，LFS保证了文件访问操作的主导开销集中在文件写入操作，因为文件读取可通过缓存命中，对比其它文件系统，读操作开销并未加剧。 垃圾回收读者可以注意到，LFS将每次文件更新后相关的数据块以日志形式顺序写入磁盘，这意味着磁盘上可能存放着一个文件的多个历史更新版本数据，但大部分情况下，仅有文件最新版本数据有价值。因此，如何有效清理文件历史版本数据是LFS重点考虑的问题（有些version file system[5]，相反选择保留旧版本文件数据，即通过引入文件历史版本这一功能特性来巧妙解决此问题）。 对于LFS而言，其选择周期性地清理那些旧的无用的文件数据，这包括元信息和数据块等。注意到，LFS使用segment来组织内存中的日志数据，这也使得其能够以segment的形式将日志存储在磁盘上，因此，在周期性清理无用日志数据时，可以以segment为单位清理，以便能够为后续日志顺序写入提供足够大的连续空闲区域（相反，如果以单个data chunk为单位进行回收，会导致有数据区域和无数据区域交叉布局在磁盘空间，因此难以提供连续的大的空闲段）。这就是引入segment的第二个目的。总而言之，LFS的清理过程大致如下：LFS首先从磁盘读入$M$个segment，然后确定其中那些活的(live)block，并将它们压缩成$N(N&lt;M)$个新segment，然后写入磁盘上另外一段连续地址空间，最后将原来$M$个segment直接释放掉，以备后续日志写入。 Segment Summary Block虽然总的垃圾回收的步骤并不复杂，但有几个需要重点考虑的问题：其一，如何确定segment中哪些block是存活的（类似地，哪些是无用的）；另外，也需要标记哪些块隶属于哪些文件，以及其在文件中的偏移量（这些信息是必要的，因为在垃圾回收阶段，你需要更新文件的inode让它指向新的数据块位置）； LFS引入segment summary block来解决这两个问题，segment summary block位于每条segment的开始位置。sumarry block标记了每条写入segment的信息。具体而言，对于每一个数据块，summary block为它记录了inode number和offset，一个segment summary block可包含多条summary block以应对segment存储多条日志的情况。值得注意的是，segment summary block也被用于crash recovery。 segment summary block也被用于识别那些存活块。简单而言，对于位于地址为$A$的数据块$D$，LFS首先从segment summary block中检索出其inode number$N$和offset$T$，然后再去imap中查找$N$所对应的inode的地址，然后读取inode，并通过文件偏移量$T$计算对应的数据块地址是否为$A$，若是，则表示此数据块仍然存活，否则为无用数据块。此过程的一个示例如下图。 其也可用如下伪码表示。 123456(N, T) = SegmentSummary[A]inode = Read(imap[N])if inode[T] == A: # block D is aliveelse: # block D is garbage 另外，LFS使用version number来优化这个标记过程（避免了不必要的读操作）。具体而言，它在每个imap项中为每个文件保存一个版本号，当文件被删除或清空时，版本号会递增。版本号和inode number共同构成文件内容的标识符。因此，segment summary block为每个块记录了对应标识符，若一个块的标识符和当前存储在imap中的标识符不同，则表明此块是无用的。 关于LFS的清理策略，论文中提到有四个方面的问题需要被解决： 何时清理segment。这个问题较简单，可等到磁盘剩余空间低于某一阈值时清理，或者周期性清理； 单次清理segment的数量。显然一次性清理越多segment，空闲磁盘空间越大，但同时也带来更多开销； 哪些segment应该被清理。是否应该选择对碎片化(fragmented)最严重的segment进行清理？ 那些存活的块应该如何被分组以写入到新的segment。一种方案是基于读的space locality，即同一目录下的块被组织到同一segment，另一种方案是基于读的time locality，即上一次修改时间接近的块被安排到同一segment。 LFS并没有详细考虑第一和第二个问题的解决方案，而是采取直接且简单的解决办法。具体地，当LFS发现空闲的segment的数量低于某一阈值（数十个），则开始启动清理程序。而且，它选择每次清理数十个segment直至空闲的segment的数量超过预设阈值（50 到 100 个）。之所以将阈值作为清理条件，是因为这些变量的具体取值和LFS的整体性能并不非常相关。相反，第三和第四个问题则和LFS的性能密切相关。 LFS通过引入write cost来度量清理策略的代价，write cost表示每写一个byte数据而带来的磁盘平均繁忙时间。若write cost取值为 1.0，则表示代价最小，此时没有任何垃圾清理开销，磁盘带宽全部用于写存活块。类似地，若write cost取值为 10.0，则表示只有 1/10 的磁盘带宽用于数据块写入。论文中给出了write cost的计算公式如下，其中$N$表示读入到内存的segment数量，$u$表示block的存活率。 \begin{align*} \text{write cost} &= \frac{\text{total bytes read and write}}{\text{new data written}} = \frac{\text{read segs + wirte live + write new}}{\text{new data written}} \\ &= \frac{N+N*u+N*(1-u)}{N*(1-u)} = \frac{2}{1-u} \end{align*}从公式中，可发现write cost和$u$密切相关，论文中给出具体模拟实验结果[2]。 Segment Usage TableLFS采取一种cost-benefit作为选择被清理的segment的策略，即选择那些具备最大benefit-to-cost比值的segment作为清理对象。其中的清理所带来的benefit包含两个部分，回收空闲空间大小$1-u$和此段回收空间能够维持空闲的时间$age$，且LFS使用segment中block最近修改时间（即最年轻的块）作为此段回收空间维持空闲时间的估计值。而cost同样包含两部分，整个segment读操作的开销$1$和写存活块到空闲区域的开销$u$。综上，benefit-to-cost的计算公式为： \frac{\text{benefit}}{\text{cost}} = \frac{\text{free space generated * age of data}}{\text{cost}} = \frac{(1-u)*\text{age}}{1+u}LFS为了实现cost-benefit策略，引入segment usage table的概念。具体地，对于每一个segment，segment usage table记录了segment中存活块的大小以及每个块的最近修改时间。且segment usage table本身同样被写入到每个segment，其地址同样被记录在CR中。为了计算每个segment的cost-to-benefit的值，只需按照每个存活块的$age$进行排序（因为$u$可以通过存活块大小比上整个segment的大小获得）即可（注意，论文中提到，LFS记录的是整个文件最近被更新的时间，而不是单个块）。 至此，关于LFS日志中无用块的回收策略相关部分已经阐述完毕。总而言之，LFS总的清理过程比较简单，但为了提高清理效率或者提高整个LFS的效率，需要重点考虑两个问题，第一个是如何判断某个segment需要被清理，LFS给出了两种解决办法，其中一种引入了segment summary block，另一种优化措施则采用版本号机制；第二个问题是如何选择哪些segment作为此次清理对象，LFS通过计算每个segment的benefit-to-cost的值来获取最合适的清理对象。 崩溃恢复那么LFS如何处理崩溃恢复呢？崩溃恢复是为了维持磁盘相关数据结构的一致性。旧文件系统必须扫描整个文件系统才能确定在系统崩溃时哪些数据结构发生了哪些变更。上一节的崩溃一致性和日志的主题为提高崩溃恢复效率。而对于LFS而言，无论系统何时崩溃，其导致的数据结构变更只能位于日志末尾位置，因此，崩溃恢复过程的执行更为迅速（这一特性也被应用于数据库管理系统和其它文件系统）。 具体而言，我们知道，大体上，写入磁盘的日志包括两个部分：CR和segment。换言之，我们只需要考虑这两个结构写入磁盘的过程中发生系统崩溃时，应该分别如何处理。LFS使用checkpoint和roll-forward来分别解决这两个问题。前者定义了磁盘某一时刻的一致性状态，而后者用于恢复最后一次checkpoint后又执行的更新操作所带来的变更。 在LFS的checkpoint操作包括两个阶段：首先将更新的数据写入磁盘（包括data block、inode、imap、segment summary block和segment usage table）；其次，将CR写入到磁盘固定位置（以 30s 为周期写入），CR中包含了imap、segment usage table和指向最后一个segment的指针的地址，以及写入时间戳。 Checkpoint为保证CR写入磁盘的原子性，LFS采取两个方法，一个是CR实际上会被交替写入到磁盘两端，当需要读取CR时，选择具有最新完整时间戳的版本；另外，在CR包含区域的首尾加入写入时间戳，因此一旦写入CR时系统崩溃，则读取此CR记录时，不可能获得两个相同的写入时间戳。因此正常情况下，在系统重启后，通过读取CR的内容，可以获得最后一次checkpoint操作时的imap等数据结构的地址信息，因此可顺利重构出最后一次checkpoint操作时的全局日志结构。 Roll-Forward理论上，若用户只需粗糙地恢复磁盘数据，则通过checkpoint来恢复即可满足要求。但其不足之处在于，它忽略了最后一次checkpoint操作后对磁盘执行的变更，这在某些情况下是不能接受的，因此，LFS采用roll-forward的方式来进行细粒度的崩溃恢复。roll-forward会利用保存在segment summary block中的信息来恢复最近的更新操作。具体地，若segment summary block中保存的信息表明一个新的inode的存在，LFS则更新imap以指向最新的inode。另外，roll-forward也会调整segment usage table记录的segment的利用率$u$。roll-forward所解决的最后一个问题是目录中目录项和对应inode的一致性问题，因为在更新inode的reference count和写入数据这两个操作之间可能发生系统崩溃。为了顺利恢复目录和inode的一致性，LFS在每条日志中额外增加了一条被称为directory operation log的记录，且LFS确保在每条日志中的directory opearation log会出现在对应的目录数据块或inode块之前。具体细节读者可参考原论文[2]。 至此，关于LFS如何提供崩溃恢复功能的内容已经阐述完毕。简单而言，LFS通过两种手段以从粗粒度和细粒度双管齐下地执行崩溃恢复。具体而言，通过巧妙设计CR的数据结构，崩溃恢复过程中的简单checkpoint操作即能保证系统恢复到崩溃前的最后一次checkpoint时刻的一致性状态；然后再通过roll-forward从最后一条segment开始实施文件级别的恢复操作。 简单小结。本文从三个方面简单介绍了LFS。LFS的核心思想容易理解，将对于磁盘上文件更新所带来的随机写入操作转化为批量的按顺序以segment为单位写入磁盘的操作，以尽可能接近磁盘带宽来执行写入操作（这种技术在数据库领域被称为是shadow paging，而在文件系统领域则被称为是copy-on-write）。关于LFS，我们首先需要清楚LFS诞生的前提或实现的动机；其次，对于一个文件系统，需要保证能够高效地检索文件数据内容。因此理解LFS如何组织文件相关的元数据和数据块信息至关重要；另外，考虑到LFS并未直接更新磁盘上的数据块，而采用将更新转化成日志的形式顺序写入磁盘的方式，因此，必须解决由此产生的无用的历史版本文件数据的问题。最后，崩溃恢复是一个文件系统必备的功能，LFS从粗粒度和细粒度两个角度分别使用checkpoint和roll-forward来解决崩溃恢复的问题。 参考文献[1]. Arpaci-Dusseau R H, Arpaci-Dusseau A C. Operating systems: Three easy pieces[M]. Arpaci-Dusseau Books LLC, 2018.[2]. Rosenblum M, Ousterhout J K. The design and implementation of a log-structured file system[J]. ACM Transactions on Computer Systems (TOCS), 1992, 10(1): 26-52.[3]. http://www.eecs.harvard.edu/~cs161/notes/lfs.pdf[4]. Zhang Y, Arulraj L P, Arpaci-Dusseau A C, et al. De-indirection for flash-based SSDs with nameless writes[C]//FAST. 2012: 1.[5]. Hitz D, Lau J, Malcolm M A. File System Design for an NFS File Server Appliance[C]//USENIX winter. 1994, 94.]]></content>
      <categories>
        <category>单机存储</category>
      </categories>
      <tags>
        <tag>单机存储</tag>
        <tag>文件系统</tag>
        <tag>日志文件系统</tag>
        <tag>崩溃恢复</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单文件系统的崩溃一致性和日志]]></title>
    <url>%2F2020%2F01%2F07%2F%E7%AE%80%E5%8D%95%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%B4%A9%E6%BA%83%E4%B8%80%E8%87%B4%E6%80%A7%E5%92%8C%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[上一篇文章从早期 unix 系统所使用的文件系统存在的典型问题切入，介绍The Fast File System三个方面的工作：基于数据访问的局部性，优化了磁盘数据块布局方式以提升磁盘效率，同时允许使用更灵活的数据块大小以提升磁盘利用率和传输速率，最后在兼容已有文件系统接口规范的前提下，通过增强已有功能以进一步提升系统易用性。本文主题是文件系统的崩溃一致性(crash consistency)和日志(journaling)，文件系统的crash consistency问题可以通过若干种方式来解决，其中典型的两种方案就是file system checker(fsck)和journaling。所谓的crash consistency问题指的是在文件系统更新磁盘数据结构的过程中发生某种软硬件问题而使得更新被迫中断，最终导致磁盘被更新数据结构处于不一致状态。其中fsck是早期文件系统使用的恢复策略，是一种lazy的修复策略，存在严重的效率问题。相反，journaling则是更为active的解决方案，也是一种广泛被使用的解决方案。 同前两篇文章类似，本文是一篇总结性的文章。本文的目标为：使用一个简单文件系统作为原型，来深入了解crash consistency问题本身，以及能够解决它的各种方法技术具体的工作原理。这对于理解一个文件系统的crash recover有较大帮助。本文首先从一个简单的crash consistency问题实例切入，即先将问题抛出，然后分别介绍它的各种解决办法，早期的文件系统使用fsck在重启文件系统过程中校验并恢复整个文件系统，主流的解决办法是使用journaling或者称为write-ahead logging，其只需在写入数据的同时花费额外的一点写开销，就能使得文件系统后期高效恢复。 Crash Consistency Problem所谓的crash consistency也被称为是consistent update。问题产生的本质原因是：文件系统对磁盘发出的更新操作通常是一个复合操作，即磁盘无法通过一次写入操作就能完成整个更新请求，相反，它需要依次更新多个相关数据结构，即多次写入磁盘才能顺利完成请求。在这多个依次写入的操作中间，系统发生了某种软件或硬件方面的错误（如突然断电power loss或者系统崩溃system crash），使得此次更新请求未能顺利完成，即只更新了部分数据结构，这就导致被更新的数据结构处于一个不不一致的状态，这可能导致很多问题，典型包括space leak、garbage data和inconsistent data structures三种 。 我们使用vsfs作为原型，通过一个简单实例来阐述crash consistency问题。假设目前磁盘上的数据块布局如下，只有一个文件 foo，其inode编号为 2，且我们想往文件 foo 追加数据内容。 从这里我们可以了解到更新一个磁盘上的文件包含了对磁盘上相关数据结构的一系列更新操作，在这个实例中，需要更新对应的inode（增加一个直接指针，以及更新文件大小、访问和修改时间等属性），写入新的数据块 Db，以及更新d-bmap（标记 Db 已被写入）。下图为写入期间未发生任何错误的情况下，更新后磁盘布局。 若不幸在整个更新过程中发生了错误（一般而言，文件系统通过page cache或buffer cache来缓存对磁盘的更新，并周期性的实施刷盘操作，但同步刷盘操作中也可能被中断），那么磁盘中关于文件 foo 的状态可能有如下两种情况： 仅顺利更新一个数据结构。那么进一步存在三种可能的状态： Db 被顺利写入。事实上，此次写入是无效的，因为没有inode能索引到 Db，也没有任何d-bmap记录了 Db 所在数据块已被占用，因此后续其可能被其它数据覆盖； inode被顺利更新。在这种情况下，虽然文件元数据所包含的指针被更新，但其指向的是garbage data，且d-bmap也没有任何记录对应的数据块被占用，因此整个文件系统处于一个不一致的状态，必须对它进行修复后，文件系统才能继续工作； d-bmap被顺利更新。同上一种情况类似，整个系统同样处于一个不一致的状态。且原本被 Db 占用的空间不会被任何数据填充，造成了磁盘space leak。 仅顺利更新两个数据结构。同样存在三种可能的状态： inode和d-bmap都被顺利更新。此时问题很简单，造成了文件系统后续将读入garbage data，尽管在文件系统看来，整个系统处于一致状态； inode被顺利更新，且 Db 也被顺利写入。此时文件系统处于不一致的状态，需修复后才能继续工作； d-bmap被顺利更新，且 Db 也被顺利写入。文件系统同样处于不一致的状态，虽然数据被成功写入，且也被成功标记，但文件系统却不知道 Db 是隶属于哪个文件。 从上面简单分析可以得出，文件系统的崩溃可能导致多种问题，具体包括space leak、garbage data和inconsistent data structures，其中后两种问题可造成严重后果。 The File System Checkerfsck是早期文件系统所采用的崩溃恢复手段，详情可参考文献[1]。它的工作原理很简单：允许不一致的情况发生，但在每次系统重启时，文件系统被挂载前，对它进行修复。但需明确的是，fsck并不能修复所有（上述三种）问题，典型的，在发生garbage data情况下，其根本无法辨别出数据块是否为garbage data。换言之，fsck只能解决文件系统存在的不一致问题。fsck会对多个方面实施校验检测，具体包括superblock检测（比如检测其完整性，否则使用其副本），free blocks检测（通过遍历所有数据块临时性重构d-bmap以检验其同inode之间是否保持一致），inode state检测（校验indoe完整性，否则清除inode及相关数据结构），inode links校验（通过临时计算出inode的链接数来校验为inode保存的链接数属性是否准确），最后也会检测duplicate pointers、bad blocks以及directory，详情可参考原文。 fsck存在的问题很明显，除了不能解决garbage data的问题外，更让人不能接受的是，修复所耗费时间过长，这在文件系统日渐增长的情况下显得尤为突出。因此，研究人员也在探索其它解决方案。 Journaling (Write-Ahead Logging)考虑到fsck存在的问题，研究人员对于文件系统的崩溃一致性的解决方案采取了同数据库管理系统类似的办法——write-ahead logging，在文件系统中被称为journaling。所谓的write-ahead logging直译为预写式日志，即在向磁盘写入实际的数据之前，先额外写一些数据（也被称为日志记录log）到磁盘指定区域，然后再更新磁盘写入实际数据。如此一来，若在磁盘更新的过程中发生了系统崩溃，则可读取之前写入到磁盘指定区域的log，以推断出被中断的更新操作的详细内容，然后针对性地重新执行更新操作。此种方案避免了对整个文件系统的全盘扫描，因此理论上提高了崩溃恢复效率。ext3正是在早期ext2文件系统之上进一步整合了journaling机制。并且假设将write-ahead log存储在同一磁盘或分区上（当然其也可存储在其它设备上）。 Data Journalingdata journaling是一种简单的journaling方式，是ext3文件系统提供的一种journaling模式。顾名思义，它会将被更新后的完整数据块连同被更新的元数据信息写入到write-ahead log中。简单而言，同样以向 foo 文件追加数据操作作为示例，data journaling模式会依次写入TxB，I[v2]、B[v2]和TxE，这四个数据结构共同构成了一条完整的data journaling log。其中TxB标记一条新的日志记录的开始，它一般会包含transaction id和被更新的 foo 文件所关联的数据结构的地址信息（即I[v2]、B[v2]和Db）；TxE标记着日志记录的结束，一般同样会包含transaction id。而中间的三个数据块即为需更新到磁盘的相关数据块。考虑到此种journaling方式将被更新的数据块也写入到日志中，因此也称为是physical logging，其示意图如何所示。相对地，也有对应的logical logging模式，它不将具体的数据块也纳入到日志，这种方式后文会阐述。 因此，当采用data journaling模式，整个更新过程可划分为两个阶段： Journal write，即先写日志，将日志内容写入到磁盘指定区域，包含上述data journaling日志组成的 5 个部分； Checkpoint，即表示将需要追加或更新的文件数据内容真正地更新到磁盘对应的数据块中。 读者可能很快会发现，这种模式存在一个问题：若在写入日志的过程中，系统崩溃，那么可能会发生意想不到的事情。考虑到文件系统一般会缓存 IO 请求以提升写性能，换言之，文件系统可能在合理的任意时间点以任意顺序写入data journaling日志记录所包含的 5 个部分。下面是一种典型的情况，即磁盘调度器可能会先将TxB、I[v2]、B[v2]和TxE先写入到磁盘，然后在未来得及写入Db时，发生了系统崩溃，那么，此时写入磁盘的日志状态如下图所示。 在崩溃恢复阶段，当读入这条日志记录时，可能引起严重错误，因为 Db 的数据块内容是任意garbage data。因此，为了应对该问题，文件系统采用一种类似于“二阶段提交”的方式来写入日志：它先写入除TxE结构之外的日志部分，等到它们真正被写入磁盘后，再接着写入TxE结构到磁盘。如此一来，一方面，若在写入第一部分的日志记录时发生了系统崩溃，那么此条日志记录是不完整的，在文件系统重启执行崩溃恢复时，会将此条日志记录视为非法，因此不会导致任何不一致的状态。另一方面，考虑到TxE结构一般较小，不足一个扇区大小，因此其写入操作不会发生torn write，换言之，日志记录的第二阶段的写入也是原子性的。总而言之，通过这种类似于“二阶段提交”式的日志写入，整个文件写入过程可进一步划分为三个阶段： Journal write，同样先写日志，只不过先要确保除TxE结构之外的日志部分先写入到磁盘； Journal commit，进一步写入TxE，以确保整条日志记录写入的原子性； Checkpoint，最后才将需要追加或更新的文件数据内容真正地更新到磁盘对应的数据块中。 在顺利为每个更新操作写入data journaling日志记录后，一旦发生任何系统崩溃情况，则在文件系统恢复过程中：一方面，若是在日志记录本身的写入过程中发生了系统崩溃，此时日志记录并不完整，因此恢复程序应当直接跳过，不会造成系统任何不一致现象。另一方面，若是在checkpoint阶段发生了系统崩溃，则只需要读取并解析对应日志内容，然后实施日志replay即可重新尝试将更新持久化到磁盘，这种日志类型在数据库管理系统中被称为redo log。 下图同样表示在对文件的更新过程中，使用data journaling策略来保证crash consistency，和文件相关的磁盘数据结构写入的相对顺序，图中往下表示时间增长，虚线隔开的各个操作表示它们之间必须严格按照相对顺序执行，而同为虚线框内的一组操作表示它们之间的执行顺序是任意的。 最后补充一点。有读者可能会想，既然为了避免日志写入发生torn write，那么是不是可以将构成日志的 5 个部分依次按顺序写入磁盘，且保证前一个部分成功写入磁盘后，才写入后一个部分。理论上，这种方式没有任何问题，但其代价也显而易见，大幅度降低文件系统性能，因为它使得文件系统的写缓冲没有任何作用。基于此，ext4文件系统采用了一种更加优雅的解决方案，为了同时保证日志写入的高效以及写入的原子性，在每条日志记录的开始和末尾处增加了日志记录中数据块的checksum，同时，在崩溃恢复程序读取日志时，会临时计算数据块的checksum，同时对比保存在日志记录中的值，若二者不等，则表明日志记录是不完整的。否则，证明日志记录确实是完整的，即日志记录在写入过程中未发生系统崩溃。详细内容可参考文献[2]。 Batching Log Updatesdata jounaling模式能基本解决crash consistency问题，其不足之处在于日志记录包含了实际数据块，因此占用了较多额外空间。另外，恢复过程还会引入部分redundant write。针对redundant write的问题，我们考虑这样一个场景：同一个目录下的两个文件 foo1 和 foo2 依次进行更新，在这里我们知道更新目录下的一个文件，至少需要更新的磁盘数据结构包括i-bmap、inode、目录所关联的data block以及目录的inode，因此，这些信息全部需要作为日志内容写入到磁盘，那么 foo1 和 foo2 的更新则需要重复写入目录的inode及其关联的data block，因此会导致较多写操作开销（考虑当更新同一个目录下的多个文件或目录的情况）。针对这一问题，ext3文件系统采用batch update策略来解决。具体而言，它会先将文件的更新进行缓存，并标记对应的日志记录需要存盘，当发现同一目录中其它文件也需要更新时，会将对应的日志记录合并到前一文件所对应的日志记录的数据块中，最后当达到刷盘周期时，将包含多个文件更新的日志记录一次性写入磁盘。而针对前一个问题，可以使用metadata journaling模式来进行优化。 Making The Log Finite上一小节提到的data journaling方式不仅存在占用过多额外磁盘空间问题，而且也会增加崩溃恢复过程的耗时。其解决方式比较直接，通过日志循环写入(circular log)配合日志释放来解决。具体而言，将存储日志的磁盘区域作为一个环形数组即可，环形数组的首尾指针即为没有被释放的日志记录边界，为了方便，可以将这两个指针存储在journal superblock中。如下图所示。 为了实现circular log，需在每次成功文件更新的操作后，即时将日志区域中对应条目释放掉（即更新circular log的首尾指针）。通过引入circular log后，为了保证crash consistency，更新文件的整个过程可扩展为如下四个阶段： Journal write，写入除TxB结构之外的其它日志记录内容； Journal commit，进一步写入日志的TxB结构，至此，完成了日志记录的原子性写入； Checkpoint，将文件更新或追加的数据真正写入到磁盘； Free，释放掉步骤 1 和 2 中写入的日志记录，以备后续空间复用。 至此，data journaling模式已较为完善，但其仍存在的问题是：每一个被更新的文件，其更新数据块需要被写入两次。 Metadata Journaling事实上，使用data journaling模式必须将更新数据块写入两次所带来的问题具体表现为：在写入元数据和实际数据之间的寻道操作开销较大。因此，进一步实现了metadata journaling(ordered journaling)来解决此问题。容易想到，metadata journaling只将被更新文件的元数据信息写入日志记录，如下图所示。 但若只纯粹将被更新的数据块从日志记录中移除，而不对文件更新的总规则进行调整，将引发一个额外的问题。我们不妨考虑何时将更新文件数据块真正写入磁盘，若仍旧按照之前的规则，即在将被更新文件的元数据信息写入日志记录后，才写入更新文件的数据块，则可能导致的问题是：假设在将更新文件的数据块写入磁盘时发生了系统崩溃，那么此时日志记录中包含的元信息索引的数据块实际上是无效的garbage data。这导致即使实施了崩溃恢复，且在文件系统的视角看来，整个文件系统确实处于一致状态，但文件中却包含garbage data。因此，为了避免这种情形发生，ext3文件系统选择先将被更新文件的数据块真正写入到磁盘，然后再写日志记录（二个阶段），最后 再将被更新文件的元数据信息真正写入磁盘。通过将数据块写入的顺序调整到日志中元信息的写入操作之前，避免了日志记录中的元信息引用了无效的数据块内容。总而言之，调整后的文件更新规则如下： Data write，将被更新的文件的数据块真正写入到磁盘； Journal metadata write，只将被更新的文件的元信息相关的数据块以及TxB构成的日志记录写入到磁盘； Journal commit，进一步将此日志记录的TxE结构写入磁盘； Checkpoint metadata，将被更新文件的元信息相关的数据块真正写入到磁盘； Free，释放掉步骤 2 和 3 中写入的日志记录，以备后续空间复用。 需要注意的是，步骤 1 和 2 的相对顺序可以任意（即可并发执行），只需确保步骤 3 在 1 和 2 之后完成即可。 同data journaling类似，下图同样表示在对文件更新过程中，使用metadata journaling模式来保证crash consistency，和文件相关的磁盘数据结构写入的相对顺序，图中往下表示时间增长，虚线隔开的各个操作表示它们之间必须严格按照相对顺序执行，而同为虚线框内的一组操作表示它们之间的执行顺序是任意的。 Tricky Case: Block Reuse关于使用journaling来保证文件更新操作的crash consistency的主要内容已阐述完毕。这一小节简单阐述在磁盘数据块被重用时可能出现的棘手问题。如下图所示，foo 表示一个目录，当我们在目录 foo 下创建一个文件时，若采用metadata journaling模式，其磁盘数据块布局可简化如下。注意，因为目录所包含内容也被视为元数据，因此会被记录到日志中。 此时若用户删除目录 foo 中所有内容以及目录自身，则原本被目录所占用的日志记录空间被释放以备复用。最后，若用户又创建新文件 bar，且文件系统恰好将 bar 文件相关内容写入到原本属于目录 foo 所在磁盘空间。若文件系统成功写入对文件 bar 的日志记录，此时用于记录日志的磁盘布局如下图所示，且文件 bar 也被成功写入磁盘（即成功checkpoint），而后某个时间点发生了系统崩溃，那么在执行replay过程中，当读取到关于目录 foo 的日志记录时，它会直接简单地重新将目录 foo 的内容写入到磁盘中已经被 bar 文件关联的data block占用的空间，因此覆盖了 bar 文件数据，导致用户读取 bar 文件时，产生意想不到的后果。 此种情况的原因很简单：未恰当地处理数据块重用。ext3文件系统的处理方式比较简单，它会将那些被删除文件或目录的日志记录标记成revoke，且在崩溃恢复过程中，当扫描到包含revoke的日志记录时，不会对此日志记录执行replay，因此避免了数据块覆盖的情况。 Other Approaches除了fsck和journaling这两种能够保证文件系统crash consistency的方法外，还有一些其它的解决方案。其中一种被称为Soft Updates，它的基本原理是将所有文件的更新操作请求进行严格排序，并且保证磁盘对应的数据结构不会处于不一致的状态，比如先写文件数据内容，再写文件元信息，以保证inode不会关联一个无效的数据块。但是Soft Updates实现起来比较复杂，需要充分了解文件系统内部相关数据结构知识，有兴趣的读者可参考文献[3]。另一种解决被称为是copy-on-write(cow)的方案则更为流行，我们对cow技术并不感到陌生，它的核心原理为：它不直接更新文件包含的数据块，相反，它会创建一个完整的更新后的副本，当完成了若干个更新操作后，再一次性将更新后的数据块关联到被对应的被更新文件。ZFS就同时使用cow和journaling两种技术，有兴趣的读者可参考文献[4]。另外，原书中还提到一种optimistic crash consistency技术，它通过使用transaction checksum技术（参考文献[2]），主要用于优化磁盘写入日志记录的过程，以减少等待数据刷盘所导致的时间开销。详细内容可参考文献[5]。 简单小节，本文内容较多。先是从文件更新可能造成的crash consistency问题切入，详细阐述其可能造成的三种后果。然后，详细介绍了能够保证文件更新的crash consistency的两种解决方案：一种是lazy性质的fsck，其主要不足之处在于校验恢复耗时。另一种则是journaling方式，它包括data journaling和metadata journaling两种模式，后者相较于前者减少了文件数据块的写操作开销，但也更为复杂。在阐述jouranling模式时，同时阐述相关操作的一些优化，比如实施batching log updates和引入cicular log来提高恢复速度和磁盘利用率。最后顺便阐述metadata journaling模式在数据块重用的情况下可能存在的一个问题。需要注意的是，除了这两种解决方案之外，也提了其它比较流行和有效的方法。总而言之，关于本文的内容，个人推荐阅读原文，并且，相关的参考文献特别是那些比较新的文献更值得阅读研究。 参考资料[1]. McKusick M K, Joy W N, Leffler S J, et al. Fsck− The UNIX† File System Check Program[J]. Unix System Manager’s Manual-4.3 BSD Virtual VAX-11 Version, 1986.[2]. Prabhakaran V, Bairavasundaram L N, Agrawal N, et al. IRON file systems[M]. ACM, 2005.[3]. Ganger G R, Patt Y N. Metadata update performance in file systems[C]//Proceedings of the 1st USENIX conference on Operating Systems Design and Implementation. USENIX Association, 1994: 5.[4]. Bonwick J, Moore B. ZFS: The last word in file systems[J]. 2007.[5]. Chidambaram V, Pillai T S, Arpaci-Dusseau A C, et al. Optimistic crash consistency[C]//Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. ACM, 2013: 228-243.[6]. Arpaci-Dusseau R H, Arpaci-Dusseau A C. Operating systems: Three easy pieces[M]. Arpaci-Dusseau Books LLC, 2018.]]></content>
      <categories>
        <category>单机存储</category>
      </categories>
      <tags>
        <tag>单机存储</tag>
        <tag>文件系统</tag>
        <tag>崩溃恢复</tag>
        <tag>一致性</tag>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Fast File System]]></title>
    <url>%2F2020%2F01%2F06%2FThe-Fast-File-System%2F</url>
    <content type="text"><![CDATA[上一篇文章从文件系统所使用的数据结构和典型文件访问操作流程两个方面，简要介绍一个简单文件系统(vsfs)原型。其目的主要在于理解一个最基本的文件系统的核心，并且如何理解和学习一个文件系统，是一篇总结和启发性的文章。本文从简单文件系统或早期 unix 系统所使用的文件系统存在的典型问题出发，引出更优秀的文件系统——The Fast File System，它在兼容已有文件系统接口规范的前提下，对已有文件系统存在的问题，针对性进行优化以提升文件系统的访问和存储效率，同时进一步提升系统的易用性。 同上一篇文章类似，本文也是一篇总结和启发性的文章。其来源于书籍 operating systems three easy pieces，以及相应的原始论文 The Fast File System。若读者对其中的细节感兴趣，推荐阅读原文。The Fast File System (FFS)是对早期 Unix 所使用的文件系统的重实现，它通过使用更灵活的数据块分配策略(allocation policies)以增强数据块访问的本地性(locality)，最终显著提升磁盘访问的吞吐率，另外它提供多个数据块大小作为选择，以适应不同文件大小的情形，同时也提升了文件的存储利用率，最后它还提出了一系列的能够提升系统易用性的功能。本文先简单介绍早期文件系统存在的典型的问题，然后再从上述几个方面介绍FFS。 早期文件系统存在的问题早期文件系统性能很糟糕，通常只能达到磁盘总带宽的 2%。造成磁盘低访问效率的根本原因是：将磁盘当成一块支持随机访问的内存来使用，导致每次磁盘访问操作的定位开销非常大。换言之，文件系统的设计没有充分考虑磁盘的顺序访问的效率比随机访问的效率高出接近两个数量级这一本质特性。 具体而言，磁盘访问效率较低包括几个典型的原因：一方面，文件所对应的inode和其索引的data blocks跨越多个磁道，因此，当由inode索引对应的data blocks时存在较大的寻道延时；其次，早期文件系统采用链表实现，即使用链表将空闲块链接起来，此种实现方式较为简单，但其存在的问题是：当文件被反复删除和创建时，原本存储于磁盘上一段连续区域的文件，被其它文件分割成多个离散片段，即访问逻辑上连续的文件的操作，最终映射到了物理磁盘上碎片化的随机访问，这严重降低了文件访问效率（很多磁盘整理小工具的基本原理即为磁盘碎片整理）。另外一个则是关于数据块大小的问题，早期文件系统所使用的数据块大小为 512b，一个较小的取值，小数据块大小导致大文件包含过多的元数据信息，并且也会导致数据块的传输速率下降（频繁累计寻道和旋转延时）。但小数据块大小有利于存储效率的提升，因为它减少了内部碎片。数据块大小的设计是个典型的trade-off。 The Fast File SystemFFS是 Berkeley 研究实现的一个文件系统。它的一个核心设计原则是disk aware。所谓disk aware指的是文件系统的设计考虑到了底层磁盘的结构，通过最优化数据的存储布局，以提升磁盘的访问效率。同时，值得提倡的是，它很好地遵循已有文件系统的接口规范，只更改增强了接口内部的实现，因此，能够很方便将FFS嵌入到已有系统中以替换原有文件系统，这也使得很多现代优秀的文件系统的设计一直延续这一准则。 优化数据结构布局根据磁盘自身的构造特点，FFS首先将整个磁盘划分成一组cylinder group，其中cylinder group是由若干个相邻的cylinder构成。下面是一个示意图。 但考虑到现代磁盘并没有将磁盘本身构造细节暴露给上层文件系统，因此文件系统也无法利用这些细节。但类似地，现代的一些文件系统（包括ext2、ext3和ext4）将整个磁盘抽象后的大数组划分为若干个block group，其中每个block group包含若干个连续数据块。换言之，文件系统将cylinder group抽象成block group，它是FFS提升文件访问效率的一个核心设计。而且，FFS将每个block group作为一个独立的区域，即其中包含了所存储文件相关的所有信息（同vsfs设计类似，包括data blocks、inodes、i-bmap、d-bmap和一个superblock）。 FFS引入block group的设计结构是为了优化数据块的布局方式。FFS对数据块的组织布局的核心原则是：keep related stuff together，即将相关的数据结构存储在一起。这主要包含两个方面：一对于同一文件其所涉及的数据结构存储在同一block group；另外，对于同一目录下的文件和目录项所涉及的数据结构存储在同一block group。以一个实例来阐述。比如存在四个文件/a/c、/a/d、/a/e和/b/f，那么FFS会将a、c、d和e存储到同一block group，而将b和f共同存储到另一个block group，示意图如下。 作为一个错误的设计示例。原有的旧的文件系统为了提升磁盘的利用空间，会尽量保证数据块的均匀分布，因此它更倾向于将不同的文件分散存储在不同的block group中。在这种情况下，当访问同一目录下的文件，会引发较多寻道操作，降低了访问效率。 事实上，FFS所遵循的这两种数据块存放策略存在相应依据。换言之，文件系统的使用中存在大量的对应使用场景。一方面，将文件的inode及其对应data blocks存放在同一block group这是显然的。其次，同一目录下的文件通常会被同时访问，比如使用gcc编译项目目录下的源文件。本质上是因为同一目录下的文件或目录其相关性较大。 最后，此种数据块的布局方式对于大文件而言却是一个例外。可以预料到，大文件可能会填满整个block group，这导致其它同它相关的文件不得不存放到其它的block group。对此，FFS的改进后的布局策略是：将一个大文件分成若干个大的chunk，每个chunk存放在一个block group上，这样后续同其有关联的文件，就能够同样存放到同一block group。但这有一个明显的弊端，将大文件分成多个chunk分开存放将导致访问大文件本身的开销显著增大（寻道操作增加）。但事实上增加的开销取决于每个chunk的大小，换言之，理论上只要chunk够大，那么传输单个chunk所包含数据的时间将基本抵消对相邻chunk的寻道开销，这是一种典型的amortization策略。简单计算，如果磁盘的peek bandwidth是 50kb/s，那么若想达到 50% 的peek bandwidth（即一半时间开销用于数据传输，一半用于寻道和旋转操作），只需要将chunk的大小设置成 409.6 kb 即可。同样，当取得 90% 的peek bandwidth时，相应的chunk大小为 3.69mb。但事实上，FFS并非通过增加大文件的chunk的大小，而是采用一种更符合实际情况的解决方案，将大文件所包含的direct pointer(12个) 存放到第一个block group中，而将后续的每一个indirect pointer及其数据块分别存放到不同的block group中。显然，这是考虑在到绝大部分情况下，文件都只由直接指针构成。 数据块大小设置数据块大小的设置面临的问题是：过小的取值会影响数据传输效率（因为这将导致过多的寻道操作），而过大的取值会造成过多的内部碎片，降低了磁盘的利用率。FFS通过引入sub-block来解决这个问题。具体而言，当应对小文件的存储时，它会分若干个sub-block，当随着文件大小的增长超过 4kb 时，会将已分配的数据进一步拷贝到以 4kb 为单位的数据块，同时释放文件原来占用的sub-block。但这也存在一个问题，即连续不断的拷贝也会降低性能。FFS针对性地修改lib库通过缓存对文件的写入，并尽量以 4kb 为大小写入磁盘，来缓解这一问题。 另外，论文中还介绍了FFS引入的一种parameterization策略，详情可参考原论文。 功能性增强所谓的功能性增强(Functional Enhancements)指的是：通过增强已有文件系统的功能，来改善系统的可用性。FFS做了较多的改进，包括 支持long file names，改进旧的文件系统只能支持固定长度的较短的文件名称的缺陷； 引入symbolic link，旧的文件系统只支持hard link，硬链接有较大限制，比如不能跨分区链接，且禁止链接到目录以避免循环链接； 引入rename系统调用以原子性地重命名文件； 引入了advisory shared/exclusive locks以方便地构建并发程序； 引入了Quotas机制，让管理员能更合理地对每个用户实施配额限制，包括具体到inode和data block数目的限制。 简单小节。本文简单介绍了Fast File System，它对旧的 unix 文件进行了改进。具体而言，以disk aware为设计准则，通过优化磁盘数据结构的存储布局，提升了文件的访问效率；同时引入一种sub-block的策略，来尽可能适应不同的文件存储情形，以提高系统的存储和访问效率；最后在遵循已有文件系统接口规范的前提下，增强已有功能的内部实现，以提升系统的易用性。总而言之，FFS有许多优秀的设计理念和方法值得学习和借鉴，这有助于学习和改进其它更加先进的文件系统。 参考资料[1]. Arpaci-Dusseau R H, Arpaci-Dusseau A C. Operating systems: Three easy pieces[M]. Arpaci-Dusseau Books LLC, 2018.[2]. McKusick M K, Joy W N, Leffler S J, et al. A fast file system for UNIX[J]. ACM Transactions on Computer Systems (TOCS), 1984, 2(3): 181-197.]]></content>
      <categories>
        <category>单机存储</category>
      </categories>
      <tags>
        <tag>单机存储</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单文件系统 vsfs]]></title>
    <url>%2F2020%2F01%2F05%2F%E7%AE%80%E5%8D%95%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-vsfs%2F</url>
    <content type="text"><![CDATA[目前 unix/linux 所采用的文件系统类型，如ext2和ext3，甚至ext4都已经实现得较为完善成熟，其实现细节也比较复杂。但事实上，它们的实现也是经过一步步对已有文件系统的不足进行改进和扩展得来。本文内容指在简要阐述如何设计一个非常简单的文件系统Very Simple File System (vsfs)，它需要重点考虑的因素，以及一些设计准则。通过这个文件系统原型，抓住文件系统设计的核心和那些不可避免的trade-offs哲学，对于理解那些更加先进和复杂的文件系统是有好处的。 本文主要包括两部分内容，在阐述vsfs前先简要指出作为文件系统的载体，硬盘的几个关键点。然后再概要阐述vsfs的设计。需要注意的是，本文并不会详细阐述设计的细节，本文的定位是一篇总结和启发式的文章，它来源于自己阅读 operating systems three easy pieces 的总结和想法。因此，强烈建议大家阅读原文。 关于硬盘一些关键点文件系统的目的在于有效地统一组织和管理硬盘上的数据，并提供一个一致的、简便的接口给上层应用。但需注意，虽然文件系统作为操作系统软件的一部分，即具备超级管理权限，能够执行一些特权指令(privileged instruction)用来操控硬盘驱动器，但大部分情况下它并不知道硬盘存储数据的细节，比如文件具体是存储在哪块盘片上，占据了哪些磁道和扇区，或者当前磁头的具体位置。这也说明了，很多文件系统的优化对于操作系统或者文件系统而言，就无能为力。相反，文件系统只能把硬盘抽象成一个大的数组，可以通过某个单位大小进行寻址/索引（典型的，如4kb），虽然优秀的文件系统会对这个大数组进一步抽象来提高组织管理的效率。 硬盘扇区大小为 512b，因此整个硬盘可以看作是一个以 512b 为寻址单位的数组，但这并不意味着文件系统也只能同样以 512b 寻址，多扇区的操作也是允许的，但显然，硬盘只能保证 512b 的寻址操作具备原子性。 另外，需要时刻记住的是，硬盘的顺序读写的速度远大于随机读写，大约有2个数量级的差异。换言之，如果能将访问硬盘的随机读写操作转化为连续或顺序的读写操作，则可显著提升硬盘访问效率。这比较容易解释，如果从整个硬盘访问的过程来分析，通常其访问时间包含三个部分：旋转时延(Rotational Delay)、寻道时间(Seek Time)以及传输时延(Transfer Time)。我们可以做一个简单的估算，一般硬盘的RPM，即每分钟转数为 15000 或 7200，因此，平均旋转时延大约为 2ms 或 4ms（注意只有一半）；而寻道时间通常由硬盘制造商给出，大约几 ms，比如 4ms；最后传输时延和所传输的数据块大小以及数据块传输速率相关，关于数据块传输速率，一般都有 100MB/s 的级别，比如 125MB/s，因此对于随机读，假定每次读的大小为 4kb（换言之，每读 4kb 的数据块就要累加一次旋转时延和寻道时间），那么其传输时延约为 31 微秒，因此，对于随机读，其（4kb 的大小块）总耗时约为 6ms，由此，随机读传输速率约为 0.67MB/s。而对于顺序读，假定传输数据块大小为 100MB，那么其传输时延为 800ms，其（100MB 的大小块）总耗时约为 806ms，因此顺序读传输速率约为 124MB/s。可以观察到，当每次随机读的块大小为 4kb，而顺序读的块大小为 100MB 这个前提条件下，顺序读的效率为随机读的效率的 100 多倍。不难理解造成这个差异的原因，随机读的主要时间消耗都在旋转时延和寻道时间上，每一次读取都有开销，而真正花费在数据块传输的时间却几乎可以忽略不计（微秒级别）。相反，对于顺序读，其主要的时间消耗主要集中在数据块传输，因此顺序读的带宽 124MB/s 很接近硬盘的理论带宽大小 125MB/s(peek bandwidth)。 关于磁盘的调度策略。所谓的磁盘调度策略在这里指的是当应用程序发出了多个 IO 请求时，磁盘调度器如何决定这些请求的执行顺序。最直接的调度策略是最短寻道时间优先(SSTF: Shortest Seek Time First)，因为磁盘调度器可以通过当前磁头的位置，以及IO 请求的详细内容估算 IO 请求的具体耗费时间，然后按照从小到大排序进行调度。它的问题也很明显，可能会引发请求starvation，即那些先发出的请求迟迟得不到执行。另外，一般情况下，实际上是由处理器来先对应用程序发出的请求进行调度，且考虑到处理器只知道整个磁盘的抽象逻辑表示，无法利用当前磁头的位置来找出最短寻道时间的请求，因此此种策略有局限性。另外就是电梯调度算法(Elevator)及其变种。这种调度策略比较简单，也较有效，能够最大程度接近最短寻道时间所实现的效果，但它只考虑了寻道时间，没有考虑旋转延时。因此，现代的 IO 调度策略准确而言是由磁盘调度器以及处理器调度合作实现完成，处理器只能做粗粒度的请求调度，因为它不知道具体的磁盘目前状态（如磁头位置）及磁盘布局细节（如柱面组划分）。且磁盘调度器还会做一些调度优化，比如对一些相邻块的请求进行合并。 vsfs关于学习文件系统。一个文件系统的精髓或者学习一个文件可以从两个方面来把握：一是实现文件系统所涉及的数据结构。换言之，其底层采用的是怎样的数据结构以有效地组织管理用户数据(user data)和元数据(metadata)。不同的文件系统可以采用不同的数据结构来实现，但它们的目标是一致的，即有效地管理和组织数据；其次，梳理文件系统的典型的方法执行流程和原理。比如对于open()系统调用是如何通过操作这些数据结构来实现的？此过程涉及到对哪些数据结构的读和写的操作？这些操作执行的效率如何？对这两个方面的深入了解，能够让我们了解一个文件系统的数据模型和基本工作原理，以方便之后更好的使用它，甚至改进和扩展它。 基本数据结构首先了解一个典型的文件系统，为了存储用户数据，包含了哪些基本的数据结构，以及它们是如何被有有效组织的。 磁盘组织布局一个最简单的文件系统的布局是容易得到的。整个磁盘，准确而言是整个分区（因为不同分区可以挂载不同的文件系统，各分区独立）可划分成用户数据区(data)和元数据区(inodes)，数据区即存储实际数据块的磁盘区域，而元数据区则用来索引数据块和记录数据块元信息的磁盘区域（准确而言，索引的方式一般是通过指针，可以是多级指针；而元信息则包括时间类，文件权限类，以及文件大小等属性类）。为了重复使用inodes和data区域，需要使用一块区域来记录它们是否已被占用，常见是采用位图来完成，位图占用空间小，检索也方便，即使用i-bmap和d-bmap来分别跟踪inodes和data所包含区域的数据块的使用情况。最后对于一个文件系统而言，需要有一个关键的全局已知区域来存放文件系统级别的信息，即是超级块superblock，它存储的信息包括整个文件系统data和inodes所包含的数据块的数目以及它们各自的索引起始位置，另外也会存储一个magic number来检测文件系统类型。superblock至关重要，一旦损坏，则整个文件系统变为不可用，因为一些会多副本存储。另外，在整个文件系统正常工作前，需要先将superblock加载到内存，换言之，在系统挂载文件系统时，其首先将读取superblock的内容并初始化全局信息，然后才将文件系统的其它部分附加到系统文件系统层级树。 一个实例下面以一个实例来直观描述文件系统对硬盘抽象后的详细布局。下图是一个典型的简化后的磁盘布局，假设磁盘的数据块大小为 4kb，则从文件系统的角度而言，整个磁盘就是一个以 4kb 为寻址单位且总大小为 256kb（共64个数据块） 的数组。数组的后 56 个数据块作为data区域，inodes区域靠近data区域，占据 5 个数据块大小，共 20kb，假设一个inode的大小为 256b，那么一个块可以包含 16 个 inode，因此，整个inodes区域(inode table)包含了 80 个inode，这对于我们的实例是足够的（我们只有 64 个data block）。在indoes区域前的是i-bmap和d-bmap区域，各占 1 个数据块的大小。最后数组的第一个数据块被设置成superblock。 每个inode都有唯一编号，即inode number。且在vsfs中，通过inode number可以直接得出对应的inode所在磁盘位置。下图中，inodes从 12kb 位置开始直至 32kb 处，占据 5 个数据块空间。因此，若读取inode number为 32 的inode，则可通过blk=(inumber * sizeof(inode_t))/blockSize 得到起始的 block编号（相对于inodes区域的block编号），但考虑到物理磁盘通过扇区寻址，因此进一步计算，sector=((blk*blockSize)+inodeStartAddr)/sectorSize以得出最后的全局扇区编号。 关于 inode关于inode的设计。即如何通过inode定位到对应的data blocks。最简单的方式即存储一个直接指针(direct pointers)，即对应的磁盘地址。但这种方式存在明显的局限性，考虑一个非常大的文件，其大小超过一个inode块所能索引总的数据块大小（约 4kb/256b 4kb）。解决办法也很直接，使用多级指针，在这个实例中，若仅多考虑一个间接指针indirect pointer，且间级指针所指向的数据块中所包含的地址大小为 4b，且再加上一级指针所能索引的数据块的大小，即若一个inode包含 12 （典型的值）个direct pointer和 1 个indirect pointer，那么此inode所能索引的最大文件大小为 (12+4kb/4b) 4kb = 4144kb。当然，在很多情况下，仍不能满足需求，则可考虑三级指针(double indirect pointer)，简单估算，可以索引的数据块的最大大小为 (12+4kb/4b+(4kb/4b 4kb/4b)) 4kb，可以看到这个大小已经非常大了。事实上，很多文件系统（包括ext2和ext3）都使用多级指针。另外，实际上在绝大部分使用场景中，一级指针已经完全够用，因为小文件在文件系统占主导地位（如 2kb），因此，为了更好地适应此种情形，一个inode包含典型的 12 个直接指针，能够直接索引 48kb 的文件大小，这已经可以满足大部分情况下的需求了。但为了应对极少部分的大文件存储，可以添加一个二级甚至三级指针。 事实上，inode可以使用任何其它能够有效管理组织数据块元数据信息的数据结构来实现，另一种常见的是基于extents的实现方式，一个extent表示一个磁盘地址和一个表示数据块数目的整数，以此来索引一个文件。明显，基于extents的表示方式需要大量连续磁盘空间，且如果文件过大，也可能需要多个extent来表示，但相比基于inode的索引方式，基于extents的方式其元数据信息所占空间小，相对紧凑，因此访问效率可能更高。 另一种用于实现inode的方式是使用链表(linked-based)，在这种情况下，我们只需要一个指针指向文件所包含的第一个数据块。但此种方式的缺点很明显，访问文件指定部分时，相当于要遍历整个链表。因此，对于FAT文件系统而言，虽然它也采用类似的方式实现，但它使用一个内存表存储着所有链表节点的信息，内存表通过数据块的地址来索引，以获得下一个数据块的地址信息，通过此种方式高效地实现数据块的随机索引。 目录组织关于目录。对于内核而言，目录也是一种特殊的文件（directory类型），因此，它也是通过inode来索引其所包含的data blocks。只不过不同于普通的文件存储的是用户数据，目录包含的是(entry name, inode number)的一个列表（可能还包含其它信息，比如entry名的长度），当然，目录也可以包含目录。类似地，也有通过B-tree来组织目录所包含的内容的实现方式。 典型方法执行流程在了解一个典型的文件系统所使用的基本数据结构后，即清楚了用户数据在磁盘是如何被组织和管理后。那么，我们还需要了解，当我们操作存储在磁盘上的文件时，具体是如何映射到对这些数据结构的相关操作的。典型的操作流程包括文件的读取和写入。 文件读取以一个简单的实例阐述。若文件系统读取一个大小为 12kb(3个数据块)的文件/foo/bar。读取过程所涉及的相应数据结构的操作类型及顺序如下图。几个关键点L：/目录所对应的inode的编号必须是全局的，因为一般的文件或目录的inode编号是通过在其父目录中检索得到，而考虑到/处在最顶级的层级，因此，其inode编号必须是全局的。另外，访问一个文件的某个数据块时，需要更新文件所对应的inode中存储的元数据信息（访问时间）。最后注意到，目标文件所处的层级越深，其访问也越耗时，因为理论上它需要一层层递归下去，而且在递归过程中，若某一目录所包含的项非常多，则会严重降低指定目录项的检索效率。但为了提高文件访问的效率，文件系统一般会采用缓存那些频繁访问的数据块。 文件写入同样以一个简单的实例阐述。文件的写入流程和文件读取的流程存在相似部分。显然，文件首先需要被打开，因此定位文件的过程是类似的。写入文件可能涉及到新数据块分配，当然，也有可能是已有数据块被覆盖。如果考虑写入额外内容，则需要更新d-bmap和inode。具体而言，每次对文件写入新数据块，需要对d-bmap读取和写入各一次，对inode读取和写入各一次，以及对新数据块的写入操作。另外，若创建文件，则流程更为复杂，因为它涉及到文件所在目录的相关操作，比如在目录所关联的数据块中分配对应的entry记录。具体而言，对d-bmap读取和写入各一次，对文件所对应inode的写入，对文件所在目录所关联的数据块的写入，以及对文件所在的目录所对应的inode的读取和写入各一次，即总共包含了 5 次IO。而且可能还需要对创建后的文件写入新数据块。同文件读取类似，文件写入所需要的磁盘操作甚至更多，因此，大部分情况下，文件系统也会对文件的写入进行缓冲，以执行批量写入，批量写入也有助于磁盘调度器的调度优化。 简单小结。本文先指出关于磁盘本身的一些重要点，这是考虑到这些关键点与文件系统的设计，甚至上层应用程序对文件系统的使用密切相关。另外，从两个方面简单介绍了简单文件系统(vsfs)，其一是vsfs为了有效组织和管理磁盘上的数据，所采用的重要数据结构，其次结合这些数据结构，简单介绍文件读取和创建两个典型操作流程中对相应的数据结构的操作类型和顺序。理解这两点有助于对一个文件系统的原理的整体把握。本文的定位是一篇总结和启发性的文章（文章中的插图全部出自参考资料中的[1]），希望通过对一个极其简单的文件系统的了解，能够有助于学习那些更加先进复杂的文件系统。 参考资料[1]. Arpaci-Dusseau R H, Arpaci-Dusseau A C. Operating systems: Three easy pieces[M]. Arpaci-Dusseau Books LLC, 2018.]]></content>
      <categories>
        <category>单机存储</category>
      </categories>
      <tags>
        <tag>单机存储</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简析 GitLab Runner]]></title>
    <url>%2F2019%2F07%2F25%2F%E7%AE%80%E6%9E%90-GitLab-Runner%2F</url>
    <content type="text"><![CDATA[原计划本文是作为此系列文章的最后一篇，即整合Kubernetes和GitLab CI并构建持续集成流程。但由于后面对GitLab Runner作了进一步的了解，因此，在此作下记录，同时方便有需要的同学。上两篇文章的主题分别是在Kubernetes中安装GitLab Runner以及Docker-in-Docker &amp; Socket-Binding这两种可以实现在容器中执行容器命令的方法。本文侧重于对GitLab Runner相关内容作一个补充。本文内容主要来源于两个地方：一部分是来提炼于官方文档，另一部分是通过查阅源码，归纳总结出来的核心操作的逻辑。因此，如果大家对GitLab Runner有兴趣或者学习/工作有需要，可以仔细查阅官方文档，和追踪它的源码。这篇文章主要阐述三个方面的内容，一是关于GitLab Runner的知识，其二是强调和细化一下Executors这个概念，最后，通过阅读源码，概要阐述GitLab Runner和GitLab Server二者的基本交互逻辑。 不是不说，有些概念官方也没有说地特别清楚（个人观点，至少对于新手而言不太友好），需要自己去实践才能彻底明白其中的原理或用法。GitLab Runner的源代码是用Golang写的，总体而言，各模块代码组织结构比较清晰，而且也不会难以读懂，强烈建议有兴趣的读者可以翻看下。下面对这三方面一一展开介绍。 关于 GitLab Runner有一个最基本的概念需要清楚——GitLab Runner到底是做什么的？事实上，GitLab Runner(Runner)并不负责最终执行我们在.gitlab-ci.yml中定义的各个stage中的脚本（真奇怪，明明都被称为是Runner了）。意识到这一点很重要。因此，对于Job的构建效率与Runner本身的配置没有直接关联（但是Runner确实会影响到CI流程的构建效率，这在后面阐述）。 另外，需要提醒的是，GitLab Runner允许以rpm包、debian包（yum源、apt-get）安装在 Linux,、macOS或FreeBSD上，甚至可以通过二进制文件安装在Windows上，当然也可以通过拉取镜像安装在Docker或 Kubernetes中。Runner本身没有任何特殊之处，它也只是一个使用Golang编写的程序而已，因此，理论上它可以安装在任何具有Golang环境的机器上。 另外一个问题是，很多时候如果你的GitLab CI工作流程跑得比较慢（这很常见），或者说构建效率较低。此时，一般而言，可以从三个方面来调整解决： 调整你的.gitlab-ci.yml文件内容（确保自己熟悉.gitlab-ci.yml各选项），实施一些优化操作。典型地，让各stage之间共享缓存。 优化你的GitLab Runner的配置和Job调度策略。这包括两个方面，其一是Runner的配置，比如，concurrent参数决定了你的项目中同时可以构建的Job的数量，另外还有其它的几个相关的选项。其二是Runner调度Job的策略，不同的调度策略会影响到你提交的Job的构建情况。典型地，若某个Project包含很多个Job，那么它很有可能会占居大量的Runner资源，而Shared Runner采用的Fair Usage Queue调度策略就可以缓解此问题。Runner的调度策略与Runner的类型相关，其中Specific/Group Runner使用的是FIFO，注意，此FIFO针对的是Job，而不是Project。而Shared Runner使用的是Fair Usage Queue这种调度策略，官方文档给了两个例子来解释。在后面，我有一张PPT有阐述Fair Usage Queue策略具体是怎样，另外有两张GIF分别对应官方文档的两个示例。 最后，当然，你也可以升级Runner的物理硬件资源配置，这种方法就不多阐述了。 关于GitLab Runner的最佳实践，这是GitLab官方论坛的讨论贴。这是网上的一个关于GitLab Best Practices的建议。若有需要，大家可以参考下。 最后，通过阅读官方文档，本文整理一些关于Runner一些tips： 关于Runner 你可以为多个Project注册同一个Specific Runner，与使用Shared Runner注册给多个Project的区别是：你需要显式地在每个Project下 enable 这个Specific Runner； 注意Specific Runner不会为会forked project自动开启，因此，你需要显式注册绑定； GitLab admin 只能注册Shared Runner，当然你也可以在Shared Runner被注册后，主动为某个Project取消注册此 Shared Runner； Specific Runner可以被 lock 到某个Project。这样其它项目不能使用此Runner； 注册Specific Runner有两种方法：a. 一是直接通过gitlabUrl和 registerToken。注意此 registerToken 是同Project绑定的！b. 另一种是将Shared Runner转换成Specific Runner。此操作是一次性的，且只有 admin 才能操作。 你可以通过使用protected branches或protected tags来关联拥有这些信息的protected project和 protected Runner。因为考虑到实际生产环境有些 Runner 可能包含私密信息； 实践建议：尝试给Runner使用tag，同时给Project打上 tag； 为Runner设置执行Job的超时时间。 注册Runner是否与Project有关，例如registerUrl 和 token 跟项目是相关的？注册Specific Runner与Project相关。必须使用gitlabUrl以及此项目下的registerToken才能将此 Runner注册到此project下。若没有提供正确的registerToken（但这个registerToken确实合法的，比如选择了其它 Project的registerToken），则也可以显式地在这些Project下手动 enable 此 Runner，前提是你是这些项目的maintainer。 TODO: 你可以实践一个错误的registerToken，即它不与任何Project关联（Kubernetes Executor）。实践结果：Runner所在的Pod启动失败，容器就绪探针有问题。因此，验证了前述逻辑。 关于Runner Token的问题？有两种类型的Token。一个是Registration Token，用于Runner注册时使用。另一个是Authentication Token，用于Runner向GitLab提供认证。这个Token可以在使用Registration Token注册到GitLab时自动获取到（由 GitLab Server返回）；然后，Runner会将它放在Runner的配置文件中。 或者是手动在Runner配置文件 的[[runners]] section下设置token=&quot;&lt;authentication_token&gt;&quot;。之后，GitLab Server 和 Runner就能正常建立连接。 关于 Executor既然Runner不是Job的执行体，那究竟是谁负责执行Job呢？事实上，这与Executor密切相关。总而言之，Runner是借助Executor来创建具体的执行我们Job的资源实体，官方文档把它称为Runner，有点尴尬，但是读者必须清楚二者的区别。那Executor又是什么呢？我个人的理解是，所谓的Executor是一个抽象的概念，它为GitLab CI构建过程提供资源环境。引入Executor这个概念，可以使得Runner使用不同的 Executor（SSH、Shell和Kubernetes等）来执行Job构建过程。典型地，在具体某个环境中，比如在Kubernets中，就由 Kuberentes Executor来请求Kubernetes API Server动态创建Pod，动态创建出来的Pod才负责Job的执行（关于Pod的含义读者可以参考Kubernetes文档）。 最后，Runner所安装的地方并不会与最终Job的执行体绑定，我们姑且称这个执行体为Executor吧，可能不是很准确。比如，如果我们使用Kubernetes Executor，则我们可以将Runner安装在Windows上，但却将它远程连接到Kubernetes集群，并通过Kubernetes Executor来为Job的构建动态创建Pod，但这种方式不是最简便或最合理的，个人是将Runner同样安装在Kubernetes中，这是官方的推荐做法，一方面，因为最终的应用是部署在Kubernetes中，因此，这会带来便利；另外，也会省去Runner连接集群的一些认证等过程。当然，如果选择将Runner安装在Windows中，这是最简单朴素的方式，此时Runnre会直接在本地为每一个Job动态启动一个进程，是的，这就是Shell Executor。更准确而言，应该是PowerShell Executor。下面是个人翻译整理官方文档的一些关于各种Executor的基本情况： SSH 通过ssh连接到远程主机，然后在远程主机上启动一个进程来执行GitLab CI构建过程。连接时需指定url、port、user、password/identity_file等参数； 若想要上传artificate，需要将Runner安装在ssh 连接到的远程主机上。 Shell 使用安装Runner的同一台主机启动一个进程来执行GitLab CI构建过程。凡是支持安装Runner的机器类型，都可以用使用shell的方式。这意味着Windows PowerShell、Bash、Sh和CMD都是可以的。 VirtualBox/Parallel 通过ssh远程连接到虚拟机，在虚拟机中执行GitLab CI构建过程，可能会创建虚拟机快照以加速下一次构建。类似地，需指定user、password/identity_file； 同SSH方式类似，若想要上传artificate，需要将Runner安装在VirtualBox的虚拟机中； 正式开启CI流程前，需提前在VirtualBox中创建或导入一个Base Virtual Machine，并在其中安装 OpenSSH Server以及依赖等。 Docker 将Executor连接到Docker Daemon，并在一个单独容器中跑每一次的构建过程，并使用在.gitlab-ci.yml文件中定义的镜像，Docker Executor具体是通过config.toml文件来配置的。 Kuberentes 让Runner连接到连Kubernetes API Server，为每一个Job动态创建一个Pod来执行GitLab CI构建过程。 且此Pod除了包含固有的Infra Container外，还一定会包含Build Container和Help Container，另外，可能包含Service Container。简单而言，Build Container用于执行.gitlab-ci.yml文件中在stage标签中定义的脚本。Help Container则用于辅助Build Container的执行构建工作，具体是负责git和certificate store相关的操作。最后Service Container的用途则对应着.gitlab-ci.yml文件中定义的service标签，即一个辅助容器，为Build Container提供服务，其基本实现原理是Docker Link。 最后，每一个Job都会包含四个阶段（Job构建过程的生命周期）：Prepare、Pre-Build、Build和Post-Build。这几个阶段的具体作用，我在这里就不阐述了，比较简单，可以阅读这里，也可以在源码中找到。 关于Executor就阐述到这里，Executor的概念非常重要，也比较抽象。 关于 GitLab Server 同 GitLab Runner 的交互这一小节简要阐述下GitLab Server同GitLab Runner的交互过程，基本是通过阅读源码总结而来，但并未详细阅读源码，只是大概理清整个交互逻辑。因此，如果读者没有跟随源码，下面的描述中涉及到源码的部分可能会有点不模糊，不过没有关系，若读者只想了解二者交互的大概过程，只需要把下面的二者的交互图搞清楚即可。但若读者有兴趣，个人还是建议，可以翻看下源码，会更清楚一些。 下面从四个重要操作展开叙述，分别是： Register Runner，Runner注册过程，即将Runner绑定到GitLab Server实例的过程； Polling Job，Runner轮询Job的过程，当Runner从GitLab Server获取到Authetication Token后，它会定期去向GitLab Server轮询是否有等待构建的Job； Handle Job，Runner一旦轮询到Job后，它会启动构建过程，即开始上述四个阶段：Prepare、Pre-Build、Build和Post-Build。（对于Kubernetes Executor而言）。 Patch Job，在构建Job的过程中，会定期将Job Trace日志信息发送给GitLab Server； Register RunnerRegister Runner。当执行客户端执行register命令(gitlab-runner register ...)并提供一些配置信息时，如gitlabUrl、executors、token和tag等，会触发对应的Runner注册过程。源码中对应的方法是 commands/register.go#Execute，然后会继续调用register.askRunner方法来配置Runner，在构造所需参数后，将调用network/gitlab.RegisterRunner方法来注册Runner。在此方法中，最终通过http POST /runners来完成向GitLab Server发送注册请求，同时处理注册请求的返回结果。其中，注册请求的重要参数包括 registrationToken，locked，maximum_timeout，tag_list等（这需要在配置时填写的）。而注册请求的响应内容包含一个token，正如前文所述，在此之后，当Runner向GitLab Server请求Job信息时，需携带此 token。最后，需要提醒的是，此接口是公开的，换言之，你可以使用程序调用此接口。 Polling JobPolling Job。当Runner注册成功后，其会定期（默认是3秒，可配置）向GitLab请求Job信息。这在源码中对应的是commands/multi.go#processRunner方法，然后调multi.requestJob法，进一步调用 network.RequestJob（即GitLabClient.RequestJob）请求Job。最终通过http POST /jobs/request接口 来完成轮询Job请求。此请求的重要参数包括token和RunnerInfo等 。而响应内容包括jobInfo、gitInfo等。当然，若没有没有等待构建的Job信息，则返回204 StatusNoContent。最后，此接口似乎没有公开。 关于Polling Job的具体源码体现。在commands/multi.go#Run方法中，异步开启一个goroutine，执行 multi.feedRunners(runners)方法，此方法会判断是否在可用的runners，若存在，则遍历所有可用的 runner，并周期性地（默认，每隔CheckInterval=3s）往runners 通道中压入runner。需要注意的是，若有多个runners，则实际的周期是CheckInterval / len(runners)。接着会调用方法链： multi.startWorkers -&gt; multi.processRunners，在此方法中通过select case结构从runners取前面压入的runner实例，一旦取出成功，则调用multi.processRunner方法，随后的步骤如前所述。 需要注意的是，在正式调用multi.requestJob方法前，会先通过common.GetExecutor获取executor，同时还要为runner申请足够资源 (multi.acquireRunnerResources)。 另外，最终构建Job是通过方法链完成的：common/build.go#build.Run(mr.config, trace) -&gt; build.run(context, executor) -&gt; build.executeScript(runContext, executor)。关于构建的四个阶段，对应的源码内容也比较清楚，在build.executeScript方法中存在如下代码调用： 1prepare -&gt; build.executeStage(ctx, BuildStagePrepare, executor) 123pre-build -&gt; build.attemptExecuteStage(ctx, BuildStageGetSources|BuildStageRestoreCache|BuildStageDownloadArtifacts, executor, b.GetGetSourcesAttempts() 123build -&gt; build.executeStage(ctx, BuildStageUserScript, executor) 和 build.executeStage(timeoutContext, BuildStageAfterScript, executor) 12post-build -&gt; build.executeStage(ctx, BuildStageArchiveCache, executor) 和 b.executeUploadArtifacts(ctx, err, executor Handle JobHandle Job。当成功获取Job息后，Runner就开始处理Job的构建过程。这在源码中对应的是 commands/multi.go#requestJob方法，然后调用network.ProcessJob方法。在这之前会构造 jobCredentials{ID, Token}，接着通过trace.newJobTrace创建Job Trace即Job处理日志，在构造函数中指定了Job Trace更新的周期，默认是UpdateInterval=3s，然后调用trace.start方法开启 Job Trace 输出。 Patch JobPatch Job。在Job被正式构建时，是通过调用trace.start方法来调用trace.watch以周期性地patch Job trace。在源码中是通过trace.incrementalUpdate -&gt; trace.sendPatch -&gt; network.PatchTrace方法链来完成调用的，最终通过http PATCH /jobs/{JobId}/trace来完成patch Job trace请求。其中重要参数即为job trace content，且为增量输出，在请求的headers中需要设置Job token。若请求发送成功，则返回 StatusAccepted 202响应码。同时，每隔forceSendInterval（默认30s） 的时间还要更新Job执行状态信息（pending、running、failed和success），在源码中是通过方法链trace.touchJob -&gt; network.UpdateJob来完成，最后通过http PUT /jobs/{JobId}完成请求的发送，其中重要参数包括runnerInfo、JobToken、JobState 等。但需要注意的是，若Job执行失败，则会附带上失败原因FailureReason，若Job Status更新成功，则返回UpdateSucceeded 200响应码。 下面是一张完整的GitLab Server同GitLab Runner的交互图。其中，最左边的表示客户端执行的Runner的命令（注册，启动和取消注册）。中间用红色标示的表示各个详细的阶段。右边中绿色标注的表示Runner同GitLab Server的Http通信细节，这个是最重要的。右边的黑色和蓝色标示的表示Runner自身内部执行的一些操作。 简单小结，本文主要阐述了三个方面的内容：一是阐述Runner相关的知识，特别要清楚Runner的本质是什么，以及提高GitLab CI构建效率的三个方面的知识，最后补充了Runner相关的细节知识点；二是阐述Executor相关的知识，包括Executor的本质，与Runner的关系，并且简要阐述了各种Executor，需要重点关注Kubernetes Executor。最后，阐述GitLab Server同GitLab Runner基本交互逻辑，主要是包括四个阶段（没包括最后的取消注册），这几个阶段都挺重要，读者可以借助二者的交互图来理解，重点关注二者之间的Http交互的各阶段。这有助于理解Runner的执行原理。 参考文献 [1].https://docs.gitlab.com/runner/[2].https://forum.gitlab.com/t/best-practices-for-ci-with-gitlab/5169[3].https://docs.gitlab.com/runner/executors/[4].https://docs.gitlab.com/runner/executors/kubernetes.html[5].https://docs.gitlab.com/ee/api/[6].https://gitlab.com/gitlab-org/gitlab-ce/tree/master]]></content>
      <categories>
        <category>devops</category>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>gitlab-ci</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析 docker-in-docker 和 socket-binding]]></title>
    <url>%2F2019%2F07%2F07%2F%E6%B5%85%E6%9E%90-docker-in-docker-%E5%92%8C-socket-binding%2F</url>
    <content type="text"><![CDATA[上一篇文章详细阐述在k8s中安装gitlab runner的整个流程，并且也阐明了其中涉及的原理。原计划这一篇博文紧接着叙述基于k8s并集成gitlab-ci的持续集成部署方案的第二阶段——研究集成gitlab-ci和k8s来实现一个以build-&gt;test-&gt;deploy为核心的持续集成部署流程。第一阶段只是搭建好了环境，显然第二阶段要更重要。但考虑到个人在第二阶段实验过程涉及到至关重要的一个问题，因此，打算单独开一篇博文总结一些看过的资料，并基于个人的理解与认识将此问题解释清楚。是的，这个问题是：若我们想在docker中运行docker应该如何实现呢？简单而言，在一个docker容器内能够安装docker daemon，以使得我们能够执行docker build/push等命令。这个问题在ci/cd中很典型，无论是采用Jenkins还是gitlab-ci同docker或k8s结合。比如，对于gitlab-ci而言，它的每一个stage都跑在一个容器中的，而若想在某个stage中执行docker命令（典型的，在服务构建阶段会涉及到docker build），默认是不支持的。我们将此种需求概略地称为在容器中运行容器。在本博文中主要讨论实现此需求的两种实现方式，但事实上，也可能不仅仅这两种方式。 本文不会过多阐述docker基本原理，但这两种实现方式确实会涉及到docker的一些知识。因此，你需要具备docker基本原理的基础。若要在一个容器中安装另外一个容器，从技术上而言，这是可以实现的。似乎在Docker 0.6版就添加了这个新特性，且从使用上而言，也较为简单，我们暂且称之docker-in-docker(dind)。但它涉及到一些安全问题，也可能会引起一些奇怪的问题。具体你可以参考这里。因此，自然而然就诞生了其它更为合理的方式——socket-binding，它实际上并非严格意义上的docker-in-docker，但它可以实现类似在容器中执行容器相关命令的效果。并且它还具备其它的优势，典型的，可以让子容器，孙子容器等等共享镜像缓存，这在某些情况下是非常合适的。下面详细介绍这两种实现方式，都遵循从实践到理论的阐述思路。 docker-in-dockerdocker-in-docker这种模式从最初作为新特性被引入docker，到当前的版本，功能确实日趋完善。但在这里仍旧只涉及其核心部分的实践及原理。 docker-in-docker 初步实践在0.6版的Docker在执行docker run命令时，增加了一项新特性——privileged选项参数，可以说就是此参数真正实现了在容器中运行容器的功能。如图1，当你执行如下命令： docker run --privileged -it jpetazzo/dind 它会从docker hub下载一个特殊的docker image，此镜像包含了docker client和docker daemon，并且指明以特权模式来执行它，然后它启动一个本地docker daemon，并进入容器交互式shell。在此特殊容器中，你可以继续执行docker run启动容器： docker run -it ubuntu bash 仔细观察你的容器ID，你的hostname发生了变化，说明你已经从外层容器进入到了内层容器了！值得注意的是，此时，内层容器与外层容器依然是隔离的。我们可以简单验证一下。 docker-in-docker-in-docker?有读者可能会思考，既然我可以在容器中启动容器，即docker-in-docker，那么我是否可以做到docker-in-docker-in-docker呢？是的，这完全可以实现，甚至，理论上你可以无限递归下去，只要你在启动下一层容器时，开启特权选项即可。你可以实践下图的操作内容，观察容器ID，说明你确实做到了容器递归嵌套容器。而且你会发现每次执行docker run命令时，它都会去下载jpetazzo/dind这个镜像，这说明了各个层级的容器不会共享image，这也间接证明了各层级的容器确实处于隔离状态。 关于 privileged 特权模式关于privileged特权模式。我们知道linux进程包括priviledged process，即root用户（或者说pid=0的用户）创建的进程，和unpriviledged process，即普通用户创建的进程。--privileged选项实际上就是创建priviledged container进程。考虑到默认情况下docker容器运行模式为unprivileged，这使得在一个docker容器中跑另外一个docker daemon不被允许，因为容器不能访问宿主机的任何device。但一个具备privileged特权的容器则允许访问所有device(cgroup device)，即可以访问/dev下所有的目录。换言之，当容器被配置成privileged模式时，容器对宿主机的所有device具有完全控制权，同时通过修改AppArmor及SELinux相关的配置，使得容器几乎相当于运行在宿主机上的进程一样，具有完全访问宿主机的权限。 虽然，在Docker 0.6版，纯粹只添加了--privileged选项。但当前的版本(18.09.6)其实限制容器对宿主机设备的访问权限的粒度已经控制得比较精确了。换言之，如果只想让容器访问部分设备，可以使用--device选项，这使得默认情况下，容器对这些设备具有read、write、和mknod权限，但可使用:rwm作出限制。 除了使用privileged选项，也可使用--cap-add和--cap-drop选项以更细粒度的控制容器对宿主机访问的某些方面的权限。更多可参考docker官方文档。 jpetazzo-dind 基本原理现在，我们来讨论一下jpetazzo/dind这个镜像的特殊之处。主要参考的是这篇文章。事实上，这个镜像也没有什么太大不同，也是由Dockerfile构建，下面是它的Dockerfile内容： 123456789101112131415161718192021FROM ubuntu:14.04MAINTAINER jerome.petazzoni@docker.com# Let's start with some basic stuff.RUN apt-get update -qq &amp;&amp; apt-get install -qqy \ apt-transport-https \ ca-certificates \ curl \ lxc \ iptables # Install Docker from Docker Inc. repositories.RUN curl -sSL https://get.docker.com/ | sh# Install the magic wrapper.ADD ./wrapdocker /usr/local/bin/wrapdockerRUN chmod +x /usr/local/bin/wrapdocker# Define additional metadata for our image.VOLUME /var/lib/dockerCMD ["wrapdocker"] Dockerfile所包含的内容并不复杂，主要做了如下四几件事情： 安装一些docker daemon依赖软件包，包括lxc和iptables。另外，当docker daemon同docker index/registry通信时，需要校验其SSL认证，因此需安装ca-certificates和apt-transport-https等。 挂载/var/lib/docker volume。因为容器文件系统是基于AUFS的挂载点(mountpoint)，而构成AUFS的分层文件系统应为正常的文件系统。 换言之，/var/lib/docker这个用于存储它创建的容器的目录不能是AUFS文件系统。因此，将此目录以volume的形式挂载到宿主机。这使得后面在容器中创建的内层容器的数据真正存储宿主机的/var/lib/docker/volumns目录下。 通过脚本快速安装一个最新的docker二进制镜像文件。 执行一个helper脚本。脚本主要操作包括如下三个方面： 确保cgroup伪文件系统已经被正确挂载，若没有挂载，则依据宿主机中cgroup层级文件系统的形式对它进行挂载，因为docker(lxc-start)需要它。 关闭宿主机上多余的文件描述符。否则可能会造成文件描述符资源泄露。虽然这不是严格必需，但目前我们关闭它可以避免一些奇怪的行为（副作用）。 检测你是否在命令行中通过-e PORT=...指定了一个PORT环境变量。如果你确实指定了，docker daemon将会在前台启动，并在指定TCP端口监听API请求。反之，它会在后台启动docker daemon进程，并且为你提供一个交互式的shell。 docker-as-a-service最后，需要补充的一点是，若你想使用docker-in-docker来作为一个服务（上述已提到helper脚本中最后一个操作，即判定docker deamon是监听指定端口，还是提供一个临时shell），即计划提供一个Docker-as-a-Service，注意不是Containers-as-a-Service，这两者在概念上是有区别的。因为我们提供的服务是一个docker实例。我们可以通过如下命令，通过让容器运行于后台模式，并对外暴露一个端口来实现： docker run --privileged -d -p 1234 -e PORT=1234 jpetazzo/dind 如上的命令可以获取到容器的ip和port，如此便可为第三方提供docker实例的服务。简单而言，它们可直接连接到docker实例(docker daemon)执行与容器相关的操作。我们简单运行一个只安装了docker clinet的容器，然后设置其DOCKER_HOST为此提供docker daemon的容器的地址，然后简单实验一下是否成功连接，并使用作为服务的docker daemon。当然，你也可以参考这里，使用docker link来做实验完成类似的效果。同样，考虑到此docker实例服务是以priviliged模式运行的，因此，它可能会因为获取了特权而造成不可预料的风险。 socket-bindingdocker-in-docker的方式可以实现在容器中启动另一个容器，但它确实存在安全风险，而且，也存在潜在的棘手问题，具体可以参考这篇博文。值得一提的是，使用dind的方式，无法让各内层容器之间或容器与主机之间共享缓存，这在基于k8s集成gitlab实现持续集成部署方案的用例中是一个比较严重的问题。因此，笔者考虑使用socket-binding的方式（socket-binding称呼源自gitlab官方文档，也可称之为bind-mount）。 socket-binding 实践事实上，很多情况下，我们并不真正需要在一个容器中运行另外一个容器（或许存在特例）。我们需要的可能只是想在docker容器中能够继续执行docker相关操作（如docker build/pull/push等），至少在笔者的使用案例中是这样的。因此，使用dind的方式是否显得小题大做了？事实上若要达到我们的目的（在容器中执行docker相关命令操作）是很简单的——在启动容器时使用-v选项以绑定挂载的方式(binding mount)将宿主机的docker socket挂载到容器，即执行如下命令： docker run -it -v /var/run/docker.sock:/var/run/docker.sock some-docker-image /bin/bash 且此docker run命令中的使用的some-docker-image镜像则不必为jpetazzo/dind，它没有任何特殊之处。当然，此镜像必须包含docker client，而可以不用包含docker engine。因为，当我们以socket-binding的形式来run一个容器时，它实际上是将宿主机的/var/run/docker.sock挂载到了容器中/var/run/docker.sock，这使得在容器中执行docker build/push/pull命令真正使用的是宿主机的docker daemon，换言之，我们使用容器中的docker client和容器外的宿主机的docker daemon进行通信。这不同于dind，它并非真正实现了在容器中运行容器的功能。当使用socket-binding的方式时，所创建的容器和执行docker命令的当前容器处于同一层级（不是父子关系，而是兄弟关系），都是直接隶属于宿主机下的一层。因此，你可以推理得到，正因为所有的”内层”容器实际上都使用的是宿主机的docker daemon，这使得宿主机和所有有的容器可以共享镜像缓存！最后，同docker-in-docker-in-docker...类似，socket-binding的方式理论上也可以无限递归。我们简单通过如下的操作过程简单实践： 先使用下面的Dockerfile构建我们的实验镜像，注意，我们在容器中只安装了docker client。 然后，构建一个名为dind-sc的镜像。 使用run命令启动容器，并进入到容器中，执行docker version命令，可以同时输出了docker client和docker engine的信息！另外，执行docker image命令，发现输出一堆image，是的，这是宿主机上的镜像。 我们再一次在当前容器中基于此Dockerfile构建（有没有发现这次构建非常快，是的，使用了上一次的镜像缓存），然后运行此容器……，重复上述的操作。可以发现，所启动的容器的地位其实是一样的，它们都在同一个层级。 最后，实验验证宿主机同各容器共享镜像cache。可以看到，我们在宿主机中构建的镜像可以在容器中看到，而在容器中拉到的nginx镜像，也能在宿主机中看到。 关于 docker volume基于socket-binding来实现在容器中执行容器相关操作的命令的原理其实就是docker volume。docker为了能够保存（持久化）数据以及共享容器间的数据，引入了volume机制。简单而言，volume就是目录或者文件，它可以绕过默认的由多个只读层及一个读写层叠加而成的联合文件系统(union file system)，而以正常的文件或者目录的形式存在于宿主机上。volume机制隔离了容器自身与数据，这是为了保证数据对于容器的生命周期来说是持久化的，换言之，即使你删除了停止的容器数据也还在（除非显式加上-v选项）。 volume可以通过两种方式来创建：其一是在Dockerfile中指定VOLUME /some/dir；其二是执行docker run -v /some/dir命令来指定。这两种方式都是让Docker在主机上创建一个目录，注意默认情况下是在/var/lib/docker下的。并将其挂载到我们指定的路径(/some/dir)，当此路径在容器中不存在时，默认会自动创建它。值得注意的是，我们也可以显式指定将宿主机的某个目录或文件挂载到容器的指定位置（在上述实践环节正是这样操作的，这种方式也被称为是bind-mount）。最后强调一点，当删除使用volume的容器时，volume本身不受影响。 更多关于volume的操作请查看官方文档。 简单小结，本文阐述了两种实现在容器中运行容器的方法——docker-in-docker和socket-binding。对于docker-in-docker这种方式，虽然它存在不少问题，但它确实实现了在容器中运行容器。围绕docker-in-docker，先简单演示了其基本用法，然后进一步推广出docker-in-docker-in-docker...模式，这在理论上都是可行的。紧接从privileged选项切入阐述dind的相关原理，重点解释了jpetazzo/dind此特殊镜像的构建过程，最后描述了生产环境中dind的实践方式，即以docker-as-a-service的模式以将docker实例通过端口暴露给外部使用。另一种巧妙实现在容器中执行容器命令的方法是socket-binding。可以说，dind能够实现的，它基本都能实现，而且，它解决了各内层容器同宿主机共享镜像缓存的问题。且socket-binding的用法也较为简单，其原理简单而言，就是采用bind-mount通过-v选项将宿主机的docker daemon挂载到容器中，使得只需在容器中安装docker client（事实上，也可不安装docker client，而直接将宿主机的/usr/bin/docker挂载到容器中，同时安装docker执行所需的依赖文件即可）即可执行docker pull/push/build命令。 参考文献 docker-in-docker[1].https://hub.docker.com/_/docker[2].https://github.com/jpetazzo/dind/[3].https://blog.docker.com/2013/09/docker-can-now-run-within-docker/[4].https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities socket-binding[1].https://docs.docker.com/storage/volumes/[2].http://dockone.io/article/128]]></content>
      <categories>
        <category>devops</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s 中安装 gitlab runner]]></title>
    <url>%2F2019%2F07%2F06%2Fk8s-%E4%B8%AD%E5%AE%89%E8%A3%85-gitlab-runner%2F</url>
    <content type="text"><![CDATA[最近工作的内容属于devops领域方向，研究的课题是基于k8s并集成gitlab-ci的持续集成部署方案。个人以前只使用过Jenkins来做持续集成部署(ci/cd)，而且，当时应该是部署在云主机上的。gitlab本身一个企业级代码托管平台，在8.0版本加入了ci，并且默认为每个项目开启。持续集成基本解放了软件项目的开发测试到最终的部署上线的繁琐流程，简单而言，它保证了每一次往版本库提交的代码都符合预期。我们知道docker解决了应用打包和发布这一运维技术难题，简单而言，docker所提供的极为方便的打包机制直接打包了应用运行所需要的整个操作系统，从而保证了本地环境和云端环境的高度一致。但毕竟在整个云计算领域中，docker只是整个容器生态的一个承载点，换言之，与开发者更为密切相关的事情是定义容器组织和管理规范的容器编排技术，这属于一个更高的层次，是一个平台级的技术。kuberentes正是这样一个开源平台，简而言之，kubernetes项目解决的问题是容器的编排、调度以及集群管理，当然，它也提供了一些高级的运维功能，如路由网关、水平扩展、监控、备份以及灾难恢复等。这也使得kubernetes从一诞生就备受关注。因此，将gitlab-ci与k8s进行整合是ci/cd实践中值得期待的方案。本系列博客会阐述个人基于k8s并集成gitlab-ci的持续集成部署方案的实现过程。实践环节包括两个部分，其一是在k8s中安装gitlab runner，其二是研究集成gitlab-ci和k8s来实现一个以build-&gt;test-&gt;deploy为核心的持续集成部署流程。本文的内容为第一个部分。 本文不会过多阐述docker相关原理，也不会过多涉及到kubernetes相关原理。只会在阐述整个基于gitlab-ci和kubernetes的持续集成部署方案的过程中，涉及到的概念原理。本文从如下几个方面来完整的阐述在k8s中安装gitlab runner的流程：其一，简述gitlab-ci的核心概念及基本原理；其二，简述gitlab runner相关知识 ；其三，详细阐述gitlab runner在k8s中的安装流程；最后，对gitlab runner相关的配置文件中重要的配置进行介绍，以更深入地理解gitlab runner集成到k8s的原理。依据官方文档是使用helm作为软件安装工具，以在k8s中安装gitlab runner，但本文不会过多涉及helm的相关知识和原理。事实上，若读者不具备相关的知识基础，也没有太大影响。若读者已经对gitlab, gitlab runner已经较为熟悉，可以直接跳到第3小节。 gitlab-ci 核心概念和基本原理所谓持续集成持续部署(ci/cd, Continuous Integration, Continuous Delivery, and Continuous Deployment)，通俗而言，即在软件开发过程中，每当涉及版本库代码变更或更迭时，通过自动化执行一些由开发人员定义的脚本，以最小化引入错误的风险。自动化执行脚本说明（几乎）不需要人为干预。具体而言，持续集成表示当开发人员提交代码到版本库时（不一定是master分支），都会触发一系列的关于测试、构建等步骤的脚本自动化执行，以验证版本库中当前的代码所产生的效果是符合预期的。另外，持续交付(Continuous Delivery)和持续部署(Continuous Deployment)的区别在于是否需要人为干预，以部署项目到生产环境，而后者不需要人为干预。 gitlab-ci/cd集成了上述功能。其基本原理是版本库每一次push或者merge reqeust操作都会触发一次gitlab-ci流程，即执行开发人员预先在.gitlab-ci.yml定义的一系列stage，典型的，包括build、test和deploy这几个核心阶段。gitlab-ci确实较为强大，提供了丰富的功能，以实现项目开发的快速更迭。比如，它可以在各stage中共享缓存，以提高各stage的构建效率。和gitlab-ci/cd相关的几个核心概念如下： pipeline，表示一次构建任务，可包含多个阶段，如依赖安装、运行测试、项目编译、服务部署。 stage，表示某个具体阶段，它们会依次串行执行，前一个成功执行后下一个才会执行，相反，若前一个执行失败，下一个则默认不会执行。 job，表示stage上所执行的具体工作，一个stage可包含若干个并行执行的job，只有所有的job都执行成功，整个stage才被标记为执行成功，否则标记为执行失败。 图1来自gitlab官网，阐述了gitlab-ci的一个典型工作流程。使用gitlab作为代码托管工具，你不需要额外的第三方ci/cd软件，并且，它提供整个流程的可视化界面。 图1 gitlab workflow example 下面的.gitlab-ci.yml模板文件同样来自gitlab官网，它是依赖spring boot的java应用服务的一个.gitlab-ci.yml示例。没有任何复杂的内容，整个pipeline中包含test和build两个stage。在全局before_script所定义的脚本会在所有stage执行之前被执行，artifacts表示此stage会生成一个artifact（比如一个war包或者可执行文件），最后only表示只会在对应的分支下执行。因此，当你将此.gitlab-ci.yml文件放到你的项目的根目录时，则表示此项目的gitlab-ci的功能已经开启，当你往版本库中push代码时，你会看到它会起作用了——自动执行你在.gitlab-ci.yml中定义的脚本（前提是你已经安装好了gitlab和gitlab runner，且将runner注册到了gitlab仓库中对应的项目，这会在后面提及）。你可以在这里找到更多的.gitlab-ci.yml示例。 123456789101112131415161718192021222324image: java:8stages: - build - deploy before_script: - chmod +x mvnw build: stage: build script: ./mvnw package artifacts: paths: - target/demo-0.0.1-SNAPSHOT.jarproduction: stage: deploy script: - curl --location "https://cli.run.pivotal.io/stable?release=linux64-binary&amp;source=github" | tar zx - ./cf login -u $CF_USERNAME -p $CF_PASSWORD -a api.run.pivotal.io - ./cf push only: - master gitlab runner 相关知识当你读完上节内容中一个示例后，你会想你在.gitlab-ci.yml中定义的脚本到底是在哪里执行的，换言之，是什么提供了你执行这些stage的资源。是的，这就是gitlab runner所做的。它会同gitlab-ci协同工作，gitlab runner用于运行stage中定义的job（job是runner执行的最小单位）。在阐述runner在k8s中的安装流程之前，让我们先来了解下gitlab runner的基本知识。 当你安装gitlab runner并将它注册到某个项目后（当然你可以使用shared runner，在这种情况下，此runner就不会隶属于某个项目，而是被一组或所有的项目所共享使用），你可以使用它来执行你在.gitlab-ci.yml中定义的job，只要runner能够访问gitlab server所在的网络，则gitlab和runner就能通过api进行网络通信，准确而言，是runner会定期轮询gitlab server是否有等待被执行的pipeline。从本质上而言，runner只是一个使用go语言编写的进程，当利用它来执行.gitlab-ci.yml中定义的pipeline时，它会clone对应的项目，然后执行你预定义在job中的脚本。 前面提到，当多个项目共享同一个runner时，则称此runner为shared runner，且理想状况下，runner不应该同gitlab server安装在同一台机器上，这会影响gitlab的正常工作。且gitlab admin只能注册shared runner。具体而言，runner主要包括如下三种： shared runner，它主要服务于多个项目中具有类似需求的job，显然，使用shared runner可以不需要为每一个项目配置单独的runner，且通常来说，shared runner与项目是多对多的关系，类似于资源池。shared runner采用fair useage queue来调度job，这可以防止某个项目由于定义了过多的job而独占整个可用的shared runner集合。 specific runner，它主要服务于具有特殊需求的项目，通常会结合tag来将specific runner与项目进行绑定，specific runner采用FIFO的方式调度job。值得注意的是，specific runner也可服务于多个项目，只是你需要显式的为每个项目enable它们。 group runner，定义在group runner集合中的runner会服务于一组项目，与shared runner不同的是，它也采用的是FIFO的方式来调度job，这意味着一个定义了较多job的项目可能会长时间独占所有runner。 最后，通过一个示例来简要阐述shared runner是如何调度job的，即fair useage queue的原理，这些内容基本来自于官方文档。示例如下：若我们为project 1定义了3个job，为project 2定义了2个job，为project 3定义了1个job，则一个典型的调度流程如下：首选会调度p1-j1(job 1 of project 1)，因为此时它是所有不存在正运行的job的项目中编号最小的job，这句话很重要。然后调度p2-j4，因为此时porject 1有一个正运行的作业job 1。再调度p3-j6，原因是类似的。其次，调度p1-j2，因为它是存在正在运行job的项目中，包含最少运行的job数（每个项目都有1个）的项目的尚未运行的job的编号。接下来的调度依次是p2-j5、p1-j3。需要注意的是，上面描述的调度顺序的前提是每个被调度的job都一直处于运行状态。因为，若当我们调度p1-j1时，它立刻完成了，则下一个调度的job则仍然从project 1中挑选，即为p1-j2。因此，总结一下，当调度job时，首先看哪个project存在最少的处于运行状态的job数量，然后在此project中选择尚未运行的job集合中编号最小的job。 此小节阐述了gitlab runner的基本原理，以及不同类型的runner的适用情形，同时通过一个示例来阐述shared runner是如何调度job的。更多详细内容可参考官方文档。 k8s 中安装gitlab runner 的详细流程本小节侧重实践，详细阐述在k8s中安装gitlab runner的整个过程。其中，gitlab的版本是GitLab Community Edition 9.4.2，minikube的版本是v1.2.0，Kubernetes的版本是v1.15.0，最后Docker的版本是18.09.6。具体而言，主要包括两个方面的内容：一是minikube安装的注意事项，其次是在k8s中部署gitlab runner的详细流程。基本都是参考官方文档。 minikube 安装注意事项需要说明的是，在这之前你应该有一个kubernetes集群，笔者的实验环境是VMware® Workstation 15 Pro安装ubuntu18.04 desktop系统，然后搭建了minikube（单节点）的k8s环境，vm driver采用的是kvm2。 笔者之前尝试过virtualbox hypervisor，并以virtualbox和kvm2作为vm driver，但都没有成功。简单说，virtualbox hypervisor不支持对硬件的虚拟化。相关的issue可以看这里和这里，virtualbox官方一个相关说明，在这里。如果有读者在virtualbox hypervisor下成功安装kvm2或virtual作为vm driver，还请留言。 关于minikube的安装，直接参考官方文档即可。但需要注意GFW，若不能上外网，可以尝试通过拉取国内镜像源来安装，参考这里。另外一个在安装前需要关注的操作是：egrep --color &#39;vmx|svm&#39; /proc/cpuinfo，必须确保此命令的输出不为空，否则，表明你的系统不支持虚拟化。安装完之后，可以执行一个hello world，参考这里，以确认minkube已经成功安装。 k8s 中安装gitlab runner前述提到gitlab runner只是一个使用go编写的程序。因此，理论上在任何安装了go的环境都能安装gitlab runner，不仅仅局限于k8s的环境，详解可参考这里。但若将runner安装在k8s中，其原理与其它方式还是略有区别，这个在后面阐述。另外，在前一小节中提到，runner会周期性的轮询gitlab server以确认当前是否有需要执行的pipeline，换言之，runner是可以安装在你本地环境的（不需要一个外网能够访问的ip），但gitlab server若安装在本地环境（主机或docker），你要确保它能够被runner访问到。 官方提供的在k8s安装runner的最新教程采用了helm，因此，在安装runner前需要提前在k8s集群中安装helm。简单而言，helm是一个在k8s环境下的软件包管理工具，类似于ubuntu下的apt-get或centos下的yum。helm会为我们管理一个软件包所包含的一系列配置文件，通过使用helm，应用发布者可以很方便地打包(pakcage)应用、管理应用依赖关系和应用版本，并将其发布应用到软件仓库。另外，helm还提供了k8s上的软件部署和卸载、应用回滚等高阶功能。helm是一个典型的cs架构，为了让helm帮助我们管理k8s中的软件包，它会将tiller server安装在k8s集群中，然后使用helm client与之通信来完成指定功能。在helm安装过程中，需要注意的就是helm的权限(RBAC)的配置，在笔者的实验中，为了方便测试，给予了tiller这个ServiceAccount的role为cluster-admin。图2为helm的相关安装配置。更多关于helm的中文资料可以参考这里和这里。 图2 k8s中安装helm的相关配置 事实上，所谓的在k8s中安装gitlab runner，也就是将gitlab runner这个helm chart包安装在k8s中，runner具体是使用kubernetes executor执行job，executor会连接到k8s集群中的kubernetes API，并为每个job创建一个pod。pod是k8s中应用编排的最小单元（不是container），相当于一个逻辑/虚拟机主机，它包含了一组共享资源的contaienr，这些container共享相同的network namespace，可通过localhost通信，另外，这些container可声明共享同一个volume。通常而言，为gitlab-ci的每个job动态创建的pod至少包含两个container（也有三个的情况），分别是build container和service container，其中build container即用于构建job，而当在.gitlab-ci.yml中定义了service标签时，就会此service container来运行对应的service，以连接到build container，并协助它完成指定功能。这同docker中的link container原理类似。最后，当使用docker/docker+machine/kubernetes的executors时，gitlab runner会使用基于helper image的help container，它的使用是处理git、artifacts以及cache相关操作。它包含了gitlab-runner-helper二进制包，提供了git、git-lfs、SSL certificates store等命令。但当使用kubernetes executor时，runner会临时从gitlab/gitlab-runner-helper下载镜像而并非从本地的归档文件中加载此二进制文件。 现在可以执行正式的安装操作了。安装之前，通常我们需要配置runner，这通过在values.yaml中自定义特定选项来实现，以覆盖默认选项值。配置过程也较为简单，唯一必须配置的选项是gitlabUrl和runnerRegistrationToken，前者即为gitlab server的url，它可以是一个完整域名（如https://example.gitlab.com），也可以是一个ip地址（记得不要漏掉端口号），而后者则为你的gitlab的token，以表明你具备向gitlab添加runner的权限。这两个值可以从gitlab-project-settings-pipelines下获取到（注意因为笔者的gitlab帐户只是普通帐户，意味着只能注册specific runner，它与admin的稍有不同）。图3显示了这两个选项参数。 图3 runner所需的gitlabUrl和token参数 确认了这两个最核心的配置选项后，如果你不需要覆盖其它的默认选项值，就可以开始安装了，非常简单。仅有两个步骤： 其一，将gitlab这个repository添加到helm repository list中。执行下面的命令即可：helm repo add gitlab https://charts.gitlab.io其二，使用helm来安装gitlab runner chart，如下：helm install --namespace &lt;NAMESPACE&gt; --name gitlab-runner -f &lt;CONFIG_VALUES_FILE&gt; gitlab/gitlab-runner其中&lt;NAMESPACE&gt;指定了你需要将runner安装在哪个namespace，因为，你很可能需要预先创建此namespace，这通过kubectl create namespace &lt;NAMESPACE&gt;命令来实现。而后一个参数则为你定义的values.yml配置文件的路径。 附带介绍下，另外两个重要的操作——gitlab runner的升级以及卸载操作。升级操作同安装操作非常类似，&lt;RELEASE-NAME&gt;即为gitlab runner，release是helm的概念，表示安装在k8s中的一个软件：helm upgrade --namespace &lt;NAMESPACE&gt; -f &lt;CONFIG_VALUES_FILE&gt; &lt;RELEASE-NAME&gt; gitlab/gitlab-runner 最后的卸载操作可通过如下命令实现：helm delete --namespace &lt;NAMESPACE&gt; &lt;RELEASE-NAME&gt;值得注意的是，即使执行了此卸载操作，helm仍然保留已删除release的记录，这允许你回滚已删除的资源并重新激活它们。若要彻底删除，可以加上--purge选项。 至此，最简版本的gitlab runner已经安装完毕。当执行helm install命令后，它会打印出此次安装所涉及的对象资源，如下： 1234567891011121314151617181920212223242526272829303132333435363738NAME: gitlab-runnerLAST DEPLOYED: Fri Jul 5 11:06:30 2019NAMESPACE: kube-gitlab-testSTATUS: DEPLOYEDRESOURCES:==&gt; v1/ConfigMapNAME DATA AGEgitlab-runner-gitlab-runner 5 0s==&gt; v1/Pod(related)NAME READY STATUS RESTARTS AGEgitlab-runner-gitlab-runner-6f996b5464-8wwnz 0/1 Init:0/1 0 0s==&gt; v1/SecretNAME TYPE DATA AGEgitlab-runner-gitlab-runner Opaque 2 0s==&gt; v1/ServiceAccountNAME SECRETS AGEgitlab-runner-gitlab-runner 1 0s==&gt; v1beta1/DeploymentNAME READY UP-TO-DATE AVAILABLE AGEgitlab-runner-gitlab-runner 0/1 1 0 0s==&gt; v1beta1/RoleNAME AGEgitlab-runner-gitlab-runner 0s==&gt; v1beta1/RoleBindingNAME AGEgitlab-runner-gitlab-runner 0sNOTES:Your GitLab Runner should now be registered against the GitLab instance reachable at: &quot;http://*******/&quot; 你也可以执行如图4中的命令，以确认是否安装成功，甚至使用-o yaml选项来查看各对象配置的详细内容。 图4 k8s中安装的gitlab ruuner详情 安装成功后，如图5所示，我们可以到gitlab管理上查看已经有一个runner实例可供特定的项目使用了，记得在这之前先在gitlab上创建一个示例项目，不然，你可能找不到此页面的位置。 图5 gitlab 特定项目所关联的 runner 列表 在本小节的最后，我们来看一下values.yml文件中定义的核心配置选项。其它的配置在下一小节阐述。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788## The GitLab Server URL (with protocol) that want to register the runner against## ref: https://docs.gitlab.com/runner/commands/README.html#gitlab-runner-register## gitlab server 的地址gitlabUrl: https://gitlab.example.com/## The registration token for adding new Runners to the GitLab server. This must## be retrieved from your GitLab instance.## ref: https://docs.gitlab.com/ee/ci/runners/## 向 gitlab server 添加 runner 的令牌runnerRegistrationToken: ""## Set the certsSecretName in order to pass custom certificates for GitLab Runner to use## Provide resource name for a Kubernetes Secret Object in the same namespace,## this is used to populate the /etc/gitlab-runner/certs directory## ref: https://docs.gitlab.com/runner/configuration/tls-self-signed.html#supported-options-for-self-signed-certificates## 当需要向 gitlab runner 提供自定义证书时，可在此附上证书对应的secret名称##（secret 需提前安装在k8s）#certsSecretName:## Configure the maximum number of concurrent jobs## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section## 使用所有注册的 runner 能并行执行的 job 数量的上限，0 表示不限制concurrent: 10## Defines in seconds how often to check GitLab for a new builds## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section## runner 轮询 gitlab server 来查询是否有待执行的 pipeline 的间隔时间/scheckInterval: 30## For RBAC support:## runner 的 RBAs 权限配置（RBAC是kubernetes 1.6版本默认使用的权限机制）。## 若你想 helm 为你创建对应角色、权限及角色绑定，则设置为 true，否则若你已额外创建，则设置为falserbac: create: true ## Run the gitlab-bastion container with the ability to deploy/manage containers of jobs ## cluster-wide or only within namespace ## false 表示创建的角色是隶属于某个 namespace,反之属于 cluster 范围内，后者权限更大 clusterWideAccess: false ## If RBAC is disabled in this Helm chart, use the following Kubernetes Service Account name. ## 若你额外创建了 RBAC 相关配置，则在这里指定 ServiceAccount 的名称 ## default 是每个 namespace 中自带的一个帐户，但其拥有的角色可能不具备执行某些操作的权限 # serviceAccountName: default## Configuration for the Pods that the runner launches for each new job## 配置 runner 为每一个 job 动态创建的 podrunners: ## Default container image to use for builds when none is specified ## 基础容器所使用的镜像 image: ubuntu:18.04 ## Run all containers with the privileged flag enabled ## This will allow the docker:stable-dind image to run if you need to run Docker ## commands. Please read the docs before turning this on: ## ref: https://docs.gitlab.com/runner/executors/kubernetes.html#using-docker-dind ## 这个选项对于 ci/cd 应用场景至关重要。简单而言，true 表示启用 pod 中容器的特权， ## 此时，容器几乎具有容器外运行在宿主机的进程完全相同的权限，可以访问所有的设备，存在一定风险 ## 但是开启此选项是实现 docker-in-docker 的前提，后面会详细阐述 dind privileged: false ## Namespace to run Kubernetes jobs in (defaults to 'default') ## 配置 runner 为每个 job 创建的 pod 所运行的 namespace # namespace: ## Build Container specific configuration ## build 容器，用作 stage 中的基础容器 builds: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m # 基础 cpu 占用量，表示 0.1 的cpu，m 表示 Milli 毫 memoryRequests: 128Mi # 基础内存占用量，表示 128M 的内存 ## Service Container specific configuration ## service 容器，通过 link build 容器，以协助 build 容器完成特定功能 services: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## Helper Container specific configuration ## helper 容器，用于执行 git, git-lfs, SSL certificates store 等命令 helpers: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi 至此，关于在k8s中安装gitlab runner的完整流程已经阐述完毕。事实上，后面的版本中使用helm来安装gitlab runner是非常便捷的。但同时，它也向我们隐藏了相关的配置细节，为了更好的理解runner在k8s中的运行原理，有必要详细了解相关的配置文件。最后，值得注意的是，runner使用kubernetes executor的原理也是值得关注的。 gitlab runner 配置详解为了进一步理解gitlab ruuner的相关原理，本小节会详细解读有关其value.yml配置选项，在正式的生产环境中，我们可能需要自定义其中的大部分的配置选项的值。其中，核心配置选项已在上一节中详细阐述。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218## GitLab Runner Image#### ref: https://hub.docker.com/r/gitlab/gitlab-runner/tags/## gitlab runner 的默认镜像# image: gitlab/gitlab-runner:alpine-v11.6.0## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images## gitlab runner 镜像的拉取策略，Never/IfNotPresent/AlwaysimagePullPolicy: IfNotPresent## ref: https://docs.gitlab.com/runner/commands/README.html#gitlab-runner-register## gitlab server 的地址# gitlabUrl: http://gitlab.your-domain.com/## ref: https://docs.gitlab.com/ce/ci/runners/README.html## 向 gitlab server 添加 runner 的令牌# runnerRegistrationToken: ""## The Runner Token for adding new Runners to the GitLab Server. This must## be retrieved from your GitLab Instance. It is token of already registered runner.## ref: (we don't yet have docs for that, but we want to use existing token)## 已注册的 runner 的token。不是特别清楚此选项的意义，感觉同上一个选项类似# runnerToken: ""## ref: https://docs.gitlab.com/runner/commands/README.html#gitlab-runner-unregister## 当 runner 被重新创建时，会导致 gitlab server 引用一个不存在的 runner，因此，开启此选项表示在## runner 关闭时会自动从 gitlab server 取消注册unregisterRunners: true## ref: https://docs.gitlab.com/runner/configuration/tls-self-signed.html#supported-options-for-self-signed-certificates## 当需要向 gitlab runner 提供自定义证书时，可在此附上证书对应的secret名称##（secret 需提前安装在k8s）# certsSecretName:## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section## 使用所有注册的 runner 能并行执行的 job 数量的上限，0 表示不限制concurrent: 10## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section## runner 轮询 gitlab server 来查询是否有待执行的 pipeline 的间隔时间/scheckInterval: 30## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section## gitlab runner 的日志级别# logLevel:## runner 的 RBAs 权限配置（RBAC是kubernetes 1.6版本默认使用的权限机制）。## 若你想 helm 为你创建对应角色、权限及角色绑定，则设置为 true，否则若你已额外创建，则设置为falserbac: create: false ## false 表示创建的角色是隶属于某个 namespace,反之属于 cluster 范围内，后者权限更大 clusterWideAccess: false ## 若你额外创建了 RBAC 相关配置，则在这里指定 ServiceAccount 的名称 ## default 是每个 namespace 中自带的一个帐户，但其拥有的角色可能不具备执行某些操作的权限 # serviceAccountName: default## ref: https://docs.gitlab.com/runner/monitoring/#configuration-of-the-metrics-http-server## 是否开启 metric 数据记录器，使用的是 Prometheus metrics exportermetrics: enabled: true## 配置 runner 为每一个 job 动态创建的 pod 相关选项runners: ## 基础容器(build container)默认使用的镜像 image: ubuntu:18.04 ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## 当从私有 registry 拉取镜像时，需要预先在 k8s 中创建对应的 secret，并在这里填写对应的 secret # imagePullSecrets: [] ## 镜像拉取策略，Never/IfNotPresent/Always # imagePullPolicy: "" ## Defines number of concurrent requests for new job from GitLab ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runners-section ## gitlab-ci 能够并发请求的 job 数量 # requestConcurrency: 1 ## 此 runner 是否与特定项目绑定 # locked: true ## ref: https://docs.gitlab.com/ce/ci/runners/#using-tags ## runner 所运行的 pod 所关联的 tag # tags: "" ## ref: https://docs.gitlab.com/runner/executors/kubernetes.html#using-docker-dind ## 这个选项对于 ci/cd 应用场景至关重要。简单而言，true 表示启用 pod 中容器的特权， ## 此时，容器几乎具有容器外运行在宿主机的进程完全相同的权限，可以访问所有的设备，存在一定风险 ## 但是开启此选项是实现 docker-in-docker 的前提，后面会详细阐述 dind privileged: false # gitlab runner 为 runner-token and runner-registration-token 创建的 secret 的名称 # secret: gitlab-runner ## 配置 runner 为每个 job 创建的 pod 所运行的 namespace # namespace: ## ref: https://gitlab.com/gitlab-org/gitlab-runner/blob/master/docs/configuration/autoscale.md#distributed-runners-caching ## 分布式 runner 缓存相关的配置，这里暂且忽略 cache: &#123;&#125; ## General settings # cacheType: s3 # cachePath: "gitlab_runner" # cacheShared: true ## S3 settings # s3ServerAddress: s3.amazonaws.com # s3BucketName: # s3BucketLocation: # s3CacheInsecure: false # secretName: s3access ## GCS settings # gcsBucketName: ## Use this line for access using access-id and private-key # secretName: gcsaccess ## Use this line for access using google-application-credentials file # secretName: google-application-credentials ## build 容器，用作 stage 中的基础容器 builds: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m # 基础 cpu 占用量，表示 0.1 的cpu，m 表示 Milli 毫 memoryRequests: 128Mi # 基础内存占用量，表示 128M 的内存 ## service 容器，通过 link build 容器，以协助 build 容器完成特定功能 services: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## helper 容器，用于执行 git, git-lfs, SSL certificates store 等命令 helpers: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## runner 动态创建的 pod 所关联的 ServiceAccount 的名称，它可能需要被赋予特定角色 # serviceAccountName: ## If Gitlab is not reachable through $CI_SERVER_URL ## # cloneUrl: ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## 限制 gitlab-ci 的 pod 只能被调度在指定 node 上 # nodeSelector: &#123;&#125; ## 指定 gitlab-ci 的 pod 所包含的 labels # podLabels: &#123;&#125; ## 指定 gitlab-ci 的 pod 所包含的 annotations # podAnnotations: &#123;&#125; ## ref: https://docs.gitlab.com/runner/commands/#gitlab-runner-register ## 为 gitlab-ci runner 注入指定的环境变量，注意不是 runner 动态创建的 pod 的环境变量 # env: # NAME: VALUE## ref: http://kubernetes.io/docs/user-guide/compute-resources/## 为 runner 配置资源限制resources: &#123;&#125; # limits: # memory: 256Mi # cpu: 200m # requests: # memory: 128Mi # cpu: 100m## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity## 配置 runner 所在 pod 的亲和性，同 label的作用类似，都用于指定 pod的调度策略，## 但其功能更加强大，它可以设置简单的逻辑组合，不单单是 label 所局限的简单的相等匹配affinity: &#123;&#125;## Ref: https://kubernetes.io/docs/user-guide/node-selection/## gitlab runner 的节点选择器，即指定只能运行在哪些节点上nodeSelector: &#123;&#125; # node-role.kubernetes.io/worker: "true"## List of node taints to tolerate (requires Kubernetes &gt;= 1.6)## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/## tolerations 与 taints 相关。一个被标记为 Taints 的节点，除非 pod 也被标识为可以容忍污点节点，## 否则该 Taints 节点不会被调度 pod。## 典型的，在 kubernetes 集群，Master 节点通过被标记为 taints 以保留给 Kubernetes 系统组件使用## 但若仍然希望某个 pod 调度到 taint 节点上，则必须在 Spec 中做出Toleration定义，才能调度到该节点tolerations: [] # Example: Regular worker nodes may have a taint, thus you need to tolerate the taint # when you assign the gitlab runner manager with nodeSelector or affinity to the nodes. # - key: "node-role.kubernetes.io/worker" # operator: "Exists"## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html## 环境变量，在执行 register 命令时使用，以进一步控制注册的过程和 config.toml 配置文件# envVars:# - name: RUNNER_EXECUTOR# value: kubernetes## 主机名与ip的映射，它们可以被注入到 runner 所在 pod 的 host 文件hostAliases: [] # Example: # - ip: "127.0.0.1" # hostnames: # - "foo.local" # - "bar.local" # - ip: "10.1.2.3" # hostnames: # - "foo.remote" # - "bar.remote"## 附加在 runner 所在 pod 上的 annotationspodAnnotations: &#123;&#125; # Example: # iam.amazonaws.com/role: &lt;my_role_arn&gt; 最后强调下关于namespace和ServerAccount的权限的问题。事实上gitlab runner会存在于某一个namespace，同时它会关联一个ServiceAccount，此sa决定了其为每个job动态创建的pod操作的权限问题。典型的，若其sa只具备某个namespace下的所有权限，则它不能在集群范围内其它namespace中创建pod。且动态创建的pod所在的namespace可以与runner所在namespace不同。最后，由runner动态创建出来的pod，也会关联一个sa，此sa所绑定的role决定了在对应的job中能够执行的脚本操作，比如，在job中又创建一个pod（我们后面的ci/cd方案中就属于此种情况），那么，此pod所处的namespace也可以和job对应的pod所处的namespace不同，这取决于job所关联的sa的权限。读者若有兴趣，完全可以自己尝试一下这些配置会带来什么影响。 至此，关于runner的相关的配置已经讲解完毕。大部分还是容易理解的，读者可以通过实验来验证它们的功能。在生产环境中，可能需要覆盖大多默认配置选项。 简单小结，本文详细分析了在k8s中安装gitlab runner的完整流程。在阐述具体的安装操作前，先是阐述了gitlab-ci/cd的核心概念和基本原理，这有助于了解gitlab-ci到底是如何工作。其次，阐述了gitlab runner的相关知识，gitlab runner才是定义在.gitlab-ci.yml的脚本的执行器，但它并不特殊，只是一个使用go写的应用程序而已。然后，重点阐述了k8s中安装gitlab runner的详细步骤，附带阐述了kubernetes executor的原理和helm的基本作用。同时，详细解释了gitlab runner chart包的values.yml配置文件的核心配置选项。最后，为了更深入了解gitlab runner的运行原理，简述了values.yml中几乎所有的配置选项。 需要提醒读者的是，整篇文件比较长。涉及到的内容也较多，提供的参考资料也挺多。但只要耐心跟随整个流程，在k8s中安装gitlab runner是完全没有问题的。 参考文献 minikube[1].https://kubernetes.io/docs/tasks/tools/install-minikube/[2].https://github.com/kubernetes/minikube[3].https://github.com/kubernetes/minikube/issues/2991 gitlab-ci/cd[1].https://docs.gitlab.com/ee/ci/[2].https://docs.gitlab.com/ee/ci/introduction/index.html#how-gitlab-cicd-works[3].https://docs.gitlab.com/ee/ci/yaml/README.html[4].https://gitlab.com/gitlab-examples gitlab runner[1].https://docs.gitlab.com/ee/ci/runners/README.html[2].https://docs.gitlab.com/runner/[3].https://docs.gitlab.com/ce/ci/docker/using_docker_images.html#what-is-a-service helm[1].https://helm.sh/docs/[2].https://zhaohuabing.com/2018/04/16/using-helm-to-deploy-to-kubernetes/[3].https://www.qikqiak.com/post/first-use-helm-on-kubernetes/ kubernetes executor[1].https://docs.gitlab.com/runner/executors/kubernetes.html install runner in k8s[1].https://docs.gitlab.com/runner/install/kubernetes.html[2].https://docs.gitlab.com/runner/configuration/advanced-configuration.html]]></content>
      <categories>
        <category>devops</category>
        <category>持续集成</category>
      </categories>
      <tags>
        <tag>gitlab-ci</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nsq diskqueue 源码简析]]></title>
    <url>%2F2019%2F05%2F19%2Fnsq-diskqueue-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[diskQueue是nsq分布式实时消息队列的消息持久化存储组件。考虑到nsq为了限制消息积压所占的内存，同时也为了保证节点宕机消息尽可能丢失，因此，当内存消息队列memoryMsgChan的长度达到配置的阈值时，会将消息写入到持久化存储消息队列中。是的，diskQueue提供两个关键特性，一是持久化存储，二是队列接口。而在nsq系统中diskQueue的用法和memoryMsgChan（buffered go channel）基本上是相同的，因此，对于生产者或者消费者而言，消息的存储方式对于它们而言是透明的，它们只需要调用相应的接口投递或获取消息即可。这确实是数据存储的最佳实践。在此前的6篇文章已经将nsq相关模块的源码阐述完毕。本文的主题是diskQueue——持久化消息队列存储组件。重点阐述其实现原理，同时分析其为上层应用程序提供的接口。 据官方介绍，diskQueue是从nsq项目中抽取而来，将它单独作为一个项目go-diskqueue。它本身比较简单，只有一个源文件diskqueue.go。本文阐述的内容更完整的源码注释可在这里找到，注释源码版本为v1.1.0，仅供参考。 本文阐述的内容可分两个部分：其一，分析diskQueue的工作原理，这包括如何从文件中读取一条消息以及如何写入一条消息到文件系统中（考虑到写入或读取文件时，可能涉及到文件的切换，因为它需要保证单个文件的大小 不能过大，因此采用滚动写入的方式。另外在读取或写入文件时，也要考虑文件损坏的情况）。同时分析diskQueue提供给应用程序的接口，典型地，包括将消息写入到diskQueue中，从diskQueue读取一条消息，以及删除或清空diskQueue存储的消息。但本文的行文方式为：从diskQueue实例结构开始，围绕diskQueue.ioLoop主循环展开，阐述上述介绍的各个流程。 diskQueue 实例结构diskQueue结构所包含字段可以分为四个部分： 第一部分为diskQueue当前读写的文件的状态，如读取或写入索引readPos/writePos，读取或写入文件编号readFileNum/writeFileNum，以及depth表示当前可供读取或消费的消息的数量； 第二部分为diskQueue的元数据信息，如单个文件最大大小maxBytesPerFile，每写多少条消息需要执行刷盘操作syncEvery等待； 第三部分是读写文件句柄readFile/wirteFile，以及文件读取流reader或写入缓冲writeBuf； 最后一部分为用于传递信号的内部管道，如writeChan，应用程序可通过此管道向diskQueue间接压入消息，emptyChan应用程序通过此管道间接发出清空diskQueue的信号等。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// diskQueue 实现了一个基于后端持久化的 FIFO 队列type diskQueue struct &#123; // run-time state (also persisted to disk) // 运行时状态，需要被持久化 readPos int64 // 当前的文件读取索引 writePos int64 // 当前的文件写入索引 readFileNum int64 // 当前读取的文件号 writeFileNum int64 // 当前写入的文件号 depth int64 // diskQueue 中等待被读取的消息数 sync.RWMutex // instantiation time metadata // 初始化时元数据 name string // diskQueue 名称 dataPath string // 数据持久化路径 maxBytesPerFile int64 // 目前，此此属性一旦被初始化，则不可变更 minMsgSize int32 // 最小消息的大小 maxMsgSize int32 // 最大消息的大小 syncEvery int64 // 累积的消息数量，才进行一次同步刷新到磁盘操作 syncTimeout time.Duration // 两次同步之间的间隔 exitFlag int32 // 退出标志 needSync bool // 是否需要同步刷新 // keeps track of the position where we have read // (but not yet sent over readChan) // 之所以存在 nextReadPos &amp; nextReadFileNum 和 readPos &amp; readFileNum // 是因为虽然消费者已经发起了数据读取请求，但 diskQueue 还未将此消息发送给消费者， // 当发送完成后，会将 readPos 更新到 nextReadPos，readFileNum 也类似 nextReadPos int64 // 下一个应该被读取的索引位置 nextReadFileNum int64 // 下一个应该被读取的文件号 readFile *os.File // 当前读取文件句柄 writeFile *os.File // 当前写入文件句柄 reader *bufio.Reader // 当前文件读取流 writeBuf bytes.Buffer // 当前文件写入流 // exposed via ReadChan() // 应用程序可通过此通道从 diskQueue 中读取消息， // 因为 readChan 是 unbuffered的，所以，读取操作是同步的 // 另外当一个文件中的数据被读取完时，文件会被删除，同时切换到下一个被读取的文件 readChan chan []byte // internal channels // 应用程序通过此通道往 diskQueue压入消息，写入操作也是同步的 writeChan chan []byte writeResponseChan chan error // 可通过此通道向应用程序返回消息写入结果 emptyChan chan int // 应用程序可通过此通道发送清空 diskQueue 的消息 emptyResponseChan chan error // 可通过此通道向应用程序返回清空 diskQueue 的结果 exitChan chan int // 退出信号 exitSyncChan chan int // 保证 ioLoop 已退出的信号 logf AppLogFunc&#125; // diskQueue.go diskQueue 构造方法首先考虑diskQueue在nsq系统中什么情况下会被实例化？答案是在实例化topic或channel时候。diskQueue的实例过程比较简单，首先根据传入的参数构造diskQueue实例，然后从配置文件中加载diskQueue的重要的属性状态，这包括readPos &amp; writePos,readFileNum &amp; writerFileNum和depth，并初始化nextReadFileNum和nextReadPos两个重要的属性。最后异步开启消息处理的主循环ioLoop方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// New 方法初始化一个 diskQueue 实例，并从持久化存储中加载元数据信息，然后开始启动func New(name string, dataPath string, maxBytesPerFile int64, minMsgSize int32, maxMsgSize int32, syncEvery int64, syncTimeout time.Duration, logf AppLogFunc) Interface &#123; // 1. 实例化 diskQueue d := diskQueue&#123; name: name, dataPath: dataPath, maxBytesPerFile: maxBytesPerFile, minMsgSize: minMsgSize, maxMsgSize: maxMsgSize, readChan: make(chan []byte), writeChan: make(chan []byte), writeResponseChan: make(chan error), emptyChan: make(chan int), emptyResponseChan: make(chan error), exitChan: make(chan int), exitSyncChan: make(chan int), syncEvery: syncEvery, syncTimeout: syncTimeout, logf: logf, &#125; // 2. 从持久化存储中初始化 diskQueue 的一些属性状态： readPos, writerPos, depth 等 err := d.retrieveMetaData() // ... // 3. 在一个单独的 goroutien 中执行主循环 go d.ioLoop() return &amp;d&#125; // diskQueue.go// 从持久化存储中初始化 diskQueue 的一些属性状态func (d *diskQueue) retrieveMetaData() error &#123; var f *os.File var err error // 1. 获取元数据文件名 *.diskqueue.meta.dat，并打开文件，准备读取文件 fileName := d.metaDataFileName() f, err = os.OpenFile(fileName, os.O_RDONLY, 0600) if err != nil &#123; return err &#125; defer f.Close() // 2. 从文件中内容初始化特定状态属性信息 readPos, writerPos, depth var depth int64 _, err = fmt.Fscanf(f, "%d\n%d,%d\n%d,%d\n", &amp;depth, &amp;d.readFileNum, &amp;d.readPos, &amp;d.writeFileNum, &amp;d.writePos) if err != nil &#123; return err &#125; // 3. 初始化 nextReadFileNum 和 nextReadPos atomic.StoreInt64(&amp;d.depth, depth) d.nextReadFileNum = d.readFileNum d.nextReadPos = d.readPos return nil&#125; diskQueue 消息处理主循环diskQueue消息处理主循环ioLoop在其被实例化后就会开始执行。diskQueue包含的逻辑主要包括四个方面：从文件中读取一条消息，并压入到readChan管道中；将应用程序传入的消息，写入到文件；每隔一段时间，将写入缓冲的数据执行刷盘动作；最后是当应用程序调用清空diskQueue的接口时，执行删除并关闭diskQueue的动作。同时，笔者在阐述这些流程的实现细节的同时，将应用程序如何同diskQueue交互放在一起串联分析。下面的代码是ioLoop的大致框架，为了使框架更清晰省略了细节： 12345678910111213141516171819202122232425262728293031// diskQueue 消息处理主循环func (d *diskQueue) ioLoop() &#123; for &#123; // 1. 只有写入缓冲中的消息达到一定数量，才执行同步刷新到磁盘的操作 // ... // 2. 刷新磁盘操作，重置计数信息，即将 writeFile 流刷新到磁盘，同时持久化元数据 // ... // 3. 从文件中读取消息的逻辑 // ... select &#123; // 4. 当读取到数据时，将它压入到 r/readChan 通道， // 同时判断是否需要更新到下一个文件读取，同时设置 needSync case r &lt;- dataRead: // ... // 5. 收到清空持久化存储 disQueue 的消息 case &lt;-d.emptyChan: // (当应用程序调用 diskQueue.Empty 方法时触发) // ... // 6. 收到写入消息到磁盘的消息 (当应用程序调用 diskQueue.Put 方法时触发) case dataWrite := &lt;-d.writeChan: // ... // 7. 定时执行刷盘操作，在存在数据等待刷盘时，才需要执行刷盘动作 case &lt;-syncTicker.C: // ... // 8. 退出信号 case &lt;-d.exitChan: goto exit &#125; &#125;exit: // ...&#125; // diskQueue.go diskQueue 读取消息从diskQueue读取一条消息涉及到的ioLoop方法中的步骤3和4，其中步骤2的核心逻辑为：若当前持久化中还有未被读取或消费的消息，则尝试从特定的文件(readFileNum)、特定偏移位置(readPos)读取一条消息。这个过程并不复杂，值得注意的一点是：程序中还使用了另外一组与读取相关的状态(nextReadFileNum和nextReadPos)。当消息未从文件中读取时，readPos == nextReadPos &amp;&amp; readFileNum == nextReadFileNum ，当消息已从文件中读出但未发送给应用程序时，readPos + totalBytes == nextReadPos &amp;&amp; readFileNum == nextReadFileNum（若涉及到文件切换，则nextReadFileNum++ &amp;&amp; nextReadPos == 0），当消息已经发送给应用程序时，readPos == nextReadPos &amp;&amp; readFileNum == nextReadFileNum。换言之，之所以存在nextReadFileNum和nextReadPos是因为虽然消费者已经发起了数据读取请求，但 diskQueue还未将此消息发送给消费者，当发送完成后，会将它们相应更新。好，文件读取过程已经阐述完毕。当消息从文件中读取出来后，是通过diskQueue.readChan发送给上层应用程序的，上层应用程序通过调用diskQueue.ReadChan获取到此管道实例，并一直等待从此管道接收消息。相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// 获取 diskQueu 的读取通道，即 readChan，通过此通道从 diskQueue 中读取/消费消息func (d *diskQueue) ReadChan() chan []byte &#123; return d.readChan&#125;func (d *diskQueue) ioLoop() &#123; var dataRead []byte var err error var count int64 var r chan []byte syncTicker := time.NewTicker(d.syncTimeout) for &#123; // dont sync all the time :) // 1. 只有写入缓冲中的消息达到一定数量，才执行同步刷新到磁盘的操作 // ... // 2. 刷新磁盘操作，重置计数信息，即将 writeFile 流刷新到磁盘，同时持久化元数据 // ... // 3. 若当前还有数据（消息）可供消费 // （即当前读取的文件编号 readFileNum &lt; 目前已经写入的文件编号 writeFileNum // 或者 当前的读取索引 readPos &lt; 当前的写的索引 writePos） // 因为初始化读每一个文件时都需要重置 readPos = 0 if (d.readFileNum &lt; d.writeFileNum) || (d.readPos &lt; d.writePos) &#123; // 保证当前处于可读取的状态，即 readPos + totalByte == nextReadPos， // 若二者相等，则需要通过 d.readOne 方法先更新 nextReadPos if d.nextReadPos == d.readPos &#123; dataRead, err = d.readOne() if err != nil &#123; d.logf(ERROR, "DISKQUEUE(%s) reading at %d of %s - %s", d.name, d.readPos, d.fileName(d.readFileNum), err) d.handleReadError() continue &#125; &#125; // 取出读取通道 readChan r = d.readChan &#125; else &#123; // 当 r == nil时，代表此时消息已经全部读取完毕， // 因此使用 select 不能将数据（消息）压入其中 r = nil &#125; select &#123; // 4. 当读取到数据时，将它压入到 r/readChan 通道， // 同时判断是否需要更新到下一个文件读取，同时设置 needSync case r &lt;- dataRead: count++ // 更新当前等待刷盘的消息数量 // 判断是否可以将磁盘中读取的上一个文件删除掉（已经读取完毕），同时需要设置 needSync // 值得注意的是，moveForward 方法中将 readPos 更新为了 nextReadPos， // 且 readFileNum 也被更新为 nextReadFileNum // 因为此时消息已经发送给了消费者了。 d.moveForward() // 5. 收到清空持久化存储 disQueue 的消息 case &lt;-d.emptyChan: // (当应用程序调用 diskQueue.Empty 方法时触发) // ... // 6. 收到写入消息到磁盘的消息 (当应用程序调用 diskQueue.Put 方法时触发) case dataWrite := &lt;-d.writeChan: // ... // 7. 定时执行刷盘操作，在存在数据等待刷盘时，才需要执行刷盘动作 case &lt;-syncTicker.C: // ... // 8. 退出信号 case &lt;-d.exitChan: goto exit &#125; &#125;exit: syncTicker.Stop() d.exitSyncChan &lt;- 1&#125; // diskQueue.go 当消息被压入到readChan管道后，随即更新等待刷盘的消息数量，然后调用diskQueue.moveForward方法判断是否可以将磁盘中读取的上一个文件删除掉（已经读取完毕），同时考虑是否需要设置needSync（因为即将读取一个新的文件），最后复原readFileNum和readPos并更新等待被读取的消息数量depth。相关源码如下： 123456789101112131415161718// 检查当前读取的文件和上一次读取的文件是否为同一个，即读取是否涉及到文件的更换，// 若是，则说明可以将磁盘中上一个文件删除掉，因为上一个文件包含的消息已经读取完毕，// 同时需要设置 needSyncfunc (d *diskQueue) moveForward() &#123; oldReadFileNum := d.readFileNum d.readFileNum = d.nextReadFileNum d.readPos = d.nextReadPos depth := atomic.AddInt64(&amp;d.depth, -1) if oldReadFileNum != d.nextReadFileNum &#123; // 每当准备读取一个新的文件时，需要设置 needSync d.needSync = true fn := d.fileName(oldReadFileNum) err := os.Remove(fn) // 将老的文件删除 // ... &#125; // 检测文件末尾是否已经损坏 d.checkTailCorruption(depth)&#125; // diskQueue.go 注意到在moveForward方法的最后，还检查了文件末尾是否损坏。它先通过元数据信息（4个变量）判断是否已经读到了最后一个文件的末尾，若未到，则返回。否则，通过depth与0的大小关系来判断文件损坏的类型或原因。详细可以查看源码中的注释，解释得较为清楚。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 检测文件末尾是否已经损坏func (d *diskQueue) checkTailCorruption(depth int64) &#123; // 若当前还有消息可供读取，则说明未读取到文件末尾，暂时不用检查 if d.readFileNum &lt; d.writeFileNum || d.readPos &lt; d.writePos &#123; return &#125; // we've reached the end of the diskqueue // if depth isn't 0 something went wrong // 若代码能够执行，则正常情况下，说明已经读取到 diskQueue 的尾部， // 即读取到了最后一个文件的尾部了，因此，此时的 depth(累积等待读取或消费的消息数量) // 应该为0,因此若其不为0,则表明文件尾部已经损坏，报错。 // 一方面，若其小于 0,则表明初始化加载的元数据已经损坏（depth从元数据文件中读取而来） // 原因是：实际上文件还有可供读取的消息，但depth指示没有了，因此 depth 计数错误。 // 否则，说明是消息实体数据存在丢失的情况 // 原因是：实际上还有消息可供读取 depth &gt; 0,但是文件中已经没有消息了，因此文件被损坏。 // 同时，强制重置 depth，并且设置 needSync if depth != 0 &#123; if depth &lt; 0 &#123; d.logf(ERROR, "DISKQUEUE(%s) negative depth at tail (%d), metadata corruption," \ resetting 0...", d.name, depth) &#125; else if depth &gt; 0 &#123; d.logf(ERROR, "DISKQUEUE(%s) positive depth at tail (%d), data loss, resetting 0...", d.name, depth) &#125; // force set depth 0 atomic.StoreInt64(&amp;d.depth, 0) d.needSync = true &#125; // 另外，若 depth == 0。 // 但文件读取记录信息不合法 d.readFileNum != d.writeFileNum || d.readPos != d.writePos // 则跳过接下来需要被读或写的所有文件，类似于重置持久化存储的状态，格式化操作 // 同时设置 needSync if d.readFileNum != d.writeFileNum || d.readPos != d.writePos &#123; if d.readFileNum &gt; d.writeFileNum &#123; d.logf(ERROR, "DISKQUEUE(%s) readFileNum &gt; writeFileNum (%d &gt; %d), " \ "corruption, skipping to next writeFileNum and resetting 0...", d.name, d.readFileNum, d.writeFileNum) &#125; if d.readPos &gt; d.writePos &#123; d.logf(ERROR, "DISKQUEUE(%s) readPos &gt; writePos (%d &gt; %d), corruption, " \ "skipping to next writeFileNum and resetting 0...", d.name, d.readPos, d.writePos) &#125; d.skipToNextRWFile() d.needSync = true &#125;&#125; // diskQueue.go 当程序发现在depth == 0的情况下，即此时所有的消息已经被读取完毕，但若某个异常的情况下，可能会有：readFileNum != writeFileNum || readPos != writePos，则diskQueue会显式地删除掉接下来需要被读或写的所有文件，类似于重置持久化存储的状态或格式化操作。同时，skipToNextRWFile也可用作清空 diskQueue当前未读取的所有文件。具体代码如下： 12345678910111213141516171819202122232425262728// 将 readFileNum 到 writeFileNum 之间的文件全部删除// 将 readFileNum 设置为 writeFileNum// 即将前面不正确的文件全部删除掉，重新开始读取// 另外，其也可用作清空 diskQueue 当前未读取的所有文件的操作，重置 depthfunc (d *diskQueue) skipToNextRWFile() error &#123; var err error if d.readFile != nil &#123; d.readFile.Close() d.readFile = nil &#125; if d.writeFile != nil &#123; d.writeFile.Close() d.writeFile = nil &#125; for i := d.readFileNum; i &lt;= d.writeFileNum; i++ &#123; fn := d.fileName(i) innerErr := os.Remove(fn) // ... &#125; d.writeFileNum++ d.writePos = 0 d.readFileNum = d.writeFileNum d.readPos = 0 d.nextReadFileNum = d.writeFileNum d.nextReadPos = 0 atomic.StoreInt64(&amp;d.depth, 0) return err&#125; // diskQueue.go 至此，从diskQueue的文件系统中读取消息，并发送到上层应用程序的相关逻辑已经阐述完毕。除了需要清楚其读取核心逻辑外，还需要关注其对文件损坏的检测与处理。 diskQueue 写入消息当topic或channel所维护的内存消息队列memoryMsgChan满了时，会通过调用backend.Put方法将消息写入到diskQueue。消息写入持久化存储的逻辑比从文件系统中读取一条消息的逻辑要简单。其关键步骤为先定位写入索引，同样是先写临时文件缓冲再执行数据刷新操作，最后需要更新writePos，当发现要切换写入文件时，还要更新writeFileNum。相关代码如下： 1234567891011121314func (d *diskQueue) ioLoop() &#123; for &#123; // 1-5. // ... // 6. 收到写入消息到磁盘的消息 (当应用程序调用 diskQueue.Put 方法时触发) case dataWrite := &lt;-d.writeChan: // 删除目前还未读取的文件，同时删除元数据文件 d.emptyResponseChan &lt;- d.deleteAllFiles() count = 0 // 重置当前等待刷盘的消息数量 // ... &#125; &#125;// ...&#125; // diskQueue.go 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 将一个字节数组内容写入到持久化存储，同时更新读写位置信息，以及判断是否需要滚动文件func (d *diskQueue) writeOne(data []byte) error &#123; var err error // 1. 若当前写入文件句柄为空，则需要先实例化 if d.writeFile == nil &#123; curFileName := d.fileName(d.writeFileNum) d.writeFile, err = os.OpenFile(curFileName, os.O_RDWR|os.O_CREATE, 0600) // ... d.logf(INFO, "DISKQUEUE(%s): writeOne() opened %s", d.name, curFileName) // 2. 同时，若当前的写入索引大于0,则重新定位写入索引 if d.writePos &gt; 0 &#123; _, err = d.writeFile.Seek(d.writePos, 0) // ... &#125; &#125; // 3. 获取写入数据长度，并检查长度合法性。然后将数据写入到写入缓冲， // 最后将写入缓冲的数据一次性刷新到文件 dataLen := int32(len(data)) if dataLen &lt; d.minMsgSize || dataLen &gt; d.maxMsgSize &#123; return fmt.Errorf("invalid message write size (%d) maxMsgSize=%d", dataLen, d.maxMsgSize) &#125; d.writeBuf.Reset() err = binary.Write(&amp;d.writeBuf, binary.BigEndian, dataLen) // ... _, err = d.writeBuf.Write(data) // ... // only write to the file once _, err = d.writeFile.Write(d.writeBuf.Bytes()) // ... // 更新写入索引 writePos 及 depth，且若 writePos 大于 maxBytesPerFile， // 则说明当前已经写入到文件的末尾。 // 因此需要更新 writeFileNum，重置 writePos， // 即更换到一个新的文件执行写入操作（为了避免一直写入单个文件） // 且每一次更换到下一个文件，都需要将写入文件同步到磁盘 totalBytes := int64(4 + dataLen) d.writePos += totalBytes atomic.AddInt64(&amp;d.depth, 1) if d.writePos &gt; d.maxBytesPerFile &#123; d.writeFileNum++ d.writePos = 0 // sync every time we start writing to a new file err = d.sync() // .. &#125; return err&#125; // diskQueue.go diskQueue 清空消息当应用程序调用diskQueue.Empty接口时，会将持久化存储diskQueue中的所有消息清空，并重置了所有状态属性信息，类似于一个格式化操作。还记得上面在阐述读取消息的流程中涉及到的diskQueue.skipToNextRWFile方法吗，它的一个作用就是删除diskQueue当前未读取的所有文件。除此之外，清空消息操作还删除了元数据文件。相关代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940// 清空 diskQueue 中未读取的文件func (d *diskQueue) Empty() error &#123; d.RLock() defer d.RUnlock() if d.exitFlag == 1 &#123; return errors.New("exiting") &#125; d.logf(INFO, "DISKQUEUE(%s): emptying", d.name) d.emptyChan &lt;- 1 return &lt;-d.emptyResponseChan&#125; // diskQueue.go// diskQueue 消息处理主循环func (d *diskQueue) ioLoop() &#123; for &#123; // 1-3. // ... select &#123; // 4. // ... // 5. 收到清空持久化存储 disQueue 的消息 case &lt;-d.emptyChan: // (当应用程序调用 diskQueue.Empty 方法时触发) // 删除目前还未读取的文件，同时删除元数据文件 d.emptyResponseChan &lt;- d.deleteAllFiles() count = 0 // 重置当前等待刷盘的消息数量 // 6-8 // ... &#125; &#125;// ...&#125; // diskQueue.go// 调用 skipToNextRWFile 方法清空 readFileNum -&gt; writeFileNum 之间的文件，// 并且设置 depth 为 0。 同时删除元数据文件func (d *diskQueue) deleteAllFiles() error &#123; err := d.skipToNextRWFile() innerErr := os.Remove(d.metaDataFileName()) // ... return err&#125; // diskQueue.go diskQueue 刷盘操作同大多的存储系统类似，diskQueue采用批量刷新缓冲区的操作来提高消息写入文件系统的性能。其中，diskQueue规定触发刷盘动作的有个条件，其中任一条件成立即可。一是当缓冲区中的消息的数量达到阈值(syncEvery)时，二是每隔指定时间(syncTimeout)。需要注意的一点为在执行刷盘动作，也会重新持久化diskQueue的元数据信息。相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344func (d *diskQueue) ioLoop() &#123; syncTicker := time.NewTicker(d.syncTimeout) for &#123; // dont sync all the time :) // 1. 只有写入缓冲中的消息达到一定数量，才执行同步刷新到磁盘的操作 if count == d.syncEvery &#123; d.needSync = true &#125; // 2. 刷新磁盘操作，重置计数信息，即将 writeFile 流刷新到磁盘，同时持久化元数据 if d.needSync &#123; err = d.sync() // ... count = 0 // 重置当前等待刷盘的消息数量 &#125; // 3. // ... select &#123; // 4-6 // ... // 7. 定时执行刷盘操作，在存在数据等待刷盘时，才需要执行刷盘动作 case &lt;-syncTicker.C: if count == 0 &#123; // avoid sync when there's no activity continue &#125; d.needSync = true // 8. // ... &#125;// ...&#125; // diskQueue.go // 同步刷新 writeFile 文件流（即将操作系统缓冲区中的数据写入到磁盘），同时持久化元数据信息func (d *diskQueue) sync() error &#123; if d.writeFile != nil &#123; err := d.writeFile.Sync() // ... &#125; err := d.persistMetaData() // ... // 重置了刷新开关 d.needSync = false return nil&#125; 简单小结，本文详细分析了持久化消息队列存储组件——diskQueue，它被用作nsq的消息持久化存储。围绕diskQueue展开，通过阐述其提供给上层应用程序的功能接口来分析其工作原理，重点梳理了从diskQueue中读取和写消息的逻辑，同一般的队列实现类似，采用一组索引标记读写的位置，只不过diskQueue采用了两组读取索引。另外，在读取消息的过程检测文件是否被损坏，同时在写入过程中，通过不断切换文件来限制写入单个文件的数据量。diskQueue同样提供了清空存储的所有消息（删除所有文件，并重置diskQueue状态信息）的操作（类似于文件系统的格式化操作），最后不要忘记缓冲区的批量刷新刷盘动作助于提高文件系统的写入性能。更完整的源码注释可参考这里。 参考文献 [1].https://github.com/nsqio/go-diskqueue]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nsq 消息发送订阅源码简析]]></title>
    <url>%2F2019%2F05%2F15%2Fnsq-%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%AE%A2%E9%98%85%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章阐述了channel模块的源码。以channel为核心分析channel结构组件、channel实例的创建、删除以及查找这四方面的源码逻辑，另外也简要分析了Message结构的字段，以及两个与消息发送相关的优先级队列in-flight queue和deferred queue。同时也将对应方法与其它方法进行串联分析，以在整体上把握程序逻辑。本文的主题是nsq消息队列系统中消息的发送和订阅相关源码的逻辑。本文的核心是阐述，当生产者将消息投递到某个nsqd实例上对应的topic时，消息是如何在nsqd内部各组件（包括nsqd、topic和chanel）之间流动的，并且分析nsq是如何处理延时消息的投递。另外，结合网络传输模块的源码，分析两个典型的过程：生产者发布消息的逻辑，以及消费者是订阅并获取消息的逻辑。此篇文章内容较前两篇复杂，它是nsq实时消息队列的核心部分，对理解nsq的关键工作原理至关重要。 本文阐述的内容更详细的源码注释可在这里找到，注释源码版本为v1.1.0，仅供参考。本文所涉及到源码文件主要为/nsq/nsqd/和/nsq/internal/下的若干子目录。具体而言，围绕nsqd.go、topic.go、channel.go展开，同时也会涉及到protocol_v2.go和http.go。 本文侧重于分析消息在nsq系统内部各组件之间是如何流动的，典型地，如消息是何时通过何种方式从topic实例流向channel实例的，另外，如何实现消息的延时投递逻辑，消息投递超时处理逻辑等。另外，也分析生产者是发布消息的到指定的topic的逻辑，以及消费者是订阅channel的逻辑，并且如何从订阅的channel收到消息。最后阐述客户端（生产者和消费者）使用的几个典型的命令请求。 topic 消息处理逻辑我们依据正常的消息发布的流程来阐述，以topic作为切入点，分析topic如何将从生产者收到的消息传递给与其关联的channel实例。当生产者发布一条消息到指定的topic时，请求在protocolV2(protocol_v2.go)实例接收，然后交由protocolV2.PUB方法处理，其接着调用topic.PutMessage方法向topic实例添加一条消息，由此消息就正式进入了nsq系统内部了。PutMessage方法通过调用put方法将接收的消息写入到消息队列，若内存消息队列(memoryMsgChan)未满，则 push 到内存消息队列，否则 push 到持久化存储消息队列(backend)。然后在topic的消息处理主循环中，从memoryMsgChan或backendChan管道中接收到新的消息，其随即遍历自己维护的所有channel实例，将此消息副本发送给每一个channel实例，即调用chanel.PutMessage将消息压入到channel维护的消息队列中（同样包括memoryMsgChan和backend两个），或者若此消息需要被延迟，则调用channel.PutMessageDeferred方法将消息压入到消息延时的优先级队列(deferred queue)。 好，topic处理消息的核心逻辑已经阐述完毕。我们贴出这个过程中消息流动所涉及到的方法调用链：protocolV2.PUB-&gt;topic.PutMessage-&gt;topic.put-&gt;topic.messagPump-&gt;，从这里开始分叉，对于正常的消息：channel.PutMessage-&gt;channel.put，最后写入channel.memoryMsgChan或channel.backend；对于被延时的消息：channel.PutMessageDeferred-&gt;channel.StartDeferredTimeout-&gt;chanel.addToDeferredPQ-&gt;deferredPQ.push。 核心流程如上所述，补充几点，topic.messagePump方法在topic启动（有可能是从配置文件中加载启动LoadMetadata或新创建时启动GetTopic）时开始工作，换言之，开始处理生产者给它发送的消息。另外，messagePump循环中也可能收到topic所维护的channel集合更新的消息（添加或移除），此时需要重新初始化两个消息队列管道(memoryMsgChan &amp; backendChan)。最后，当收到paused的消息时，会重置这两个消息队列管道，因为一旦topic.Paused属性被设置，则表示此topic不应该再处理消息。相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156// [ topic.go 文件中，由 topic 实例的消息处理逻辑 ]// 此方法由 httpServer.PUB 或 protocolV2.PUB 方法中调用，即生产者通过 http/tcp 投递消息到 topicfunc (t *Topic) PutMessage(m *Message) error &#123; t.RLock() defer t.RUnlock() // 1. 消息写入操作只在 exitFlag 为0时才进行 if atomic.LoadInt32(&amp;t.exitFlag) == 1 &#123; return errors.New("exiting") &#125; // 2. 写入消息内存队列 memoryMsgChan 或者 持久化存储 backend err := t.put(m) if err != nil &#123; return err &#125; // 3. 更新当前 topic 所对应的消息数量以及消息总大小 atomic.AddUint64(&amp;t.messageCount, 1) atomic.AddUint64(&amp;t.messageBytes, uint64(len(m.Body))) return nil&#125; // /nsq/nsqd/topic.go// 将指定消息进行持久化// 通常情况下，在 memoryMsChan 未达到其设置的最大的消息的数量时// （即内存中的消息队列中保存的消息的数量未达到上限时，由 MemQueueSize 指定）// 会先将消息 push 到内在消息队列 memoryChan 中，否则会被 push 到后端持久化队列 backend 中。// 这是通过 go buffered channel 语法来实现。func (t *Topic) put(m *Message) error &#123; select &#123; case t.memoryMsgChan &lt;- m: default: // 内存消息队列已满时，会将消息存放到持久化存储 // 从缓冲池中获取缓冲 b := bufferPoolGet() // 将消息写入持久化消息队列 err := writeMessageToBackend(b, m, t.backend) bufferPoolPut(b) // 回收从缓冲池中获取的缓冲 t.ctx.nsqd.SetHealth(err) if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, "TOPIC(%s) ERROR: failed to write message to backend - %s", t.name, err) return err &#125; &#125; return nil&#125; // /nsq/nsqd/topic.go// messagePump 监听 message 的更新的一些状态，以及时将消息持久化，// 同时写入到此 topic 对应的channelfunc (t *Topic) messagePump() &#123; var msg *Message var buf []byte var err error var chans []*Channel var memoryMsgChan chan *Message var backendChan chan []byte // 1. 等待开启 topic 消息处理循环，即等待调用 topic.Start， // 在 nsqd.GetTopic 和 nsqd.LoadMetadata 方法中调用 for &#123; select &#123; case &lt;-t.channelUpdateChan: continue case &lt;-t.pauseChan: continue case &lt;-t.exitChan: goto exit // 在 nsqd.Main 中最后一个阶段会开启消息处理循环 topic.Start // （处理由客户端（producers）向 topci 投递的消息） // 在此之前的那些信号全部忽略 case &lt;-t.startChan: &#125; break &#125; t.RLock() // 2. 根据 topic.channelMap 初始化两个通道 memoryMsgChan，backendChan // 并且保证 topic.channelMap 存在 channel，且 topic 未被 paused for _, c := range t.channelMap &#123; chans = append(chans, c) &#125; t.RUnlock() if len(chans) &gt; 0 &amp;&amp; !t.IsPaused() &#123; memoryMsgChan = t.memoryMsgChan backendChan = t.backend.ReadChan() &#125; // 3. topic 处理消息的主循环 for &#123; select &#123; // 3.1 从内存消息队列 memoryMsgChan 或 持久化存储 backend 中收到消息 // 则将消息解码，然后会将消息 push 到此 topic 关联的所有 channel case msg = &lt;-memoryMsgChan: case buf = &lt;-backendChan: msg, err = decodeMessage(buf) if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, "failed to decode message - %s", err) continue &#125; // 3.2 当从 channelUpdateChan 读取到消息时， // 表明有 channel 更新，比如创建了新的 channel， // 因此需要重新初始化 memoryMsgChan及 backendChan case &lt;-t.channelUpdateChan: chans = chans[:0] t.RLock() for _, c := range t.channelMap &#123; chans = append(chans, c) &#125; t.RUnlock() if len(chans) == 0 || t.IsPaused() &#123; memoryMsgChan = nil backendChan = nil &#125; else &#123; memoryMsgChan = t.memoryMsgChan backendChan = t.backend.ReadChan() &#125; continue // 3.3 当收到 pause 消息时，则将 memoryMsgChan及backendChan置为 nil，注意不能 close， // 二者的区别是 nil的chan不能接收消息了，但不会报错。 // 而若从一个已经 close 的 chan 中尝试取消息，则会 panic。 case &lt;-t.pauseChan: // 当 topic 被 paused 时，其不会将消息投递到 channel 的消息队列 if len(chans) == 0 || t.IsPaused() &#123; memoryMsgChan = nil backendChan = nil &#125; else &#123; memoryMsgChan = t.memoryMsgChan backendChan = t.backend.ReadChan() &#125; continue // 3.4 当调用 topic.exit 时会收到信号，以终止 topic 的消息处理循环 case &lt;-t.exitChan: goto exit &#125; // 4. 当从 memoryMsgChan 或 backendChan 中 pull 到一个 msg 后，会执行这里： // 遍历 channelMap 中的每一个 channel，将此 msg 拷贝到 channel 中的后备队列。 // 注意，因为每个 channel 需要一个独立 msg，因此需要在拷贝时需要创建 msg 的副本 // 同时，针对 msg 是否需要被延时投递来选择将 msg 放到 // 延时队列 deferredMessages中还是 in-flight queue 中 for i, channel := range chans &#123; chanMsg := msg if i &gt; 0 &#123; // 若此 topic 只有一个 channel，则不需要显式地拷贝了 chanMsg = NewMessage(msg.ID, msg.Body) chanMsg.Timestamp = msg.Timestamp chanMsg.deferred = msg.deferred &#125; // 将 msg push 到 channel 所维护的延时消息队列 deferred queue // 等待消息的延时时间走完后，会把消息进一步放入到 in-flight queue 中 if chanMsg.deferred != 0 &#123; channel.PutMessageDeferred(chanMsg, chanMsg.deferred) continue &#125; // 将 msg push 到普通消息队列 in-flight queue err := channel.PutMessage(chanMsg) // ... &#125; &#125;exit: t.ctx.nsqd.logf(LOG_INFO, "TOPIC(%s): closing ... messagePump", t.name)&#125; // /nsq/nsqd/topic.go 1234567891011121314151617181920212223242526272829303132// [ channel.go 文件中，topic 发送消息到 in-flight queue 相关逻辑 ]// 此方法会由 topic.messagePump 方法中调用。// 即当 topic 收到生产者投递的消息时，将此消息放到与其关联的 channels 的延迟队列 deferred queue// 或者 普通的消息队列中(包括 内存消息队列 memoryMsgChan 或 后端持久化 backend)（即此方法）// channel 调用 put 方法将消息放到消息队列中，同时更新消息计数func (c *Channel) PutMessage(m *Message) error &#123; c.RLock() defer c.RUnlock() if c.Exiting() &#123; return errors.New("exiting") &#125; err := c.put(m) if err != nil &#123; return err &#125; atomic.AddUint64(&amp;c.messageCount, 1) return nil&#125; // /nsq/nsqd/channel.go// 同 topic.put 方法类似，其在 put message 时，// 依据实际情况将消息 push 到内在队列 memoryMsgChan 或者后端持久化 backendfunc (c *Channel) put(m *Message) error &#123; select &#123; case c.memoryMsgChan &lt;- m: default: b := bufferPoolGet() err := writeMessageToBackend(b, m, c.backend) bufferPoolPut(b) c.ctx.nsqd.SetHealth(err) // ... &#125; return nil&#125; // /nsq/nsqd/channel.go 12345678910111213141516171819202122232425262728// [ channel.go 文件中，topic 发送消息到 deferred queue 相关逻辑 ]// 将 message 添加到 deferred queue 中func (c *Channel) PutMessageDeferred(msg *Message, timeout time.Duration) &#123; atomic.AddUint64(&amp;c.messageCount, 1) c.StartDeferredTimeout(msg, timeout)&#125; // /nsq/nsqd/channel.go// 将 message 加入到 deferred queue 中，等待被 queueScanWorker 处理func (c *Channel) StartDeferredTimeout(msg *Message, timeout time.Duration) error &#123; // 1. 计算超时超时戳，作为 Priority absTs := time.Now().Add(timeout).UnixNano() // 2. 构造 item item := &amp;pqueue.Item&#123;Value: msg, Priority: absTs&#125; // 3. item 添加到 deferred 字典 err := c.pushDeferredMessage(item) if err != nil &#123; return err &#125; // 4. 将 item 放入到 deferred message 优先级队列 c.addToDeferredPQ(item) return nil&#125; // /nsq/nsqd/channel.gofunc (c *Channel) addToDeferredPQ(item *pqueue.Item) &#123; c.deferredMutex.Lock() heap.Push(&amp;c.deferredPQ, item) c.deferredMutex.Unlock()&#125; // /nsq/nsqd/channel.go 至此，关于topic如何处理消息，如何将消息传递给channel已经讲述完毕，接下来，分析对于那些超时的消息应该如何处理，deferred queue中存储的延时投递的消息如何发送给客户端。这涉及到nsqd.go文件中nsqd实例的消息处理主循环，它循环扫描所有的channel关联的两个消息队列中的消息，并做针对性处理。 nsqd 消息处理循环在nsqd源码分析的文章中，没有涉及到nsqd消息处理循环相关的逻辑，考虑到在介绍之前必须要先了解topic及channel的相关功能。因此，把nsqd关于消息处理的部分单独开篇文章介绍。在nsqd.Main启动方法中，异步开启三个处理循环：nsqd.queueScanLoop、nsqd.lookupLoop和nsqd.statsLoop，分别作为消息处理循环，同nsqlookupd通信交互循环，以及数据统计循环。在nsqd.queueScanLoop方法中： 它首先根据配置文件参数初始化了一些重要属性：workTicker根据QueueScanInterval初始化，表示每隔QueueScanInterval的时间（默认100ms），nsqd随机挑选QueueScanSelectionCount数量的channel执行dirty channel的计数统计；另外refreshTicker根据QueueScanRefreshInterval初始化，每过QueueScanRefreshInterval时间（默认5s）就调整queueScanWorker pool的大小。之后， queueScanLoop的任务是处理发送中的消息队列(in-flight queue)，以及被延迟发送的消息队列(deferred queue)两个优先级消息队列中的消息。具体而言，它循环执行两个定时任务： 其一，由workTicker计时器触发，每过QueueScanInterval（默认为100ms）的时间，就从本地的消息缓存队列中（nsqd维护的所有topic所关联的channel集合），随机选择QueueScanSelectionCount（默认20）个channel。检查这些channel集合中被标记为dirty属性的channel的数量，所谓的dirty channel即表示此channel实例中存在消息需要处理，这包含两个方面的处理逻辑： 对于in-flight queue而言，检查消息是否已经处理超时（消费者处理超时），若存在超时的消息，则将消息从in-flight queue中移除，并重新将它压入到此channel的消息队列中(memoryMsgChan或backend)，等待后面重新被发送（即之后还会被重新压入到in-flight queue中），此为消息发送超时的处理逻辑。 对于deferred queue而言，检查消息的延迟时间是否已经走完，换言之，检查被延迟的消息现在是否应该发送给消费者了。若某个被延时的消息的延时时间已经达到，则将它从deferred queue中移除，并重新压入到此channel的消息队列中(memoryMsgChan或backend)，等待后面正式被发送（即之后还会被重新压入到in-flight queue中），此为消息延迟发送超时的处理逻辑。 若处理的结果显示，dirty channel的数量超过QueueScanDirtyPercent（默认25%）的比例，则再次随机选择QueueScanSelectionCount（默认20）个channel，并让queueScanWorker对它们进行处理。 另一个定时任何由refreshTicker计时器触发，每过QueueScanRefreshInterval（默认5s）的时间，就调整queueScanWorker pool的大小。具体的调整措施为： 若现有的queueScanWorker的数量低于理想值（nsqd包含的channel集合的总数的1/4，程序硬编码），则显式地增加queueScanWorker的数量，即异步执行queueScanWorker方法。 否则，若现有的queueScanWorker数量高于理想值，则通过exitCh显式地结束执行queueScanWorker方法。 所谓的queueScanWorker实际上只是一个循环消息处理的方法，一旦它从workCh管道接收到消息，则会开始处理in-fligth queue和deferred queue中的消息，最后将处理结果，即队列中是否存在dirty channel通过responseCh通知给queeuScanLoop主循环。 上述逻辑即为nsqd对两个消息队列(in-flight queue和deferred queue)的核心处理逻辑。相关代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// queueScanLoop 方法在一个单独的 go routine 中运行。// 用于处理正在发送的 in-flight 消息以及被延迟处理的 deferred 消息// 它管理了一个 queueScanWork pool，其默认数量为5。queueScanWorker 可以并发地处理 channel。// 它借鉴了Redis随机化超时的策略，即它每 QueueScanInterval 时间（默认100ms）就从本地的缓存队列中// 随机选择 QueueScanSelectionCount 个（默认20个） channels。// 其中 缓存队列每间隔 QueueScanRefreshInterval 还会被刷新。func (n *NSQD) queueScanLoop() &#123; // 1. 获取随机选择的 channel 的数量，以及队列扫描的时间间隔，及队列刷新时间间隔 workCh := make(chan *Channel, n.getOpts().QueueScanSelectionCount) responseCh := make(chan bool, n.getOpts().QueueScanSelectionCount) closeCh := make(chan int) workTicker := time.NewTicker(n.getOpts().QueueScanInterval) refreshTicker := time.NewTicker(n.getOpts().QueueScanRefreshInterval) // 2. 获取 nsqd 所包含的 channel 集合，一个 topic 包含多个 channel， // 而一个 nsqd 实例可包含多个 topic 实例 channels := n.channels() n.resizePool(len(channels), workCh, responseCh, closeCh) // 3. 这个循环中的逻辑就是依据配置参数， // 反复处理 nsqd 所维护的 topic 集合所关联的 channel 中的消息 // 即循环处理将 channel 从 topic 接收到的消息，发送给订阅了对应的 channel 的客户端 for &#123; select &#123; // 3.1 每过 QueueScanInterval 时间（默认100ms）， // 则开始随机挑选 QueueScanSelectionCount 个 channel。转到 loop: 开始执行 case &lt;-workTicker.C: if len(channels) == 0 &#123; // 此 nsqd 没有包含任何 channel 实例当然就不用处理了 continue &#125; // 3.2 每过 QueueScanRefreshInterval 时间（默认5s）， // 则调整 pool 的大小，即调整开启的 queueScanWorker 的数量为 pool 的大小 case &lt;-refreshTicker.C: channels = n.channels() n.resizePool(len(channels), workCh, responseCh, closeCh) continue // 3.3 nsqd 已退出 case &lt;-n.exitChan: goto exit &#125; num := n.getOpts().QueueScanSelectionCount if num &gt; len(channels) &#123; num = len(channels) &#125; // 3.4 利用 util.UniqRands，随机选取 num（QueueScanSelectionCount 默认20个）channel // 将它们 push 到 workCh 管道，queueScanWorker 中会收到此消息， // 然后立即处理 in-flight queue 和 deferred queue 中的消息。 // 注意，因为这里是随机抽取 channel 因此，有可能被选中的 channel 中并没有消息 loop: for _, i := range util.UniqRands(num, len(channels)) &#123; workCh &lt;- channels[i] &#125; // 3.5 统计 dirty 的 channel 的数量， responseCh 管道在上面的 nsqd.resizePool 方法中 // 传递给了 len(channels) * 0.25 个 queueScanWorker。 // 它们会在循环中反复查看两个消息优先级队列中是否有消息等待被处理： // 即查看 inFlightPQ 和 deferredPQ。 numDirty := 0 for i := 0; i &lt; num; i++ &#123; if &lt;-responseCh &#123; numDirty++ &#125; &#125; // 3.6 若其 dirtyNum 的比例超过配置的 QueueScanDirtyPercent（默认为25%） if float64(numDirty)/float64(num) &gt; n.getOpts().QueueScanDirtyPercent &#123; goto loop &#125; &#125;exit: n.logf(LOG_INFO, "QUEUESCAN: closing") close(closeCh) workTicker.Stop() refreshTicker.Stop()&#125; // /nsq/nsqd/nsqd.go 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// 调整 queueScanWorker 的数量func (n *NSQD) resizePool(num int, workCh chan *Channel, responseCh chan bool, closeCh chan int) &#123; // 1. 根据 channel 的数量来设置合适的 pool size，默认为 1/4 的 channel 数量 idealPoolSize := int(float64(num) * 0.25) if idealPoolSize &lt; 1 &#123; idealPoolSize = 1 &#125; else if idealPoolSize &gt; n.getOpts().QueueScanWorkerPoolMax &#123; idealPoolSize = n.getOpts().QueueScanWorkerPoolMax &#125; // 2. 开启一个循环，直到理想的 pool size 同实际的 pool size 相同才退出。 // 否则，若理想值更大，则需扩展已有的 queueScanWorker 的数量， // 即在一个单独的 goroutine 中调用一次 nsqd.queueScanWorker 方法（开启了一个循环）。 // 反之， 需要减少已有的 queueScanWorker 的数量， // 即往 closeCh 中 push 一条消息，强制 queueScanWorker goroutine 退出 for &#123; if idealPoolSize == n.poolSize &#123; break &#125; else if idealPoolSize &lt; n.poolSize &#123; // contract closeCh &lt;- 1 n.poolSize-- &#125; else &#123; // expand n.waitGroup.Wrap(func() &#123; n.queueScanWorker(workCh, responseCh, closeCh) &#125;) n.poolSize++ &#125; &#125;&#125; // /nsq/nsqd/nsqd.go// 在 queueScanLoop 中处理 channel 的具体就是由 queueScanWorker 来负责。// 调用方法 queueScanWorker 即表示新增一个 queueScanWorker goroutine 来处理 channel。// 一旦开始工作 (从 workCh 中收到了信号， 即 dirty 的 channel 的数量达到阈值)，// 则循环处理 in-flight queue 和 deferred queue 中的消息，// 并将处理结果（即是否是 dirty channel）通过 reponseCh 反馈给 queueScanWorker。func (n *NSQD) queueScanWorker(workCh chan *Channel, responseCh chan bool, closeCh chan int) &#123; for &#123; select &#123; // 开始处理两个消息队列中的消息 case c := &lt;-workCh: now := time.Now().UnixNano() dirty := false // 若返回true，则表明 in-flight 优先队列中有存在处理超时的消息， // 因此将消息再次写入到 内存队列 memoryMsgChan 或 后端持久化 backend // 等待消息被重新投递给消费者（重新被加入到 in-flight queue） if c.processInFlightQueue(now) &#123; dirty = true &#125; // 若返回 true，则表明 deferred 优先队列中存在延时时间已到的消息， // 因此需要将此消息从 deferred queue 中移除， // 并将消息重新写入到 内存队列 memoryMsgChan 或后端持久化 backend // 等待消息被正式投递给消费者 （正式被加入到 in-flight queue） if c.processDeferredQueue(now) &#123; dirty = true &#125; // 报告 queueScanLoop 主循环，发现一个 dirty channel responseCh &lt;- dirty // 退出处理循环，缩减 queueScanWorker 数量时，被调用 case &lt;-closeCh: return &#125; &#125;&#125; // /nsq/nsqd/nsqd.go 为了更好的理解消息的流动，小结一下，生产者投递消息到指定topic后，消息进入了topic维护的消息队列(memoryMsgChan和backend)，而在启动nsqd时，会异步开启一个消息处理循环即queueScanLoop，它包含两个计时任务，其中一个是，定时调整已经正在运行的queueScanWorker数量，其中queueScanWorker的任务为查看两个优先级队列中是否存在需要被处理的消息，若存在，则标记对应的channel为dirty channel。另一个计时任务是，定时抽取一定数量的channel，查看其中为dirty channel的比例（由queueScanWorker完成），若达到一定比例，则继续执行抽取channel的动作，如此反复。 好，到目前为止，nsq内部的消息处理逻辑已经阐述完毕。这对于理解整个nsq实时消息队列的关键原理至关重要。下面阐述几个典型的命令请求的核心实现逻辑。 生产者消息发布消息考虑到nsq为生产者提供了http/tcp两种方式来发布消息。因此，笔者以tcp的命令请求处理器为示例来阐述其核心处理逻辑（http的方式也类似）。当生产者通过go-nsq库以tcp的方式发送消息发布请求命令给指定topic时，请求首先从protocolV2.IOLoop中被读取，然后其调用protocolV2.Exec方法根据命令请求的类型调用相应的处理方法，此为PUB命令，因此调用protocolV2.PUB方法处理。处理过程比较简单，首先解析请求，取出topic名称，然后执行权限检查，检查通过后，便依据topic名称获取此topic实例，然后，构建一条消息，并调用topic.PutMessage方法发布消息，最后，调用cleint.PublishedMessage方法更新一些信息，并返回ok。整个流程比较简单，因为关键的处理逻辑在前文介绍过了，读者需要把它们串联起来。代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 客户端在指定的 topic 上发布消息func (p *protocolV2) PUB(client *clientV2, params [][]byte) ([]byte, error) &#123; var err error if len(params) &lt; 2 &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID" , "PUB insufficient number of parameters") &#125; // 1. 读取 topic 名称 topicName := string(params[1]) if !protocol.IsValidTopicName(topicName) &#123; return nil, protocol.NewFatalClientErr(nil, "E_BAD_TOPIC", fmt.Sprintf("PUB topic name %q is not valid", topicName)) &#125; // 2. 读取消息体长度 bodyLen，并在长度上进行校验 bodyLen, err := readLen(client.Reader, client.lenSlice) if err != nil &#123; return nil, protocol.NewFatalClientErr(err, "E_BAD_MESSAGE", "PUB failed to read message body size") &#125; if bodyLen &lt;= 0 &#123; return nil, protocol.NewFatalClientErr(nil, "E_BAD_MESSAGE", fmt.Sprintf("PUB invalid message body size %d", bodyLen)) &#125; if int64(bodyLen) &gt; p.ctx.nsqd.getOpts().MaxMsgSize &#123; return nil, protocol.NewFatalClientErr(nil, "E_BAD_MESSAGE", fmt.Sprintf("PUB message too big %d &gt; %d", bodyLen, p.ctx.nsqd.getOpts().MaxMsgSize)) &#125; // 3. 读取指定字节长度的消息内容到 messageBody messageBody := make([]byte, bodyLen) _, err = io.ReadFull(client.Reader, messageBody) // ... // 4. 检查客户端是否具备 PUB 此 topic 命令的权限 if err := p.CheckAuth(client, "PUB", topicName, ""); err != nil &#123; return nil, err &#125; // 5. 获取 topic 实例 topic := p.ctx.nsqd.GetTopic(topicName) // 6. 构造一条 message，并将此 message 投递到此 topic 的消息队列中 msg := NewMessage(topic.GenerateID(), messageBody) err = topic.PutMessage(msg) // ... // 7. 开始发布此消息，即将对应的 client 修改为此 topic 保存的消息的计数。 client.PublishedMessage(topicName, 1) // 回复 Ok return okBytes, nil&#125; // /nsq/nsqd/protocol_v2.go 生产者除了可以发送PUB命令外，类似地，还有命令请求MPUB来一次性发布多条消息，DPUB用于发布延时投递的消息等等，逻辑都比较简单，不多阐述。下面介绍消费者处理消息的相关流程。 消费者处理消息消费者处理消息的流程包括，消费者发送SUB命令请求以订阅channel，消费者发送RDY命令请求以通知服务端自的消息处理能力，消费者发送FIN消息表示消息已经处理完成，最后还有个消费者发送REQ消息请求服务端重新将消息入队。下面依次分析这些命令请求的核心实现。 消费者订阅消息当消费者通过tcp发送订阅消息的请求时，请求同样是首先从protocolV2.IOLoop方法中被接收，然后交由Exec方法处理。最后进入到SUB方法的流程，它首先执行必要的请求校验工作，其中容易被忽略的一点是，只有当client处于stateInit状态才能订阅某个topic的channel，换言之，当一个client订阅了某个channel后，它的状态会被更新为stateSubscribed，因此不能再订阅其它channel了。总而言之，一个 client同一时间只能订阅一个channel。之后，获取并校验订阅的topic名称、channel名称，然后，客户端是否有订阅的权限，权限检查通过后，通过topic和channel名称获取对应的实例，并将此client实例添加到其订阅的channel的客户端集合中。最后，也是最关键的步骤是，它将订阅的channel实例传递给了client，同时将channel发送到了client.SubEventChan管道中，因此在protocolV2.messagePump方法中就能够根据，此客户端可以利用channel.memoryMsgChan和channel.backend来获取channel实例从topic实例接收到的消息，具体过程可以参考这里。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 客户端在指定的 topic 上订阅消息func (p *protocolV2) SUB(client *clientV2, params [][]byte) ([]byte, error) &#123; // 1. 做一些校验工作，只有当 client 处于 stateInit 状态才能订阅某个 topic 的 channel // 换言之，当一个 client 订阅了某个 channel 之后， // 它的状态会被更新为 stateSubscribed，因此不能再订阅 channel 了。 // 总而言之，一个 client 只能订阅一个 channel if atomic.LoadInt32(&amp;client.State) != stateInit &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "cannot SUB in current state") &#125; if client.HeartbeatInterval &lt;= 0 &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "cannot SUB with heartbeats disabled") &#125; if len(params) &lt; 3 &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "SUB insufficient number of parameters") &#125; // 2. 获取订阅的 topic 名称、channel 名称，并对它们进行校验 topicName := string(params[1]) if !protocol.IsValidTopicName(topicName) &#123; return nil, protocol.NewFatalClientErr(nil, "E_BAD_TOPIC", fmt.Sprintf("SUB topic name %q is not valid", topicName)) &#125; channelName := string(params[2]) if !protocol.IsValidChannelName(channelName) &#123; return nil, protocol.NewFatalClientErr(nil, "E_BAD_CHANNEL", fmt.Sprintf("SUB channel name %q is not valid", channelName)) &#125; // 3. 同时检查此客户端是否有订阅的权限 if err := p.CheckAuth(client, "SUB", topicName, channelName); err != nil &#123; return nil, err &#125; // 此循环是为了避免 client 订阅到正在退出的 ephemeral 属性的 channel 或 topic var channel *Channel for &#123; // 4. 获取 topic 及 channel 实例 topic := p.ctx.nsqd.GetTopic(topicName) channel = topic.GetChannel(channelName) // 5. 调用 channel的 AddClient 方法添加指定客户端 if err := channel.AddClient(client.ID, client); err != nil &#123; return nil, protocol.NewFatalClientErr(nil, "E_TOO_MANY_CHANNEL_CONSUMERS", fmt.Sprintf("channel consumers for %s:%s exceeds limit of %d", topicName, channelName, p.ctx.nsqd.getOpts().MaxChannelConsumers)) &#125; // 6. 若此 channel 或 topic 为ephemeral，并且channel或topic正在退出，则移除此client if (channel.ephemeral &amp;&amp; channel.Exiting()) || (topic.ephemeral &amp;&amp; topic.Exiting()) &#123; channel.RemoveClient(client.ID) time.Sleep(1 * time.Millisecond) continue &#125; break &#125; // 6. 修改客户端的状态为 stateSubscribed atomic.StoreInt32(&amp;client.State, stateSubscribed) // 7. 这一步比较关键，将订阅的 channel 实例传递给了 client， // 同时将 channel 发送到了 client.SubEventChan 通道中。 // 后面的 SubEventChan 就会使得当前的 client 在一个 goroutine 中订阅这个 channel 的消息 client.Channel = channel // update message pump // 8. 通知后台订阅协程来订阅消息,包括内存管道和磁盘 client.SubEventChan &lt;- channel // 9. 返回 ok return okBytes, nil&#125; // /nsq/nsqd/protocol_v2.go 消费者发送 RDY 命令在消费者未发送RDY命令给服务端之前，服务端不会推送消息给客户端，因为此时服务端认为消费者还未准备好接收消息（由方法client.IsReadyForMessages实现）。另外，此RDY命令的含义，简而言之，当RDY 100即表示客户端具备一次性接收并处理100个消息的能力，因此服务端此时更可推送100条消息给消费者（如果有的话），每推送一条消息，就要修改client.ReadyCount的值。而RDY命令请求的处理非常简单，即通过client.SetReadyCount方法直接设置client.ReadyCount的值。注意在这之前的两个状态检查动作。 12345678910111213141516171819202122232425262728293031// 消费者发送 RDY 命令请求表示服务端可以开始推送指定数目的消息了func (p *protocolV2) RDY(client *clientV2, params [][]byte) ([]byte, error) &#123; state := atomic.LoadInt32(&amp;client.State) if state == stateClosing &#123; // just ignore ready changes on a closing channel p.ctx.nsqd.logf(LOG_INFO, "PROTOCOL(V2): [%s] ignoring RDY after CLS in state ClientStateV2Closing", client) return nil, nil &#125; if state != stateSubscribed &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "cannot RDY in current state") &#125; count := int64(1) if len(params) &gt; 1 &#123; b10, err := protocol.ByteToBase10(params[1]) if err != nil &#123; return nil, protocol.NewFatalClientErr(err, "E_INVALID", fmt.Sprintf("RDY could not parse count %s", params[1])) &#125; count = int64(b10) &#125; if count &lt; 0 || count &gt; p.ctx.nsqd.getOpts().MaxRdyCount &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", fmt.Sprintf("RDY count %d out of range 0-%d", count, p.ctx.nsqd.getOpts().MaxRdyCount)) &#125; client.SetReadyCount(count) return nil, nil&#125; // /nsq/nsqd/protocol_v2.go 消费者发送 FIN 命令当消费者将channel发送的消息消费完毕后，会显式向nsq发送FIN命令（类似于ACK）。当服务端收到此命令后，就可将消息从消息队列中删除。FIN方法首先调用client.Channel.FinishMessage方法将消息从channel的两个集合in-flight queue队列及inFlightMessages 字典中移除。然后调用client.FinishedMessage更新client的维护的消息消费的统计信息。相关代码如下： 123456789101112131415161718192021// 消费者 client 收到消息后，会向 nsqd 响应 FIN+msgID 通知服务器成功投递消息，可以清空消息了'func (p *protocolV2) FIN(client *clientV2, params [][]byte) ([]byte, error) &#123; // 1. 正式处理 FIN 请求前，对 client 及 请求参数属性信息进行校验 state := atomic.LoadInt32(&amp;client.State) if state != stateSubscribed &amp;&amp; state != stateClosing &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "cannot FIN in current state") &#125; if len(params) &lt; 2 &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "FIN insufficient number of params") &#125; // 2. 获取 msgID id, err := getMessageID(params[1]) // ... // 3. client 调用 channel.FinishMessage 方法， // 即将消息从 channel 的 in-flight queue 及 inFlightMessages 字典中移除 err = client.Channel.FinishMessage(client.ID, *id) // ... // 4. 更新 client 维护的消息消费的统计信息 client.FinishedMessage() return nil, nil&#125; // /nsq/nsqd/protocol_v2.go 消费者发送 REQ 命令消费者可以通过向服务端发送REQ命令以将消息重新入队，即让服务端一定时间后（也可能是立刻）将消息重新发送给channel关联的客户端。此方法的核心是client.Channel.RequeueMessage，它会先将消息从in-flight queue优先级队列中移除，然后根据客户端是否需要延时timeout发送，分别将消息压入channel的消息队列(memoryMsgChan或backend)，或者构建一个延时消息，并将其压入到deferred queue。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// nsqd 为此 client 将 message 重新入队，并指定是否需要延时发送func (p *protocolV2) REQ(client *clientV2, params [][]byte) ([]byte, error) &#123; // 1. 先检验 client 的状态以及参数信息 state := atomic.LoadInt32(&amp;client.State) if state != stateSubscribed &amp;&amp; state != stateClosing &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "cannot REQ in current state") &#125; if len(params) &lt; 3 &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "REQ insufficient number of params") &#125; id, err := getMessageID(params[1]) if err != nil &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", err.Error()) &#125; // 2. 从请求中取出重入队的消息被延迟的时间， // 并转化单位，同时限制其不能超过最大的延迟时间 maxReqTimeout timeoutMs, err := protocol.ByteToBase10(params[2]) // ... timeoutDuration := time.Duration(timeoutMs) * time.Millisecond maxReqTimeout := p.ctx.nsqd.getOpts().MaxReqTimeout clampedTimeout := timeoutDuration if timeoutDuration &lt; 0 &#123; clampedTimeout = 0 &#125; else if timeoutDuration &gt; maxReqTimeout &#123; clampedTimeout = maxReqTimeout &#125; // ... // 3. 调用 channel.RequeueMessage 将消息重新入队。首先会将其从 in-flight queue 中删除， // 然后依据其 timeout 而定，若其为0,则直接将其添加到消息队列中， // 否则，若其 timeout 不为0,则构建一个 deferred message， // 并设置好延迟时间为 timeout，并将其添加到 deferred queue 中 err = client.Channel.RequeueMessage(client.ID, *id, timeoutDuration) // ... // 4. 更新 client 保存的关于消息的统计计数 client.RequeuedMessage() return nil, nil&#125; // /nsq/nsqd/protocol_v2.go// 将消息重新入队。这与 timeout 参数密切相关。// 当 timeout == 0 时，直接将此消息重入队。// 否则，异步等待此消息超时，然后 再将此消息重入队，即是相当于消息被延迟了func (c *Channel) RequeueMessage(clientID int64, id MessageID, timeout time.Duration) error &#123; // 1. 先将消息从 inFlightMessages 移除 msg, err := c.popInFlightMessage(clientID, id) if err != nil &#123; return err &#125; // 2. 同时将消息从 in-flight queue 中移除，并更新 chanel 维护的消息重入队数量 requeueCount c.removeFromInFlightPQ(msg) atomic.AddUint64(&amp;c.requeueCount, 1) // 3. 若 timeout 为0,则将消息重新入队。即调用 channel.put 方法， // 将消息添加到 memoryMsgChan 或 backend if timeout == 0 &#123; c.exitMutex.RLock() if c.Exiting() &#123; c.exitMutex.RUnlock() return errors.New("exiting") &#125; err := c.put(msg) c.exitMutex.RUnlock() return err &#125; // 否则，创建一个延迟消息，并设置延迟时间 return c.StartDeferredTimeout(msg, timeout)&#125; // /nsq/nsqd/channel.go 至此，关于客户端的消息处理相关的命令请求已经阐述完毕，其实还有一些，比如TOUCH命令请求，即重置消息的超时时间。但这些处理过程都比较简单，只是对前面两小节的逻辑进行封装调用。 简单小结，本文的重点在两个方面：topic消息处理逻辑，即消息是如何从topic实例流向channel实例的，实际上就是将从topic.memoryMsgChan或topic.backend收到的消息的副本依次压入到其关联的channel的in-flight queue（对于正常的消息）或者deferred queue（对于延时消息）。另一个方面，nsqd消息处理处理逻辑，nsqd负责in-flight queue中的消息超时的处理工作，以及deferred queue中的消息延时时间已到的处理工作。另外，也阐述了一些有关客户端的命令请求的核心处理逻辑，包括生产者发布消息的流程，消费者订阅消息，以及发送RDY/FIN/REQ命令请求的实现逻辑。 至此，整个nsq实时消息队列的源码基本已经分析完毕，总共包括6篇文章。这里简单总结： nsq 简介和特性理解简要介绍nsq的各个组件及系统的核心工作流程，并重点阐述几个值得关注的特性； nsq nsqlookupd 源码简析是以nsqlookupd命令为切入点，详细阐述nsqlookupd启动过程，其重点在于分析nsqlookupd的tcp请求处理器的相关逻辑，并梳理了topic查询和创建这两个典型的流程； nsq nsqd 服务启动源码简析同样是以nsqd命令为切入点，行文逻辑同上一篇类似，即阐述nsqd服务启动的一系列流程，并详述nsqd与nsqlookupd交互的主循环逻辑，以及nsqd为客户端建立的tcp请求处理器； nsq topic 源码简析内容相对简单，以topic为核心，阐述topic实例结构组成以及topic实例的创建、删除、关闭和查询流程； nsq channel 源码简析文章的行文同上一篇文章类似，以channel为核心，阐述channel实例结构组成以及channel实例的创建、删除、关闭和查询流程，并附带分析了Message实例结构； nsq 消息发送订阅源码简析，是这一系列文章中最重要的一篇，它对于理解nsq分布式实时消息消息队列的关键工作原理至关重要。它重点阐述topic实例如何将消息发送给它所关联的channel集合，以及nsqd实例如何处理消息处理超时和被延迟的消息处理。另外，简要分析了客户端执行的几条命令请求，如生产者发布消息流程和消费者订阅消息流程。 完整的源码注释可以参考这里。考虑到个人能力有限，因此无论文章内容或源码注释存在错误，欢迎留言指正！ 参考文献 [1]. https://github.com/nsqio/nsq[2]. https://nsq.io/overview/quick_start.html]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nsq channel 源码简析]]></title>
    <url>%2F2019%2F05%2F14%2Fnsq-channel-%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章阐述了topic模块的源码。即以topic为核心分析topic结构组件、topic实例的创建、删除以及查找这四方面的源码逻辑。同时也将这些方法放到一个完整的请求调用链中串联分析，以在整体上把握程序逻辑。本文的主题是channel（注意不要与go channel混淆）。channel可以视为发送消息的队列，它更贴近于消费者端，一旦topic实例从生产者那里收到一条消息，它会将这条消息复制并发送到每一个与它关联的channel，然后由channel将此消息随机发送到一个订阅了此channel的客户端。channel实例存储消息涉及到两个消息队列：内存消息队列和持久化消息队列。另外，channel还维护两个和消息发送相关的优先级队列：正在发送的消息队列和被推迟发送的消息队列。同样，本文只论述channel本身的相关逻辑，不涉及channel收发消息的逻辑。 本文分析nsq channel模块的源码，更详细nsq源码注释可在这里找到，注释源码版本为v1.1.0，仅供参考。本文所涉及到源码主要为/nsq/nsqd/和/nsq/internal/下的若干子目录。 同上一篇分析topic的源码类似，本文侧重于分析channel模块本身相关源码，而关于channel如何接收topic发送的消息、又如何对消息进行存储管理，最后又如何将消息推送给客户端，这部分会另外写一篇文章专门阐述。本文从五个方面来阐述channel：其一，简要介绍channel结构字段的组成；其二，由于channel同message密切相关，因此，也会分析message相关的字段，以及chanel维护的两个和消息发送相关的优先级队列。其三，阐述创建channel相关逻辑；其四，分析删除channel的过程；最后阐述chanel的查询过程。显然，本文分析channel的模式大体上同上一篇文章分析topic的模式相同，因此笔者会尽量精简介绍。 channel 实例结构相比topic结构，channel结构所包含的字段稍复杂些，重要的有：topicName代表channel实例所隶属的topic实例的名称；两个消息队列实例：backend表示channel使用的消息持久化队列接口，memoryMsgChan则表示内存消息队列；clients表示订阅此channel的客户端实例集合；ephemeral字段表示channel是否是临时的，临时的channel（#ephemeral开头）同样不会被持久化(PersistMetadata)，且当channel 关联的所有客户端都被移除后，此channel也会被删除（同临时的topic含义类似）。最后还有两个和消息发送相关的优先级队列：deferredPQ代表被延迟发送的消息集合，它是一个最小堆优先级队列，其中优先级比较字段为消息发送时间(Item.Priority)。inFlightPQ代表正在发送的消息集合，同样是最小堆优先级队列，优先级比较字段也为消息发送时间(Message.pri)。相关代码如下： 123456789101112131415161718192021222324252627282930313233343536type Channel struct &#123; // 64bit atomic vars need to be first for proper alignment on 32bit platforms requeueCount uint64 // 需要重新排队的消息数 messageCount uint64 // 接收到的消息的总数 timeoutCount uint64 // 正在发送的消息的数量 sync.RWMutex // guards topicName string // 其所对应的 topic 名称 name string // channel 名称 ctx *context // nsqd 实例 backend BackendQueue // 后端消息持久化的队列 // 内存消息通道。 其关联的 topic 会向此 channel 发送消息， // 且所有订阅的 client 会开启一个 go routine 订阅此 channel memoryMsgChan chan *Message exitFlag int32 // 退出标识（同 topic 的 exitFlag 作用类似） exitMutex sync.RWMutex // state tracking clients map[int64]Consumer// 与此 channel关联的client集合，即订阅的Consumer 集合 paused int32 // 若paused属性被设置，则那些订阅了此channel的客户端不会被推送消息 ephemeral bool // 标记此 channel 是否是临时的 deleteCallback func(*Channel) // 删除回调函数（同 topic 的 deleteCallback 作用类似） deleter sync.Once // Stats tracking e2eProcessingLatencyStream *quantile.Quantile // 延迟投递消息集合，消息体会放入 deferredPQ，并且由后台的queueScanLoop协程来扫描消息 // 将过期的消息照常使用 c.put(msg) 发送出去。 deferredMessages map[MessageID]*pqueue.Item deferredPQ pqueue.PriorityQueue // 被延迟投递消息集合对应的 PriorityQueue deferredMutex sync.Mutex // guards deferredMessages // 正在发送中的消息记录集合，直到收到客户端的 FIN 才删除，否则一旦超过 timeout，则重传消息。 // （因此client需要对消息做去重处理 de-duplicate） inFlightMessages map[MessageID]*Message inFlightPQ inFlightPqueue // 正在发送中的消息记录集合 对应的 inFlightPqueue inFlightMutex sync.Mutex // guards inFlightMessages&#125; // /nsq/nsqd/channel.go Message 实例结构Message代表生产者生产或消费者消费的一条消息。它是nsq消息队列系统中最基本的元素。Message结构包含的重要字段有：Attempts表示消息已经重复发送的次数（一旦消息投递次数过多，客户端可针对性地做处理）；deliveryTS表示channel向client发送消息时刻的时间戳；clientID表示消息被投递的目的客户端标识；pri表示消息优先级（即为消息被处理的deadline）；deferred为消息被延迟的时间（若消息确实被延迟了）。另外，网络传输的消息包格式构成为：Timestamp(8byte) + Attempts(2byte) + MessageID(16byte) + MessageBody(N-byte)。具体可参考相关源码，Message相关代码如下： 12345678910111213// 代表逻辑消息实体结构type Message struct &#123; ID MessageID // 消息 ID Body []byte // 消息体 Timestamp int64 // 当前时间戳 Attempts uint16 // 消息重复投递次数 // for in-flight handling deliveryTS time.Time // 投递消息的时间戳 clientID int64 // 接收此消息的 client ID pri int64 // 消息的优先级（即消息被处理的 deadline 时间戳） index int // 当前消息在 priority queue 中的索引 deferred time.Duration // 若消息被延迟，则为延迟时间&#125; // /nsq/nsqd/message.go 另外简要贴出两个消息发送优先级队列inFlightPQ和deferredPQ核心组成代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546type inFlightPqueue []*Message// 使用一个 heap 堆来存储所有的 message，// 根据 Message.pri（即消息处理时间的 deadline 时间戳） 来组织成一个小顶堆// 非线程安全，需要 caller 来保证线程安全func newInFlightPqueue(capacity int) inFlightPqueue &#123; return make(inFlightPqueue, 0, capacity)&#125; // /nsq/nsqd/in_flight_pqueue.go// 若堆顶元素的 pri 大于此时的 timestamp，则返回 nil, 及二者的差值// 此种情况表示还未到处理超时时间，即 nsqd 还不需要将它重新加入发送队列。// 否则返回堆顶元素, 0，表示堆顶元素已经被客户端处理超时了，需要重新加入发送队列func (pq *inFlightPqueue) PeekAndShift(max int64) (*Message, int64) &#123; if len(*pq) == 0 &#123; return nil, 0 &#125; x := (*pq)[0] if x.pri &gt; max &#123; return nil, x.pri - max &#125; pq.Pop() return x, 0&#125; // /nsq/nsqd/in_flight_pqueue.go// 最小堆优先级队列，其操作接口同 in_flight_queue （nsqd/in_flight_queue.go）类似// 不同的是它借用了标准库 container/heap/heap.gotype PriorityQueue []*Itemtype Item struct &#123; Value interface&#123;&#125; Priority int64 Index int&#125;func New(capacity int) PriorityQueue &#123; return make(PriorityQueue, 0, capacity)&#125; // /nsq/internal/pqueue/pqueue.gofunc (pq *PriorityQueue) PeekAndShift(max int64) (*Item, int64) &#123; if pq.Len() == 0 &#123; return nil, 0 &#125; item := (*pq)[0] if item.Priority &gt; max &#123; return nil, item.Priority - max &#125; heap.Remove(pq, 0) // Remove 方法中重新调整了堆的结构 return item, 0&#125; 创建 topic 实例channel的构造方法同topic的构造方法所涉及的逻辑非常相似，只不过channel还初始化了前面阐述的两个用于存储发送消息的优先级队列inFlightPQ和deferredPQ。因此就不再阐述，读者若需参考，可以看这里。注意，它同样会通过nsqd.Notify通知nsqlookupd有新的channel创建，因此需要重新调用PersistMetadata以持久化元数据。（方法调用链为：NewChannel-&gt;nsqd.Notify-&gt;nsqd.lookupLoop-&gt;nsqd.PersistMetadata）相关代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// channel 构造函数func NewChannel(topicName string, channelName string, ctx *context, deleteCallback func(*Channel)) *Channel &#123; // 1. 初始化 channel 部分参数 c := &amp;Channel&#123; topicName: topicName, name: channelName, memoryMsgChan: make(chan *Message, ctx.nsqd.getOpts().MemQueueSize), clients: make(map[int64]Consumer), deleteCallback: deleteCallback, ctx: ctx, &#125; if len(ctx.nsqd.getOpts().E2EProcessingLatencyPercentiles) &gt; 0 &#123; c.e2eProcessingLatencyStream = quantile.New( ctx.nsqd.getOpts().E2EProcessingLatencyWindowTime, ctx.nsqd.getOpts().E2EProcessingLatencyPercentiles, ) &#125; // 2. 初始化 channel 维护的两个消息队列 c.initPQ() // 3. 同 topic 类似，那些 ephemeral 类型的 channel 不会关联到一个 BackendQueue， // 而只是被赋予了一个 dummy BackendQueue if strings.HasSuffix(channelName, "#ephemeral") &#123; c.ephemeral = true c.backend = newDummyBackendQueue() &#125; else &#123; dqLogf := func(level diskqueue.LogLevel, f string, args ...interface&#123;&#125;) &#123; opts := ctx.nsqd.getOpts() lg.Logf(opts.Logger, opts.LogLevel, lg.LogLevel(level), f, args...) &#125; // backend names, for uniqueness, automatically include the topic... // 4. 实例化一个后端持久化存储，同样是通过 go-diskqueue 来创建的， // 其初始化参数同 topic 中实例化 backendQueue 参数类似 backendName := getBackendName(topicName, channelName) c.backend = diskqueue.New( backendName, ctx.nsqd.getOpts().DataPath, ctx.nsqd.getOpts().MaxBytesPerFile, int32(minValidMsgLength), int32(ctx.nsqd.getOpts().MaxMsgSize)+minValidMsgLength, ctx.nsqd.getOpts().SyncEvery, ctx.nsqd.getOpts().SyncTimeout, dqLogf, ) &#125; // 5. 通知 lookupd 添加注册信息 c.ctx.nsqd.Notify(c) return c&#125; // /nsq/nsqd/channel.go 类似地，前文提过channel不会被预先创建，一般是因为某个消费者在订阅channel时才被创建的。同样，我们追踪方法调用，发现只有topic.getOrCreateChannel方法调用了NewChannel构造方法，而它又只会被topic.GetChannel方法调用。因此，程序中只存在三条调用链：其一，nsqd.Start-&gt;nsqd.LoadMetadata-&gt;topic.GetChannel-&gt;topic.getOrCreateChannel-&gt;NewChannel；其二，httpServer.doCreateChannel-&gt;topic.GetChannel；以及protocolV2.SUB-&gt;topic.GetChannel。 删除或关闭 channel 实例删除(Delete)或者关闭(Close)channel实例的方法逻辑同topic也非常类似。相似部分不多阐述，读者若需要参考，可以看这里。这里重点阐述两个不同点：其一，无论是关闭还是删除channel都会显式地将订阅了此channel的客户端强制关闭（当然是关闭客户端在服务端的实体）；其二，关闭和删除channel都会显式刷新channel，即将channel所维护的三个消息队列：内存消息队列memoryMsgChan、正在发送的优先级消息队列inFlightPQ以及被推迟发送的优先级消息队列deferredPQ，将它们的消息显式写入到持久化存储消息队列。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100// 删除此 channel，清空所有消息，然后关闭func (c *Channel) Delete() error &#123; return c.exit(true)&#125;// 只是将三个消息队列中的消息刷盘，然后关闭func (c *Channel) Close() error &#123; return c.exit(false)&#125; // /nsq/nsqd/channel.gofunc (c *Channel) exit(deleted bool) error &#123; c.exitMutex.Lock() defer c.exitMutex.Unlock() // 1. 保证还未被设置 exitFlag，即还在运行中，同时设置 exitFlag if !atomic.CompareAndSwapInt32(&amp;c.exitFlag, 0, 1) &#123; return errors.New("exiting") &#125; // 2. 若需要删除数据，则通知 nsqlookupd，有 channel 被删除 if deleted &#123; c.ctx.nsqd.logf(LOG_INFO, "CHANNEL(%s): deleting", c.name) c.ctx.nsqd.Notify(c) &#125; else &#123; c.ctx.nsqd.logf(LOG_INFO, "CHANNEL(%s): closing", c.name) &#125; c.RLock() // 3. 强制关闭所有订阅了此 channel 的客户端 for _, client := range c.clients &#123; client.Close() &#125; c.RUnlock() // 4. 清空此 channel 所维护的内存消息队列和持久化存储消息队列中的消息 if deleted &#123; // empty the queue (deletes the backend files, too) c.Empty() // 5. 删除持久化存储消息队列中的消息 return c.backend.Delete() &#125; // 6. 强制将内存消息队列、以及两个发送消息优先级队列中的消息写到持久化存储中 c.flush() // 7. 关闭持久化存储消息队列 return c.backend.Close()&#125; // /nsq/nsqd/channel.go// 清空 channel 的消息func (c *Channel) Empty() error &#123; c.Lock() defer c.Unlock() // 1. 重新初始化（清空） in-flight queue 及 deferred queue c.initPQ() // 2. 清空由 channel 为客户端维护的一些信息，比如 当前正在发送的消息的数量 InFlightCount // 同时更新了 ReadyStateChan for _, client := range c.clients &#123; client.Empty() &#125; // 3. 将 memoryMsgChan 中的消息清空 for &#123; select &#123; case &lt;-c.memoryMsgChan: default: goto finish &#125; &#125; // 4. 最后将后端持久化存储中的消息清空finish: return c.backend.Empty()&#125; // /nsq/nsqd/channel.go// 将未消费的消息都写到持久化存储中，// 主要包括三个消息集合：memoryMsgChan、inFlightMessages和deferredMessagesfunc (c *Channel) flush() error &#123; var msgBuf bytes.Buffer // ... // 1. 将内存消息队列中的积压的消息刷盘 for &#123; select &#123; case msg := &lt;-c.memoryMsgChan: err := writeMessageToBackend(&amp;msgBuf, msg, c.backend) // ... default: goto finish &#125; &#125; // 2. 将还未发送出去的消息 inFlightMessages 也写到持久化存储finish: c.inFlightMutex.Lock() for _, msg := range c.inFlightMessages &#123; err := writeMessageToBackend(&amp;msgBuf, msg, c.backend) // ... &#125; c.inFlightMutex.Unlock() // 3. 将被推迟发送的消息集合中的 deferredMessages 消息也到持久化存储 c.deferredMutex.Lock() for _, item := range c.deferredMessages &#123; msg := item.Value.(*Message) err := writeMessageToBackend(&amp;msgBuf, msg, c.backend) // ... &#125; c.deferredMutex.Unlock() return nil&#125; // /nsq/nsqd/channel.go 查询 channel 实例同样是即依据名称查询（获取）channel实例，它被定义为topic实例的方法。查询逻辑的关键是，若此channl不在topic的channel集合中，则需要创建一个新的channel实例。并为其注册channel实例的删除回调函数。接下来，还要更新topic.memoryMsgChan和topoc.backendChan结构（因为channel集合更新了）。相关代码如下： 1234567891011121314151617181920212223242526272829303132// 根据 channel 名称返回 channel 实例，且有可能是新建的。线程安全方法。func (t *Topic) GetChannel(channelName string) *Channel &#123; t.Lock() channel, isNew := t.getOrCreateChannel(channelName) t.Unlock() if isNew &#123; // update messagePump state select &#123; // 若此 channel 为新创建的，则 push 消息到 channelUpdateChan中， // 使 memoryMsgChan 及 backend 刷新状态 case t.channelUpdateChan &lt;- 1: case &lt;-t.exitChan: &#125; &#125; return channel&#125; // /nsq/nsqd/topic.go// 根据 channel 名称获取指定的 channel，若不存在，则创建一个新的 channel 实例。非线程安全func (t *Topic) getOrCreateChannel(channelName string) (*Channel, bool) &#123; channel, ok := t.channelMap[channelName] if !ok &#123; // 注册 channel 被删除时的回调函数 deleteCallback := func(c *Channel) &#123; t.DeleteExistingChannel(c.name) &#125; channel = NewChannel(t.name, channelName, t.ctx, deleteCallback) t.channelMap[channelName] = channel t.ctx.nsqd.logf(LOG_INFO, "TOPIC(%s): new channel(%s)", t.name, channel.name) return channel, true &#125; return channel, false&#125; // /nsq/nsqd/topic.go 简单小结，本文内容同上一篇文章分析topic源码非常相似，因此阐述得比较简单，只是贴了注释的源码，并重点阐述二者不同。文章围绕channel展开，首先简要介绍channel结构字段的组成；然后，分析message相关的字段，以及chanel维护的两个和消息发送相关的优先级队列：inFlightPQ，存放正在发送的消息的优先级队列，以及deferredPQ，存放被推迟发送的消息的优先级队列。接下来，分析了channel实例化的逻辑以及channel删除逻辑；最后阐述channel的查询过程，查询过程需要注意的是通知topic的消息处理主循环messagePump更新两个消息队列实例。 参考文献 [1]. https://github.com/nsqio/nsq[2]. https://nsq.io/overview/quick_start.html]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nsq topic 源码简析]]></title>
    <url>%2F2019%2F05%2F14%2Fnsq-topic-%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章阐述了nsqd模块的源码。准确而言是nsqd服务启动过程的部分源码。内容非常多，总共分为五个部分来分析，其中略讲的内容包括利用svc启动进程的流程、nsqd实例创建及初始化过程。重点阐述的内容包括nsqd异步开启nsqlookupd查询过程，以及nsqd与nsqlookupd通信的主循环逻辑。另外，还有nsqd建立的tcp连接处理器的相关内容，这点比较复杂，涉及两个过程：IOLoop主循环读取连接的请求内容，以及messagePump处理消息发送的核心逻辑。最后略讲http连接处理器的创建过程。本文的主题是topic，相对简单。topic可以看作是生产者投递消息的一个逻辑键，一个nsqd实例可以维护多个topic实例，每当生产者将消息投递到某个的nsqd上特定的topic时，它随即将消息拷贝后发送到与其关联的channel实例集合。与channel实例类似，topic实例存储消息也涉及到两个消息队列：内存消息队列和持久化消息队列，因此对于消息的存储维护，topic实例和channel实例分开管理。同样，为了方便读者理解，本文只论述到topic本身的相关逻辑，换言之，不涉及到topic收发消息的逻辑。 本文分析nsq topic模块的相关逻辑，更详细nsq源码注释可在这里找到，注释源码版本为v1.1.0，仅供参考。本文所涉及到源码主要为/nsq/nsqd/和/nsq/internal/下的若干子目录。 本文侧重于分析topic模块本身相关源码，而关于topic如何从生产者收到消息、如何对消息进行存储处理，最后又如何将消息转发给channel实例，这部分会另外写一篇文章专门阐述，这也是nsq系统非常核心的处理流程。本文从四个方面来阐述topic：其一，简要介绍topic结构相关字段；其二，阐述创建topic相关逻辑；其三，分析删除topic的过程；最后阐述topic的查询过程。 topic 实例结构topic结构所包含的字段比较简单，其中重要的包括：channelMap表示topic实例所关联的channel实例集合；backend表示topic所使用的消息持久化队列接口；memoryMsgChan则表示内存消息队列；还有一个channelUpdateChan通道表示当消息被更新时（添加或删除），通知topic的消息处理主循环中执行相应的逻辑，即更新两个消息队列。ephemeral字段表示topic是否是临时的，所谓临时的topic（#ephemeral开头）不会被持久化(PersistMetadata)，且当topic 包含的所有channel都被删除后，此topic也会被删除。 123456789101112131415161718192021222324252627type Topic struct &#123; // 64bit atomic vars need to be first for proper alignment on 32bit platforms messageCount uint64 // 此 topic 所包含的消息的总数（内存+磁盘） messageBytes uint64 // 此 topic 所包含的消息的总大小（内存+磁盘） sync.RWMutex // guards channelMap name string // topic 名称 channelMap map[string]*Channel // topic 所包含的 channel 集合 backend BackendQueue // 代表持久化存储的通道 memoryMsgChan chan *Message // 代表消息在内存中的通道 startChan chan int // 消息处理循环开关 exitChan chan int // topic 消息处理循环退出开关 channelUpdateChan chan int // 消息更新的开关 waitGroup util.WaitGroupWrapper // waitGroup 的一个 wrapper // 其会在删除一个topic时被设置，且若被设置，则 putMessage(s)操作会返回错误，拒绝写入消息 exitFlag int32 idFactory *guidFactory // 用于生成客户端实例的ID // 临时的 topic（#ephemeral开头），此种类型的 topic 不会进行持久化， // 当此 topic 所包含的所有的 channel 都被删除后，被标记为ephemeral的topic也会被删除 ephemeral bool // topic 被删除前的回调函数，且对 ephemeral 类型的 topic有效，并且它只在 DeleteExistingChannel 方法中被调用 deleteCallback func(*Topic) deleter sync.Once // 标记此 topic 是否有被 paused，若被 paused，则其不会将消息写入到其关联的 channel 的消息队列 paused int32 pauseChan chan int ctx *context // nsqd 实例的 wrapper&#125; // /nsq/nsqd/topic.go 创建 topic 实例先简单了解topic的构造方法，其大概涉及到这么几个步骤：先初始化实例结构，然后若此topic为ephemeral的，则设置标记，并且为此topic关联一个DummyBackendQueue作为其持久化存储，事实上，DummyBackendQueue表示不执行任何有效动作，显然这是考虑到临时的topic不用被持久化。对于正常的topic，则为其创建一个diskqueue实例作为后端存储消息队列，通过nsqd配置参数进行初始化（diskqueue在后面单独开一篇文章解析）。最后异步开启topic的消息处理主循环messagePump，并通知nsqlookupd有新的topic实例产生。它会在nsqd.Notify方法中被接收，然后将此topic实例压入到nsqd.notifyChan管道，相应地，此topic实例在nsqd.lookupLoop方法中被取出，然后构建并发送REGISTER命令请求给nsqd所维护的所有nsqlookupd实例。最后，通过调用PersistMetadata方法将此topic元信息持久化。（方法调用链为：NewTopic-&gt;nsqd.Notify-&gt;nsqd.lookupLoop-&gt;nsqd.PersistMetadata）相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// topic 的构造函数func NewTopic(topicName string, ctx *context, deleteCallback func(*Topic)) *Topic &#123; // 1. 构造 topic 实例 t := &amp;Topic&#123; name: topicName, channelMap: make(map[string]*Channel), memoryMsgChan: make(chan *Message, ctx.nsqd.getOpts().MemQueueSize), startChan: make(chan int, 1), exitChan: make(chan int), channelUpdateChan: make(chan int), ctx: ctx, paused: 0, pauseChan: make(chan int), deleteCallback: deleteCallback, idFactory: NewGUIDFactory(ctx.nsqd.getOpts().ID), &#125; // 2. 标记那些带有 ephemeral 的 topic，并为它们构建一个 Dummy BackendQueue， // 因为这些 topic 所包含的的消息不会被持久化，因此不需要持久化队列 BackendQueue。 if strings.HasSuffix(topicName, "#ephemeral") &#123; t.ephemeral = true t.backend = newDummyBackendQueue() &#125; else &#123; dqLogf := func(level diskqueue.LogLevel, f string, args ...interface&#123;&#125;) &#123; opts := ctx.nsqd.getOpts() lg.Logf(opts.Logger, opts.LogLevel, lg.LogLevel(level), f, args...) &#125; // 3. 通过 diskqueue (https://github.com/nsqio/go-diskqueue) 构建持久化队列实例 t.backend = diskqueue.New( topicName, // topic 名称 ctx.nsqd.getOpts().DataPath, // 数据存储路径 ctx.nsqd.getOpts().MaxBytesPerFile, // 存储文件的最大字节数 int32(minValidMsgLength), // 最小的有效消息的长度 int32(ctx.nsqd.getOpts().MaxMsgSize)+minValidMsgLength, // 最大的有效消息的长度 // 单次同步刷新消息的数量，即当消息数量达到 SyncEvery 的数量时， // 需要执行刷新动作（否则会留在操作系统缓冲区） ctx.nsqd.getOpts().SyncEvery, ctx.nsqd.getOpts().SyncTimeout, // 两次同步刷新的时间间隔，即两次同步操作的最大间隔 dqLogf, // 日志 ) &#125; // 4. 执行 messagePump 方法，即 开启消息监听 go routine t.waitGroup.Wrap(t.messagePump) // 5. 通知 nsqlookupd 有新的 topic 产生 t.ctx.nsqd.Notify(t) return t&#125; // /nsq/nsqd/topic.go// 通知 nsqd 将 metadata 信息持久化到磁盘，若 nsqd 当前未处于启动过程func (n *NSQD) Notify(v interface&#123;&#125;) &#123; // 考虑到若在 nsqd 刚启动处于加载元数据，则此时数据并不完整，因此不会在此时执行持久化操作 persist := atomic.LoadInt32(&amp;n.isLoading) == 0 n.waitGroup.Wrap(func() &#123; select &#123; case &lt;-n.exitChan: case n.notifyChan &lt;- v: if !persist &#123; return &#125; n.Lock() // 重新持久化 topic 及 channel 的元信息 err := n.PersistMetadata() // ... n.Unlock() &#125; &#125;)&#125; 最后，简单分析下，程序中哪些地方会调用此构造方法。前文提到topic不会被提前创建，一定是因为某个生产者在注册topic时临时被创建的。其实通过追踪方法调用，发现只有nsqd.GetTopic方法调用了NewTopic构造方法。因此，程序中存在以下几条调用链：其一，nsqd.Start-&gt;nsqd.PersistMetadata-&gt;nsqd.GetTopic-&gt;NewTopic；其二，httpServer.getTopicFromQuery-&gt;nsqd.GetTopic-&gt;NewTopic；以及protocolV2.PUB/SUB-&gt;nsqd.GetTopic这三条调用路径。相信读者已经非常清楚了。 删除或关闭 topic 实例topic删除的方法(topic.Delete)与其被关闭的方法(topic.Close)相似，都调用了topic.exit方法，区别有三点：一是前者还显式调用了nsqd.Notify以通知nsqlookupd有topic实例被删除，同时重新持久化元数据。二是前者还需要递归删除topic关联的channel集合，且显式调用了channel.Delete方法（此方法同topic.Delete方法相似）。最后一点区别为前者还显式清空了memoryMsgChan和backend两个消息队列中的消息。因此，若只是关闭或退出topic，则纯粹退出messagePump消息处理循环，并将memoryMsgChan中的消息刷盘，最后关闭持久化存储消息队列。（方法调用链为：topic.Delete-&gt;topic.exit-&gt;nsqd.Notify-&gt;nsqd.PersistMetadata-&gt;chanel.Delete-&gt;topic.Empty-&gt;topic.backend.Empty-&gt;topic.backend.Delete，以及topic.Close-&gt;topic.exit-&gt;topic.flush-&gt;topic.backend.Close）相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// Delete 方法和 Close 方法都调用的是 exit 方法。// 区别在于 Delete 还需要显式得通知 lookupd，让它删除此 topic 的注册信息// 而 Close 方法是在 topic 关闭时调用，因此需要持久化所有未被处理/消费的消息，然后再关闭所有的 channel，退出func (t *Topic) Delete() error &#123; return t.exit(true)&#125;func (t *Topic) Close() error &#123; return t.exit(false)&#125; // /nsq/nsqd/topic.go// 使当前 topic 对象 exit，同时若指定删除其所关联的 channels 及 closes，则清空它们func (t *Topic) exit(deleted bool) error &#123; // 1. 保证目前还处于运行的状态 if !atomic.CompareAndSwapInt32(&amp;t.exitFlag, 0, 1) &#123; return errors.New("exiting") &#125; // 2. 当被 Delete 调用时，则需要先通知 lookupd 删除其对应的注册信息 if deleted &#123; t.ctx.nsqd.logf(LOG_INFO, "TOPIC(%s): deleting", t.name) t.ctx.nsqd.Notify(t) // 通知 nsqlookupd 有 topic 更新，并重新持久化元数据 &#125; else &#123; t.ctx.nsqd.logf(LOG_INFO, "TOPIC(%s): closing", t.name) &#125; // 3. 关闭 exitChan，保证所有的循环全部会退出，比如消息处理循环 messagePump 会退出 close(t.exitChan) // 4. 同步等待消息处理循环 messagePump 方法的退出， // 才继续执行下面的操作（只有消息处理循环退出后，才能删除对应的 channel集合） t.waitGroup.Wait() // 4. 若是被 Delete 方法调用，则需要清空 topic 所包含的 channel（同 topic 的操作类似） if deleted &#123; t.Lock() for _, channel := range t.channelMap &#123; delete(t.channelMap, channel.name) channel.Delete() &#125; t.Unlock() t.Empty() // 清空 memoryMsgChan 和 backend 中的消息 return t.backend.Delete() &#125; // 5. 否则若是被 Close 方法调用，则只需要关闭所有的 channel， // 不会将所有的 channel 从 topic 的 channelMap 中删除 for _, channel := range t.channelMap &#123; err := channel.Close() // ... &#125; // 6. 将内存中的消息，即 t.memoryMsgChan 中的消息刷新到持久化存储 t.flush() return t.backend.Close()&#125; // /nsq/nsqd/topic.go// 清空内存消息队列和持久化存储消息队列中的消息func (t *Topic) Empty() error &#123; for &#123; select &#123; case &lt;-t.memoryMsgChan: default: goto finish &#125; &#125;finish: return t.backend.Empty()&#125; // /nsq/nsqd/topic.go// 刷新内存消息队列即 t.memoryMsgChan 中的消息到持久化存储 backendfunc (t *Topic) flush() error &#123; var msgBuf bytes.Buffer // ... for &#123; select &#123; case msg := &lt;-t.memoryMsgChan: err := writeMessageToBackend(&amp;msgBuf, msg, t.backend) // ... default: goto finish &#125; &#125;finish: return nil&#125; // /nsq/nsqd/topic.go 最后同样简单分析程序中哪些地方会调用Delete方法。其一，httpServer.doDeleteTopic-&gt;nsqd.DeleteExistingTopic-&gt;topic.Delete；其二，nsqd.GetTopic-&gt;nsqd.DeleteExistingTopic-&gt;topic.Delete。而对于topic.Close方法，则比较直接：nsqd.Exit-&gt;topic.Close。 查询 topic 实例即依据名称查询（获取）topic实例，包含了两个方法，都被定义为nsqd实例的方法。我们重点阐述nsqd.GetTopic方法。查询逻辑的关键是，若此topic不存在nsqd的topic集合中，则需要创建一个新的实例。同时，为其注册topic实例的删除回调函数。接下来，若此nsqd并非处于启动过程（还记得nsqd.LoadMetadata会调用nsqd.GetTopic方法吗），则还要进一步填充此topic所关联的channel，即nsqd实例向nsqlookupd实例查询指定topic所关联的channel集合，然后更新topic.channelMap，同时也要更新topic.memoryMsgChan和topoc.backendChan结构（因为channel集合更新了）。最后，启动此topic，即开启topic处理消息的主循环topic.messagePump。相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// GetTopic 是一个线程安全的方法，其根据 topic 名称返回指向一个 topic 对象的指针，// 此 topic 对象有可能是新创建的func (n *NSQD) GetTopic(topicName string) *Topic &#123; // 1. 通常，此 topic 已经被创建，因此（使用读锁）先从 nsqd 的 topicMap 中查询指定指定的 topic n.RLock() t, ok := n.topicMap[topicName] n.RUnlock() if ok &#123; return t &#125; n.Lock() // 2. 因为上面查询指定的 topic 是否存在时，使用的是读锁， // 因此有线程可能同时进入到这里，执行了创建同一个 topic 的操作，因此这里还需要判断一次。 t, ok = n.topicMap[topicName] if ok &#123; n.Unlock() return t &#125; // 3. 创建删除指定 topic 的回调函数，即在删除指定的 topic 之前，需要做的一些清理工作， // 比如关闭 与此 topic 所关联的channel，同时判断删除此 topic 所包含的所有 channel deleteCallback := func(t *Topic) &#123; n.DeleteExistingTopic(t.name) &#125; // 4. 通过 nsqd、topicName 和删除回调函数创建一个新的 topic，并将此 topic 添加到 nsqd 的 topicMap中 // 创建 topic 过程中会初始化 diskqueue, 同时开启消息协程 t = NewTopic(topicName, &amp;context&#123;n&#125;, deleteCallback) n.topicMap[topicName] = t n.Unlock() n.logf(LOG_INFO, "TOPIC(%s): created", t.name) // 此时内存中的两个消息队列 memoryMsgChan 和 backend 还未开始正常工作 if atomic.LoadInt32(&amp;n.isLoading) == 1 &#123; return t &#125; // 对于新建的 topic还要查询channel集合的原因，只能是 nsqd 实例重启，丢失了topic和channel信息 // TODO lookupdHTTPAddrs := n.lookupdHTTPAddrs() if len(lookupdHTTPAddrs) &gt; 0 &#123; // 5.1 从指定的 nsqlookupd 及 topic 所获取的 channel 的集合 // nsqlookupd 存储所有之前此 topic 创建的 channel 信息，因此需要加载消息 channelNames, err := n.ci.GetLookupdTopicChannels(t.name, lookupdHTTPAddrs) // ... // 5.2 对那些非 ephemeral 的 channel， // 创建对应的实例（因为没有使用返回值，因此纯粹是更新了内在中的memoryMsgChan和backend结构） for _, channelName := range channelNames &#123; // 对于临时的 channel，则不需要创建，使用的时候再创建 if strings.HasSuffix(channelName, "#ephemeral") &#123; continue // do not create ephemeral channel with no consumer client &#125; // 5.3 根据 channel name 获取 channel 实例，且有可能是新建的 // 若是新建了一个 channel，则通知 topic 的后台消息协程去处理 channel 的更新事件 // 之所以在查询到指定 channel 的情况下，新建 channel，是为了保证消息尽可能不被丢失， // 比如在 nsq 重启时，需要在重启的时刻创建那些 channel，避免生产者生产的消息 // 不能被放到 channel 中，因为在这种情况下， // 只能等待消费者来指定的 channel 中获取消息才会创建。 t.GetChannel(channelName) &#125; &#125; else if len(n.getOpts().NSQLookupdTCPAddresses) &gt; 0 &#123; // ... &#125; // 6. 启动了消息处理的循环，往 startChan 通道中 push 了一条消息， // 此时会内存消息队列 memoryMsgChan，以及持久化的消息队列 backendChan 就开始工作。 // 即能处理内存中消息更新的的事件了。 t.Start() return t&#125; // /nsq/nsqd/nsqd.gofunc (t *Topic) Start() &#123; select &#123; case t.startChan &lt;- 1: default: &#125;&#125; // /nsq/nsqd/topic.go 关于ClusterInfo.GetLookupdTopicChannels方法没有展开分析了，比较简单，纯粹就构建查询请求，并获得响应内容，最后对channel集合执行合并操作。另外，程序中关于nsqd.GetTopic的逻辑，在前面已阐述过。 简单小结，本文内容相比上一篇文章较少，逻辑性也相对较弱，因此较容易理解消化。文章围绕topic展开，从四个方面对topic进行介绍，其中topic实例所包含的字段比较简单。而topic实例化方法也很直接，关键在于从一条主线来把握方法，即结合系统调用逻辑，理解涉及到的整个调用方法链。删除或关闭topic的核心是对两个消息队列的操作。最后查询topic实例的方法GetTopic方法比较关键。 参考文献 [1]. https://github.com/nsqio/nsq[2]. https://nsq.io/overview/quick_start.html]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nsq nsqd 服务启动源码简析]]></title>
    <url>%2F2019%2F05%2F13%2Fnsq-nsqd-%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章阐述了nsqlookupd模块的源码，主要分析nsqlookupd服务进程启动、实例构建及初始化、tcp &amp; http请求处理器的构建和注册以及由nsqlookupd提供的topic注册及查询功能这几个流程的相关源码。nsqlookupd耦合的模块较少，程序逻辑也较简单。但由于其和topic及channel等密切相关，因此nsqd更为复杂。nsqd充当nsq消息队列核心角色，它负责接收、排队以及转发（投递）消息，因此这需要同nsq各个组件交互，包括生产者、消费者、nsqlookupd以及nsqadmin。nsq提供http/https的方式与生产者通信，主要包括topic和channel创建和查询，配置更新，以及消息发布等功能。另外nsq提供了tcp的方式与消费者及生产者通信，为消费者提供消息订阅功能，而为生产者提供消息发布的功能。最后，考虑到nsqd无状态的特性，nsqd可以通过横向扩展来增强请求处理能力，也可以通过增加一个或多个备份来提高数据可靠性。 再次强调，个人认为查看或分析源码最好从某个业务逻辑流程切入，必要时忽略某些旁支或细节，做到从宏观上把握整个流程。本文分析nsqd服务启动的关键流程，更详细nsq源码注释可在这里找到，注释源码版本为v1.1.0，仅供参考。本文所涉及到源码主要为/nsq/apps/nsqd/、/nsq/nsqd/和/nsq/internal/下的若干子目录。 考虑到nsqd本身比较复杂，难以在一篇文章中介绍全部内容，因此选择将其进行拆分。本文侧重于分析nsqd服务启动相关源码，而在启动过程中涉及到的与topic和channel耦合部分会另外写一篇文章专门阐述。本文从五个方面来阐述nsqd：其一，以nsqd命令为切入点，介绍服务启动流程（这部分同nsqlookupd非常类似，因此会简述）；其二，同样追溯nsqd启动流程，进一步分析介绍初始化过程中NSQ的创建及初始化逻辑；其三，阐述nsqd异步开启nsqlookupd查询过程；其四，阐述nsqd同nsqlookupd交互的主循环的逻辑（对应源码的NSQD.lookupLoop方法）；最后，分析初始化过程所涵盖的nsqd建立tcp和http请求处理器相关逻辑。注意，nsqd的核心流程——开启消息队列扫描（对应源码的NSQD.queueScanLoop方法），这部分与topic及channel密切相关，因此放到后面单独阐述。 当我们在命令行执行nsqd命令时（同时可指定参数），相当于运行了nsq/apps/nsqd程序的main方法。此方法启动了一个进程（服务），并且通过创建NSQD并调用其Main方法执行启动逻辑。 利用 svc 启动进程同nsqlookupd进程启动类似，nsqd进程的启动，同样是简单包装 svc的Run方法以启动一个进程（守护进程或服务），然后在 svc.Run 方法中依次调用 Init 和 Start 方法，并阻塞直到接收到 SIGINT或SIGTERM，最后调用 stop方法后退出进程。更多可以查看 golang 标准包的signal.Notify以及svc包是如何协助启动一个进程。启动过程中，首先加载、设置并解析配置参数实例opts，然后由此配置实例化NSQD。接下来调用nsqd的LoadMetaData方法加载元数据信息，所谓的元数据信息即包括了nsqd所维护的topic及channel信息（重点是其名称及paused状态）。加载完成后，立即就重新将元信息存盘（个人暂时也没完全想明白原因 #TODO）。最后异步调用nsqd.Main方法完成启动过程的核心逻辑，同时在退出时先调用了nsqd.Exit方法。下面对这些过程一一展开叙述。启动过程代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576type program struct &#123; once sync.Once nsqd *nsqd.NSQD&#125;// nsqd 服务程序执行入口，关于 svc 参考 apps/nsqlookupd/main.gofunc main() &#123; prg := &amp;program&#123;&#125; if err := svc.Run(prg, syscall.SIGINT, syscall.SIGTERM); err != nil &#123; logFatal("%s", err) &#125;&#125; // /nsq/apps/nsqd/main.go// 在 Start 方法调用之前执行，在此无实际用途func (p *program) Init(env svc.Environment) error &#123; if env.IsWindowsService() &#123; dir := filepath.Dir(os.Args[0]) return os.Chdir(dir) &#125; return nil&#125;// 启动方法func (p *program) Start() error &#123; // 1. 通过程序默认的参数构建 options 实例 opts := nsqd.NewOptions() // 2. 将 opts 结合命令行参数集进行进一步初始化 flagSet := nsqdFlagSet(opts) flagSet.Parse(os.Args[1:]) rand.Seed(time.Now().UTC().UnixNano()) // 3. 若 version 参数存在，则打印版本号，然后退出 if flagSet.Lookup("version").Value.(flag.Getter).Get().(bool) &#123; fmt.Println(version.String("nsqd")) os.Exit(0) &#125; // 4. 若用户指定了自定义配置文件，则加载配置文件，读取配置文件，校验配置文件合法性 // 读取解析配置文件采用的是第三方库 https://github.com/BurntSushi/toml var cfg config configFile := flagSet.Lookup("config").Value.String() if configFile != "" &#123; _, err := toml.DecodeFile(configFile, &amp;cfg) // ... &#125; cfg.Validate() options.Resolve(opts, flagSet, cfg) // 5. 通过给定参数 opts 构建 nsqd 实例 nsqd, err := nsqd.New(opts) if err != nil &#123; logFatal("failed to instantiate nsqd - %s", err) &#125; p.nsqd = nsqd // 6. 加载 metadata 文件， // 若文件存在，则恢复 topic 和 channel的信息（如pause状态），并调用 topic.Start方法 err = p.nsqd.LoadMetadata() if err != nil &#123; logFatal("failed to load metadata - %s", err) &#125; // 7. 重新持久化 metadata 到文件，原因？ TODO // 即持久化 topic 及 channel的元信息（即不包括其数据）到文件中 err = p.nsqd.PersistMetadata() if err != nil &#123; logFatal("failed to persist metadata - %s", err) &#125; // 8. 在单独的 go routine 中启动 nsqd.Main 方法 go func() &#123; err := p.nsqd.Main() if err != nil &#123; p.Stop() os.Exit(1) &#125; &#125;() return nil&#125; // /nsq/apps/nsqd/main.gofunc (p *program) Stop() error &#123; p.once.Do(func() &#123; p.nsqd.Exit() &#125;) return nil&#125; // /nsq/apps/nsqd/main.go 下面分析main方法中的关键方法。元数据加载和持久化方法（nsqd.LoadMetadata和nsqd.persistMetadata），这里的元数据不包括具体的数据，比如message。在加载元数据过程中即读取nsqd.dat文件，如果文件内容为空，则表明是首次启动，直接返回。否则，读取文件内容并反序列化，针对读取的topic的列表中的每一个topic，会获取与其关联的channel列表，并设置它们的paused属性。关于paused属性，对于topic而言，若paused属性被设置，则它不会将由生产者发布的消息写入到关联的channel的消息队列。而对channel而言，若其paused属性被设置，则那些订阅了此channel的客户端不会被推送消息（这两点在后面的源码中可以验证）。其中根据topic或channel的名称获取对应的实例的方法为nsqd.GetTopic和topic.GetChannel方法，这两个方法会在阐述topic和channel的时详细分析。但注意一点是，若获取一个不存在的topic/channel，则会创建一个对应实例（还记得第一篇文章所述，topic及channel实例不会被提前创建，而是在生产者发布消息或显式创建一个topic时才被创建，而channel则是在生产者显式地订阅一个channel时才被创建）。最后调用topic.Start方法向topic.startChan通道中压入一条消息，消息会在topic.messagePump方法中被取出，以表明topic可以开始进入消息队列处理的主循环。元数据的持久化则恰是一个逆过程，即获取nsqd实例内存中的topic集合，并递归地将其对应的channel集合保存到文件，且持久化也是通过先写临时文件，再原子性地重命名。值得注意的是整个nsq中（包括nsqd和nsqlookupd）涉及到数据持久化的过程只有nsqd的元数据的持久化以及nsqd对消息的持久化（通过diskQueue完成），而nsqlookupd则不涉及持久化操作。元数据加载和持久化相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100// metadata 结构， Topic 结构的数组type meta struct &#123; Topics []struct &#123; Name string `json:"name"` Paused bool `json:"paused"` Channels []struct &#123; Name string `json:"name"` Paused bool `json:"paused"` &#125; `json:"channels"` &#125; `json:"topics"`&#125;// 加载 metadata func (n *NSQD) LoadMetadata() error &#123; atomic.StoreInt32(&amp;n.isLoading, 1) defer atomic.StoreInt32(&amp;n.isLoading, 0) // 1. 构建 metadata 文件全路径， nsqd.dat，并读取文件内容 fn := newMetadataFile(n.getOpts()) data, err := readOrEmpty(fn) // ... // 2. 若文件内容为空，则表明是第一次启动， metadata 加载过程结束 if data == nil &#123; return nil // fresh start &#125; var m meta err = json.Unmarshal(data, &amp;m) // ... // 3. 若文件内容不为空，则遍历所有 topic，针对每一个 topic 及 channel先前保持的情况进行还原． // 比如是否有被 pause，最后启动 topic for _, t := range m.Topics &#123; // ... // 根据 topic name 获取对应的 topic 实例，若对应的 topic 实例不存在，则会创建它。 // （因此在刚启动时，会创建所有之前保存的到文件中的 topic 实例，后面的 channel 也是类似的） topic := n.GetTopic(t.Name) if t.Paused &#123; topic.Pause() &#125; for _, c := range t.Channels &#123; if !protocol.IsValidChannelName(c.Name) &#123; n.logf(LOG_WARN, "skipping creation of invalid channel %s", c.Name) continue &#125; channel := topic.GetChannel(c.Name) if c.Paused &#123; channel.Pause() &#125; &#125; // 启动对应的 topic，开启了消息处理循环 topic.Start() &#125; return nil&#125; // /nsq/nsqd/nsqd.go// 创建 metadata 文件，遍历 nsqd 节点所有的 topic，// 针对每一个非 ephemeral 属性的 topic，保存其 name、paused 属性//（换言之不涉及到 topic 及 channel 的数据部分）// 另外，保存 topic 所关联的非 ephemeral 的 channel 的 name、paused 属性// 最后同步写入文件，注意在写文件，先是写到临时文件中，然后调用 OS.rename操作，以保证写入文件的原子性func (n *NSQD) PersistMetadata() error &#123; // persist metadata about what topics/channels we have, across restarts fileName := newMetadataFile(n.getOpts()) n.logf(LOG_INFO, "NSQ: persisting topic/channel metadata to %s", fileName) js := make(map[string]interface&#123;&#125;) topics := []interface&#123;&#125;&#123;&#125; for _, topic := range n.topicMap &#123; if topic.ephemeral &#123; // 临时的 topic 不被持久化 continue &#125; topicData := make(map[string]interface&#123;&#125;) topicData["name"] = topic.name topicData["paused"] = topic.IsPaused() channels := []interface&#123;&#125;&#123;&#125; topic.Lock() for _, channel := range topic.channelMap &#123; channel.Lock() if channel.ephemeral &#123; // 临时的 channel 不被持久化 channel.Unlock() continue &#125; channelData := make(map[string]interface&#123;&#125;) channelData["name"] = channel.name channelData["paused"] = channel.IsPaused() channels = append(channels, channelData) channel.Unlock() &#125; topic.Unlock() topicData["channels"] = channels topics = append(topics, topicData) &#125; js["version"] = version.Binary js["topics"] = topics data, err := json.Marshal(&amp;js) // ... tmpFileName := fmt.Sprintf("%s.%d.tmp", fileName, rand.Int()) err = writeSyncFile(tmpFileName, data) // ... err = os.Rename(tmpFileName, fileName) // ... // technically should fsync DataPath here return nil&#125; // /nsq/nsqd/nsqd.go 至此，nsqd的进程启动过程进行了大概地梳理。其包含两个重点一个是其利用svc来启动一个进程，另一个为nsqd加载及持久化元数据相关的逻辑，其中的paused属性与topic/channel密切相关。 nsqd 创建及初始化在阐述nsqd实例创建和启动过程前，先了解下其组成结构。其中最重要的几个字段为topicMap存储nsqd所维护的topic集合，lookupPeers为nsqd与nsqlookupd之间网络连接的抽象实体，cliens为订阅了此nsqd所维护的topic的客户端实体，还有notifyChan通道的作用是当channel或topic更新时（新增或删除），通知nsqlookupd服务更新对应的注册信息。相关代码如下： 12345678910111213141516171819202122232425262728type NSQD struct &#123; // 64bit atomic vars need to be first for proper alignment on 32bit platforms clientIDSequence int64 // nsqd 借助它为订阅的 client 生成 ID sync.RWMutex opts atomic.Value // 配置参数实例 dl *dirlock.DirLock isLoading int32 // nsqd 当前是否处于启动加载过程 errValue atomic.Value startTime time.Time topicMap map[string]*Topic // nsqd 所包含的 topic 集合 clientLock sync.RWMutex // guards clients // 向 nsqd 订阅的 client 的集合，即订阅了此 nsqd 所维护的 topic 的客户端 clients map[int64]Client lookupPeers atomic.Value // nsqd与nsqlookupd之间网络连接抽象实体 tcpListener net.Listener // tcp 连接 listener httpListener net.Listener // http 连接 listener httpsListener net.Listener // https 连接 listener tlsConfig *tls.Config // queueScanWorker 的数量，每个 queueScanWorker代表一个单独的goroutine，用于处理消息队列 poolSize int // 当 channel 或 topic 更新时（新增或删除），用于通知 nsqlookupd 服务更新对应的注册信息 notifyChan chan interface&#123;&#125; optsNotificationChan chan struct&#123;&#125; // 当 nsqd 的配置发生变更时，可以通过此 channel 通知 exitChan chan int // nsqd 退出开关 waitGroup util.WaitGroupWrapper // waitGroup 的一个 wrapper 结构 ci *clusterinfo.ClusterInfo&#125; // /nsq/nsqd/nsqd.go nsqd实例化过程，比较简单，没有特别关键逻辑，主要是初始化一些属性，创建tcp/http/https的连接监听。简要贴下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142func New(opts *Options) (*NSQD, error) &#123; var err error dataPath := opts.DataPath // ... n := &amp;NSQD&#123; startTime: time.Now(), topicMap: make(map[string]*Topic), clients: make(map[int64]Client), exitChan: make(chan int), notifyChan: make(chan interface&#123;&#125;), optsNotificationChan: make(chan struct&#123;&#125;, 1), dl: dirlock.New(dataPath), &#125; httpcli := http_api.NewClient(nil, opts.HTTPClientConnectTimeout, opts.HTTPClientRequestTimeout) n.ci = clusterinfo.New(n.logf, httpcli) n.lookupPeers.Store([]*lookupPeer&#123;&#125;) n.swapOpts(opts) n.errValue.Store(errStore&#123;&#125;) err = n.dl.Lock() // ... // ... if opts.TLSClientAuthPolicy != "" &amp;&amp; opts.TLSRequired == TLSNotRequired &#123; opts.TLSRequired = TLSRequired &#125; tlsConfig, err := buildTLSConfig(opts) // ... n.tlsConfig = tlsConfig for _, v := range opts.E2EProcessingLatencyPercentiles &#123; if v &lt;= 0 || v &gt; 1 &#123; return nil, fmt.Errorf("invalid E2E processing latency percentile: %v", v) &#125; &#125; n.tcpListener, err = net.Listen("tcp", opts.TCPAddress) // ... n.httpListener, err = net.Listen("tcp", opts.HTTPAddress) // ... if n.tlsConfig != nil &amp;&amp; opts.HTTPSAddress != "" &#123; n.httpsListener, err = tls.Listen("tcp", opts.HTTPSAddress, n.tlsConfig) // ... &#125; return n, nil&#125; // /nsq/nsqd/nsqd.go 重点在nsqd.Main方法中所涉及到的逻辑。它首先构建一个Context实例（纯粹nsqd实例的wrapper），然后注册一个方法退出的hook函数。接下来，构建并注册用于处理tcp和http请求的handler，其中tpc handler比较简单，而http handler同样复用httprouter作为请求路由器。而最关键的部分在于开启了三个goroutine用于处理nsqd内部逻辑，其中NSQD.queueScanLoop开启了nsqd的消息队列扫描处理逻辑，而NSQD.lookupLoop则开启了nsqlookupd查询过程，以及同nsqlookupd交互的主循环中的逻辑，最后的NSQD.statsdLoop则开启了一些数据统计工作（这部分不会做多介绍）。在nsqd.Main方法的最后，阻塞等待退出的信号exitChan。上述逻辑的相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// NSQD 进程启动入口程序func (n *NSQD) Main() error &#123; // 1. 构建 Context 实例， NSQD wrapper ctx := &amp;context&#123;n&#125; // 2. 同 NSQLookupd 类似，构建一个退出 hook 函数，且在退出时仅执行一次 exitCh := make(chan error) var once sync.Once exitFunc := func(err error) &#123; once.Do(func() &#123; if err != nil &#123; n.logf(LOG_FATAL, "%s", err) &#125; exitCh &lt;- err &#125;) &#125; // 3. 构建用于处理 tcp连接的tcp handler，同样注册退出前需要执行的函数（打印连接关闭错误信息） tcpServer := &amp;tcpServer&#123;ctx: ctx&#125; n.waitGroup.Wrap(func() &#123; exitFunc(protocol.TCPServer(n.tcpListener, tcpServer, n.logf)) &#125;) // 4. 构建用于处理 http 连接的 http handler，注册错误打印函数 httpServer := newHTTPServer(ctx, false, n.getOpts().TLSRequired == TLSRequired) n.waitGroup.Wrap(func() &#123; exitFunc(http_api.Serve(n.httpListener, httpServer, "HTTP", n.logf)) &#125;) // 5. 若配置了 https 通信，则仍然构建 http 连接的 https handler（但同时开启 tls）， // 同样注册错误打印函数 if n.tlsConfig != nil &amp;&amp; n.getOpts().HTTPSAddress != "" &#123; httpsServer := newHTTPServer(ctx, true, true) n.waitGroup.Wrap(func() &#123; exitFunc(http_api.Serve(n.httpsListener, httpsServer, "HTTPS", n.logf)) &#125;) &#125; // 6. 等待直到 queueScanLoop循环，lookupLoop 循环以及 statsdLoop，主程序才能退出 // 即开启了 队列scan扫描 goroutine 以及 lookup 的查找 goroutine n.waitGroup.Wrap(n.queueScanLoop) n.waitGroup.Wrap(n.lookupLoop) if n.getOpts().StatsdAddress != "" &#123; // 还有 状态统计处理 go routine n.waitGroup.Wrap(n.statsdLoop) &#125; err := &lt;-exitCh return err&#125; // /nsq/nsqd/nsqd.go nsqd 开启 nsqlookupd 查询过程这一小节介绍NSQD.lookupLoop方法，它代表nsqd开启nsqlookupd查询过程（这在一篇文章有介绍）。在nsqd刚创建时，通过读取配置文件中所配置的nsqlookupd实例地址(nsqlookupd_tcp_addresses)集合（一个nsqd可连接到多个nsqlookupd实例），建立对应的网络连接的抽象实体(lookupPeer实例)，设置自己的状态为stateDisconnected，同时传入连接建立成功后的一个回调函数(connectCallback)。接下来，则调用lookupPeer.Command方法向指定nsqlookupd发起连接建立过程。此时，连接建立成功后，立即向对方发送一个MagicV1的消息以声明自己的通信协议版本（官方称这有用于协议升级），并忽略响应。并判断若此前的连接状态为stateDisconnected，则调用其连接成功的回调函数connectCallback。上述逻辑相关的代码如下： 12345678910111213141516171819202122232425262728293031323334// 开启 lookup 循环func (n *NSQD) lookupLoop() &#123; var lookupPeers []*lookupPeer var lookupAddrs []string connect := true hostname, err := os.Hostname() // ... // for announcements, lookupd determines the host automatically ticker := time.Tick(15 * time.Second) for &#123; // 1. 在 nsqd 刚创建时，先构造 nsqd 同各 nsqlookupd（从配置文件中读取） // 的 lookupPeer 连接，并执行一个回调函数 if connect &#123; // 在 nsqd 启动时会进入到这里，即创建与各 nsqlookupd 的连接 for _, host := range n.getOpts().NSQLookupdTCPAddresses &#123; if in(host, lookupAddrs) &#123; continue &#125; n.logf(LOG_INFO, "LOOKUP(%s): adding peer", host) lookupPeer := newLookupPeer(host, n.getOpts().MaxBodySize, n.logf, connectCallback(n, hostname)) lookupPeer.Command(nil) // 开始建立连接，nil 代表连接初始建立，没用实际命令请求 // 更新 nsqlookupd 的连接实体和地址 lookupPeers = append(lookupPeers, lookupPeer) lookupAddrs = append(lookupAddrs, host) &#125; n.lookupPeers.Store(lookupPeers) connect = false &#125; // 这里是 nsqd 实例处理与 nsqlookupd 实例交互的主循环（在后面详细介绍） // ... &#125;exit: n.logf(LOG_INFO, "LOOKUP: closing")&#125; // /nsq/nsqd/nsqd.go 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// lookupPeer 代表 nsqd 同 nsqdlookupd 进行连接、读取以及写入操作的一种抽象类型结构// lookupPeer 实例被设计成延迟连接到 nsqlookupd，并且会自动重连type lookupPeer struct &#123; logf lg.AppLogFunc addr string // 需要连接到对端的地址信息，即为 nsqlookupd 的地址 conn net.Conn // 网络连接 state int32 // 当前 lookupPeer 连接的状态 5 种状态之一 connectCallback func(*lookupPeer) // 成功连接到指定的地址后的回调函数 maxBodySize int64 // 在读取命令请求的处理返回结果时，消息体的最大字节数 Info peerInfo&#125; // /nsq/nsqd/lookup_peer.go// peerInfo contains metadata for a lookupPeer instance (and is JSON marshalable)// peerInfo 代表 lookupPeer 实例的与网络连接相关的信息实体type peerInfo struct &#123; TCPPort int `json:"tcp_port"` HTTPPort int `json:"http_port"` Version string `json:"version"` BroadcastAddress string `json:"broadcast_address"`&#125;// ...// Read implements the io.Reader interface, adding deadlines// lookupPeer 实例从指定的连接中 lookupPeer.conn 中读取数据，并指定超时时间func (lp *lookupPeer) Read(data []byte) (int, error) &#123; lp.conn.SetReadDeadline(time.Now().Add(time.Second)) return lp.conn.Read(data)&#125;// Write implements the io.Writer interface, adding deadlines// lookupPeer 实例将数据写入到指定连接中，并指定超时时间func (lp *lookupPeer) Write(data []byte) (int, error) &#123; lp.conn.SetWriteDeadline(time.Now().Add(time.Second)) return lp.conn.Write(data)&#125; // /nsq/nsqd/lookup_peer.go// ...// 为 lookupPeer执行一个指定的命令，并且获取返回的结果。// 如果在这之前没有连接到对端，则会先进行连接动作func (lp *lookupPeer) Command(cmd *nsq.Command) ([]byte, error) &#123; initialState := lp.state // 1. 当连接尚未建立时，走这里 if lp.state != stateConnected &#123; err := lp.Connect() // 2. 发起连接建立过程 // ... lp.state = stateConnected // 3. 更新对应的连接状态 // 4. 在发送正式的命令请求前，需要要先发送一个 4byte 的序列号，用于协定后面用于通信的协议版本 _, err = lp.Write(nsq.MagicV1) if err != nil &#123; lp.Close() return nil, err &#125; // 5. 在连接成功后，需要执行一个成功连接的回调函数（在正式发送命令请求之前） if initialState == stateDisconnected &#123; lp.connectCallback(lp) &#125; if lp.state != stateConnected &#123; return nil, fmt.Errorf("lookupPeer connectCallback() failed") &#125; &#125; // 6. 在创建 lookupPeer 时会发送一个空的命令请求， // 其目的为创建正式的网络连接，同时，执行连接成功的回调函数 if cmd == nil &#123; return nil, nil &#125; // 7. 发送指定的命令请求到对端（包括命令的 name、params，一个空行以及body（写body之前要先写入其长度）） _, err := cmd.WriteTo(lp) // ... // 8. 读取并返回响应内容 resp, err := readResponseBounded(lp, lp.maxBodySize) // ... return resp, nil&#125; // /nsq/nsqd/lookup_peer.go 当连接建立成功后（不要忘记在这之前发送了一个MagicV1的消息），会执行一个回调函数。此回调函数的主要逻辑为nsqd向nsqlookupd发送一个IDENTIFY命令请求以表明自己身份信息，然后遍历自己所维护的topicMap集合，构建所有即将执行的REGISTER命令请求，最后依次执行每一个请求。相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041// 连接成功后需要执行的回调函数func connectCallback(n *NSQD, hostname string) func(*lookupPeer) &#123; return func(lp *lookupPeer) &#123; // 1. 打包 nsqd 自己的信息，主要是与网络连接相关 ci := make(map[string]interface&#123;&#125;) ci["version"] = version.Binary ci["tcp_port"] = n.RealTCPAddr().Port ci["http_port"] = n.RealHTTPAddr().Port ci["hostname"] = hostname ci["broadcast_address"] = n.getOpts().BroadcastAddress // 2. 发送一个 IDENTIFY 命令请求，以提供自己的身份信息 cmd, err := nsq.Identify(ci) // ... resp, err := lp.Command(cmd) // 3. 解析并校验 IDENTIFY 请求的响应内容 // ... // 4. 构建所有即将发送的 REGISTER 请求，用于向 nsqlookupd注册信息 topic 和channel信息 var commands []*nsq.Command n.RLock() for _, topic := range n.topicMap &#123; topic.RLock() if len(topic.channelMap) == 0 &#123; commands = append(commands, nsq.Register(topic.name, "")) &#125; else &#123; for _, channel := range topic.channelMap &#123; commands = append(commands, nsq.Register( channel.topicName, channel.name)) &#125; &#125; topic.RUnlock() &#125; n.RUnlock() // 5. 最后，遍历 REGISTER 命令集合，依次执行它们， // 并忽略返回结果（当然肯定要检测请求是否执行成功） for _, cmd := range commands &#123; n.logf(LOG_INFO, "LOOKUPD(%s): %s", lp, cmd) _, err := lp.Command(cmd) // ... &#125; &#125;&#125; // /nsq/nsqd/lookup.go nsqd 与 nsqlookupd 交互主循环当nsqd启动后与nsqlookup之后，它便开启了和nsqlookupd交互的主循环，即lookupLoop方法剩余部分。主循环中的逻辑主要分为三个部分，通过select语法来触发执行。其一，nsqd每过15s（好像是硬编码的）向nsqlookupd发送一个心跳消息(PING)；其二，通过notifyChan通道从nsqd收到消息时（即nsqd.Notify方法被调用），表明nsqd所维护的topic集合（包括channel）发生了变更（添加或移除）。若接收到的消息为channel，则根据此channel是否存在，进而发送REGISTER/UNREGISTER通知所有的nsqlookupd有channel 添加或除移。而topic的执行逻辑完全类似；最后，当从nsqd通过optsNotificationChan通道收到nsqlookupd地址变更消息，则重新从配置文件中加载nsqlookupd的配置信息。当然，若nsqd退出了，此处理循环也需要退出。相关的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990// 开启 lookup 循环func (n *NSQD) lookupLoop() &#123; var lookupPeers []*lookupPeer var lookupAddrs []string connect := true hostname, err := os.Hostname() // ... // for announcements, lookupd determines the host automatically ticker := time.Tick(15 * time.Second) for &#123; // 1. 在 nsqd 刚创建时，先构造 nsqd 同各 nsqlookupd（从配置文件中读取） // 的 lookupPeer 连接，并执行一个回调函数 // ... select &#123; // 2. 每 15s 就发送一个 heartbeat 消息给所有的 nsqlookupd，并读取响应。 // 此目的是为了及时检测到已关闭的连接 case &lt;-ticker: // send a heartbeat and read a response (read detects closed conns) for _, lookupPeer := range lookupPeers &#123; n.logf(LOG_DEBUG, "LOOKUPD(%s): sending heartbeat", lookupPeer) // 发送一个 PING 命令请求，利用 lookupPeer 的 Command 方法发送此命令请求， // 并读取响应，忽略响应（正常情况下 nsqlookupd 端的响应为 ok） cmd := nsq.Ping() _, err := lookupPeer.Command(cmd) // ... &#125; // 3. 收到 nsqd 的通知，即 nsqd.Notify 方法被调用， // 从 notifyChan 中取出对应的对象 channel 或 topic //（在 channel 或 topic 创建及退出/exit(Delete)会调用 nsqd.Notify 方法） case val := &lt;-n.notifyChan: var cmd *nsq.Command var branch string switch val.(type) &#123; // 3.1 若是 Channel，则通知所有的 nsqlookupd 有 channel 更新（新增或者移除） case *Channel: branch = "channel" channel := val.(*Channel) // 若 channel 已退出，即 channel被 Delete，则构造 UNREGISTER 命令请求 if channel.Exiting() == true &#123; cmd = nsq.UnRegister(channel.topicName, channel.name) &#125; else &#123; // 否则表明 channel 是新创建的，则构造 REGISTER 命令请求 cmd = nsq.Register(channel.topicName, channel.name) &#125; // 3.2 若是 Topic，则通知所有的 nsqlookupd 有 topic 更新（新增或者移除）， // 处理同 channel 类似 case *Topic: branch = "topic" topic := val.(*Topic) // 若 topic 已经退出，即 topic 被 Delete，则 nsqd 构造 UNREGISTER 命令请求 if topic.Exiting() == true &#123; cmd = nsq.UnRegister(topic.name, "") &#125; else &#123; // 若 topic 已经退出，即 topic 被 Delete，则 nsqd 构造 UNREGISTER 命令请求 cmd = nsq.Register(topic.name, "") &#125; &#125; // 3.3 遍历所有 nsqd 保存的 nsqlookupd 实例的地址信息 // 向每个 nsqlookupd 发送对应的 Command for _, lookupPeer := range lookupPeers &#123; n.logf(LOG_INFO, "LOOKUPD(%s): %s %s", lookupPeer, branch, cmd) // 这里忽略了返回的结果，nsqlookupd 返回的是 ok _, err := lookupPeer.Command(cmd) if err != nil &#123; n.logf(LOG_ERROR, "LOOKUPD(%s): %s - %s", lookupPeer, cmd, err) &#125; &#125; // 4. 若是 nsqlookupd 的地址变更消息，则重新从配置文件中加载 nsqlookupd 的配置信息 case &lt;-n.optsNotificationChan: var tmpPeers []*lookupPeer var tmpAddrs []string for _, lp := range lookupPeers &#123; if in(lp.addr, n.getOpts().NSQLookupdTCPAddresses) &#123; tmpPeers = append(tmpPeers, lp) tmpAddrs = append(tmpAddrs, lp.addr) continue &#125; n.logf(LOG_INFO, "LOOKUP(%s): removing peer", lp) lp.Close() &#125; lookupPeers = tmpPeers lookupAddrs = tmpAddrs connect = true // 5. nsqd 退出消息 case &lt;-n.exitChan: goto exit &#125; &#125;exit: n.logf(LOG_INFO, "LOOKUP: closing")&#125; // /nsq/nsqd/lookup.go 至此，NSQD.lookupLoop方法已经解析完毕。接下来，介绍nsqd网络连接tpc/http处理器的建立及注册。 nsqd 的 tcp &amp; http 连接处理器nsqd为客户端（包括生产者和消费者）建立的tcp/http连接的请求处理器的逻辑，和nsqlookupd为客户端（包括nsqd和消费者）建立的tcp/http连接请求处理器是类似的。因此这里不会详细阐述。可参考这里。另外，监听tcp连接请求的处理器（用于accpet连接）与nsqlookupd的建立的都是tcpServer，另外，当accpet到连接后，首先从连接中读取一个4byte的协议版本号，且目前源码中只支持V2，真正处理连接请求的方法为protocolV2.IOLoop。而http连接请求，则同样复用httprouter作为请求路由器。 tcp 连接处理器nsqd为每一个客户端都会异步开启一个protocolV2.IOLoop方法处理作为参数的连接上的请求。本文只是涉及到IOLoop方法的主体结构，而对不同请求的特定处理过程，则留待后文分析。因此，这要包括两个方面的处理逻辑，其一为IOLoop中的主循环，其负责等待请求并从连接上读取请求内容，并分析命令请求类型（典型包括PUB、SUB等），调用对应的请求处理函数。另一个异步循环为protocolV2.messagePump，此方法更为复杂，其核心逻辑为负责处理此nsqd所维护的channel发送消息的流程。下面分别进行介绍。 tcp 请求读取解析针对每个客户端，在IOLoop方法中，它首先创建此客户端在nsqd服务端所代表的通信实体clientV2(/nsq/nsqd/client_v2.go)，并将其添加到nsqd所维护的clients集合。然后通过messagePumpStartedChan同步等待messagePump先执行，之所以要先等待，原因是messagePump方法会从client获取的一些属性，因此需避免与执行IDENTIFY命令的client产生数据竞争，即在IOLoop方法后面可能修改相关的当前client的数据。而messagePump先执行的的部分为获取client的部分属性，然后通知IOLoop主循环可以继续执行。在IOLoop主循环中，其首先阻塞等待在客户端的连接上，需要注意的是，若客户端设置了心跳间隔(HeartbeatInterval)，则若间隔超过2*HeartbeatInterval未收到客户端的消息，则会关闭连接。相反，若未设置心跳间隔，则读取操作永不超时。当读取并解析请求内容后，会调用protocolV2.Exec方法来根据命令请求的类型来针对性处理。最后，将结果返回给客户端。相关代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 针对连接到 nsqd 的每一个 client，都会单独在一个 goroutine 中执行这样一个 IOLoop请求处理循环func (p *protocolV2) IOLoop(conn net.Conn) error &#123; var err error var line []byte var zeroTime time.Time clientID := atomic.AddInt64(&amp;p.ctx.nsqd.clientIDSequence, 1) client := newClientV2(clientID, conn, p.ctx) p.ctx.nsqd.AddClient(client.ID, client) // 1. 同步 messagePump 的启动过程，因为 messagePump会从client获取的一些属性 // 而避免与执行 IDENTIFY 命令的 client 产生数据竞争，即当前client在后面可能修改相关的数据 messagePumpStartedChan := make(chan bool) go p.messagePump(client, messagePumpStartedChan) &lt;-messagePumpStartedChan // 2. 开始循环读取 client 的请求，然后解析参数并处理 for &#123; // 如果在与客户端协商 negotiation过程中，客户端设置了 HeartbeatInterval， // 则在正常通信情况下，若间隔超过 2*HeartbeatInterval 未收到客户端的消息， // 则关闭连接。 if client.HeartbeatInterval &gt; 0 &#123; client.SetReadDeadline(time.Now().Add(client.HeartbeatInterval * 2)) &#125; else &#123; // 若客户端未设置 HeartbeatInterval，则读取等待不会超时。 client.SetReadDeadline(zeroTime) &#125; // 2.1 读取命令请求，并对它进行解析，解析命令的类型及参数 line, err = client.Reader.ReadSlice('\n') // .. line = line[:len(line)-1] if len(line) &gt; 0 &amp;&amp; line[len(line)-1] == '\r' &#123; line = line[:len(line)-1] &#125; params := bytes.Split(line, separatorBytes) p.ctx.nsqd.logf(LOG_DEBUG, "PROTOCOL(V2): [%s] %s", client, params) var response []byte // 2.2 执行命令 response, err = p.Exec(client, params) // .. // 2.3 返回命令处理结果 if response != nil &#123; err = p.Send(client, frameTypeResponse, response) // ... &#125; &#125; p.ctx.nsqd.logf(LOG_INFO, "PROTOCOL(V2): [%s] exiting ioloop", client) conn.Close() close(client.ExitChan) if client.Channel != nil &#123; client.Channel.RemoveClient(client.ID) &#125; p.ctx.nsqd.RemoveClient(client.ID) return err&#125; // /nsq/nsqd/protocol_v2.go 消息发送处理消息发送处理流程即为protocolV2.messagePump方法。此方法先获取客户端的一些属性信息，然后通知protocolV2.IOLoop主循环继续执行。接下来进入主循环，主循环主要包括两个部分的逻辑。 其一是更新memoryMsgChan、backendMsgChan和flusherChan通道，这三个channel在后面的作用至关重要，另外就是是否需要将发送给客户端的内容进行显式刷新（V2版本协议采用了选择性地将返回给client的数据进行缓冲，以通过减少系统调用频率来提高效率）。而memoryMsgChan和backendMsgChan都是nsqd所维护的channel的属性（不是拷贝），一个代表的是内存的消息队列，另一个代表的是持久化存储的消息队列，messagePump通过这从这两个channel中接收消息，以执行发送消息的逻辑，关于这两个channel会在分析channel时详细阐述。 其二，阻塞等待从各个channel通道中取出消息，通过一个select操作进行组织： flusherChan表示需要进行显式地刷新（是一个ticker）； 而ReadyStateChan则表示客户端的消息处理能力发生了变化，其主要与消息处理状态相关，而在这里并没有对应的后续处理； subEventChan通道起到传递另外两个关键的通道相关，当客户端发送了SUB命令请求时，即请求订阅某个topic的某个channel时，对应的channel实例会被压入到此通道中（SUB方法的逻辑）； 类似的，identifyEventChan通道起到传递一些由客户端设置的一些参数的作用，这些参数包括OutputBufferTimeout、HeartbeatInterval、SampleRate以及MsgTimeout，它们是在客户端发出IDENTIFY命令请求时，被压入到identifyEventChan管道的。其中OutputBufferTimeout用于构建flusherChan定时刷新发送给客户端的消息数据，而HeartbeatInterval用于定时向客户端发送心跳消息，SampleRate则用于确定此次从channel中取出的消息，是否应该发送给此客户端，还记得在第一篇文章中所提到的对于多个客户端连接到同一个channel的情形，channel会将topic发送给它的消息随机发送给其中一个客户端，此处就体现了随机负载。最后的MsgTimeout则用于设置消息投递并被处理的超时时间，最后会被设置成message.pri作为消息先后发送顺序的依据； heartbeatChan的作用就比较明显了，定时向客户端发送心跳消息，由HeartbeatInterval确定； backendMsgChan，它是channel实例的一个关键属性，表示channel实例维护的持久化存储中的消息队列，当channel所接收到的消息的长度超过内存消息队列长度时，则将消息压入到持久化存储中的消息队列backendMsgChan。因此，当从此通道中收到消息时，表明有channel实例有消息需要发送给此客户端。它首先通过生成一个0到100范围内的随机数，若此随机数小于SampleRate则此消息会发送给此客户端，反之亦然。并更新消息尝试发送的次数msg.Attempts。然后，调用channel实例的StartInFlightTimeout将消息压入到in-flight queue中（代表正在发送的消息队列），等待被queueScanWorker处理。接下来，更新为此客户端保存的关于消息的计数信息（比如增加正在发送消息的数量）。最后将消息通过网络发送出去，并更新flushed表示可能需要更新了； memoryMsgChan和backendMsgChan的作用非常类似，只不过它表示的是内存消息队列。处理过程也一样； client.ExitChan表示当客户端退出时，则对应的处理循环也需要退出。 这部分的逻辑较为复杂，希望上述的分析能够帮助读者理解，先看下相关代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175// nsqd 针对每一个 client 的订阅消息的处理循环func (p *protocolV2) messagePump(client *clientV2, startedChan chan bool) &#123; var err error var memoryMsgChan chan *Message var backendMsgChan chan []byte var subChannel *Channel var flusherChan &lt;-chan time.Time var sampleRate int32 // 1. 获取客户端的属性 subEventChan := client.SubEventChan identifyEventChan := client.IdentifyEventChan outputBufferTicker := time.NewTicker(client.OutputBufferTimeout) heartbeatTicker := time.NewTicker(client.HeartbeatInterval) heartbeatChan := heartbeatTicker.C msgTimeout := client.MsgTimeout // V2 版本的协议采用了选择性地将返回给 client 的数据进行缓冲，即通过减少系统调用频率来提高效率 // 只有在两种情况下才采取显式地刷新缓冲数据 // 1. 当 client 还未准备好接收数据。 // a. 若 client 所订阅的 channel 被 paused // b. client 的readyCount被设置为0， // c. readyCount小于当前正在发送的消息的数量 inFlightCount // 2. 当 channel 没有更多的消息给我发送了，在这种情况下，当前程序会阻塞在两个通道上 flushed := true // 2. 向 IOLoop goroutine 发送消息，可以继续运行 close(startedChan) for &#123;// 1. 当前 client 未准备好接消息，原因包括 subChannel为空，即此客户端未订阅任何 channel // 或者客户端还未准备好接收消息， // 即 ReadyCount(通过 RDY 命令设置) &lt;= InFlightCount \ // InFlightCount (已经给此客户端正在发送的消息的数量) 或 ReadyCount &lt;= 0 // 刚开始进入循环是肯定会执行这个分支 if subChannel == nil || !client.IsReadyForMessages() &#123; // the client is not ready to receive messages... // 初始化各个消息 channel memoryMsgChan = nil backendMsgChan = nil flusherChan = nil // 强制刷新缓冲区 client.writeLock.Lock() err = client.Flush() client.writeLock.Unlock() if err != nil &#123; goto exit &#125; flushed = true &#125; else if flushed &#123; // 2. 表明上一个循环中，我们已经显式地刷新过 // 准确而言，应该上从 client.SubEventChan 中接收到了 subChannel //（client订阅某个 channel 导致的） // 因此 初始化 memoryMsgChan 和 backendMsgChan 两个 channel， // 实际上这两个 channel 即为 client 所订阅的 channel的两个消息队列 memoryMsgChan = subChannel.memoryMsgChan backendMsgChan = subChannel.backend.ReadChan() // 同时，禁止从 flusherChan 取消息， // 因为才刚刚设置接收消息的 channel，缓冲区不会数据等待刷新 flusherChan = nil &#125; else &#123; // 3. 在执行到此之前，subChannel 肯定已经被设置过了， // 且已经从 memoryMsgChan 或 backendMsgChan 取出过消息 // 因此，可以准备刷新消息发送缓冲区了，即设置 flusherChan memoryMsgChan = subChannel.memoryMsgChan backendMsgChan = subChannel.backend.ReadChan() flusherChan = outputBufferTicker.C &#125; select &#123; // 4. 定时刷新消息发送缓冲区 case &lt;-flusherChan: client.writeLock.Lock() err = client.Flush() client.writeLock.Unlock() if err != nil &#123; goto exit &#125; flushed = true // 5. 客户端处理消息的能力发生了变化，比如客户端刚消费了某个消息 case &lt;-client.ReadyStateChan: // 6. 发现 client 订阅了某个 channel，channel 是在 SUB命令请求方法中被压入的 // 然后，将 subEventChan 重置为nil，重置为 nil原因表之后不能从此通道中接收到消息 // 而置为nil的原因是，在SUB命令请求方法中第一行即为检查此客户端是否处于 stateInit 状态， // 而调用 SUB 了之后，状态变为 stateSubscribed case subChannel = &lt;-subEventChan: // you can't SUB anymore subEventChan = nil // 7. 当 nsqd 收到 client 发送的 IDENTIFY 请求时，会设置此 client 的属性信息， // 然后将信息 push 到 identifyEventChan。 // 因此此处就会收到一条消息，同样将 identifyEventChan 重置为nil， // 这表明只能从 identifyEventChan 通道中接收一次消息，因为在一次连接过程中， // 只允许客户端初始化一次 // 在 IDENTIFY 命令处理请求中可看到在第一行时进行了检查， // 若此时客户端的状态不是 stateInit，则会报错。 // 最后，根据客户端设置的信息，更新部分属性，如心跳间隔 heartbeatTicker case identifyData := &lt;-identifyEventChan: // you can't IDENTIFY anymore identifyEventChan = nil outputBufferTicker.Stop() if identifyData.OutputBufferTimeout &gt; 0 &#123; outputBufferTicker = time.NewTicker(identifyData.OutputBufferTimeout) &#125; heartbeatTicker.Stop() heartbeatChan = nil if identifyData.HeartbeatInterval &gt; 0 &#123; heartbeatTicker = time.NewTicker(identifyData.HeartbeatInterval) heartbeatChan = heartbeatTicker.C &#125; if identifyData.SampleRate &gt; 0 &#123; sampleRate = identifyData.SampleRate &#125; msgTimeout = identifyData.MsgTimeout // 8. 定时向所连接的客户端发送 heartbeat 消息 case &lt;-heartbeatChan: err = p.Send(client, frameTypeResponse, heartbeatBytes) if err != nil &#123; goto exit &#125; // 9. 从 backendMsgChan 队列中收到了消息 case b := &lt;-backendMsgChan: // 根据 client 在与 nsqd 建立连接后，第一次 client 会向 nsqd 发送 IDENFITY 请求 \ // 以为 nsqd 提供 client 自身的信息。 // 即为 identifyData，而 sampleRate 就包含在其中。 // 换言之，客户端会发送一个 0-100 的数字给 nsqd。 // 在 nsqd 服务端，它通过从 0-100 之间随机生成一个数字， \ // 若其大于 客户端发送过来的数字 sampleRate \ // 则 client 虽然订阅了此 channel，且此 channel 中也有消息了， \ // 但是不会发送给此 client。 // 这里就体现了 官方文档 中所说的，当一个 channel 被 client 订阅时， \ // 它会将收到的消息随机发送给这一组 client 中的一个。 // 而且，就算只有一个 client，从程序中来看，也不一定能够获取到此消息， \ // 具体情况也与 client 编写的程序规则相关 if sampleRate &gt; 0 &amp;&amp; rand.Int31n(100) &gt; sampleRate &#123; continue &#125; // 将消息解码 msg, err := decodeMessage(b) if err != nil &#123; p.ctx.nsqd.logf(LOG_ERROR, "failed to decode message - %s", err) continue &#125; // 递增消息尝试发送次数 // 注意： 当消息发送的次数超过一定限制时，可由 client 自己在应用程序中做处理 msg.Attempts++ // 调用client 所订阅 channel 的 StartInFlightTimeout 方法，将消息压入发送队列 subChannel.StartInFlightTimeout(msg, client.ID, msgTimeout) // 更新client 的关于正在发送消息的属性 client.SendingMessage() // 正式发送消息到指定的 client err = p.SendMessage(client, msg) if err != nil &#123; goto exit &#125; // 重置 flused 变量 flushed = false // 9. 从 memoryMsgChan 队列中收到了消息 case msg := &lt;-memoryMsgChan: if sampleRate &gt; 0 &amp;&amp; rand.Int31n(100) &gt; sampleRate &#123; continue &#125; msg.Attempts++ subChannel.StartInFlightTimeout(msg, client.ID, msgTimeout) client.SendingMessage() err = p.SendMessage(client, msg) if err != nil &#123; goto exit &#125; flushed = false // 10. 客户端退出 case &lt;-client.ExitChan: goto exit &#125; &#125;exit: // ... heartbeatTicker.Stop() outputBufferTicker.Stop() // ...&#125; // /nsq/nsqd/protocol_v2.go 读者在理解这一段代码时，先可以看懂每一段代码的含义，然后进行一次“肉眼DEBUG”，即走一遍正常的代码处理流程（注意那些channel上的竞争条件）。这里，我简要阐述一下（for循环中的处理逻辑）： 首先刚开始肯定是进行第一个if执行，因为subChannel == nil且客户端也未准备好接收消息。注意此时会将各个channel，并刷新缓冲，而且将flushed设置为true； 然后，正常情况下，应该是identifyEventChan分支被触发，即客户端发送了IDENTIFY命令，此时，设置了部分channel。比如heartbeatChan，因此nsqd可以定期向客户端发送hearbeat消息了。并且heartbeatChan可能在下述的任何时刻触发，但都不影响程序核心执行逻辑； 此时，就算触发了heartbeatChan，上面的仍然执行第一个if分支，没有太多改变； 假如此时客户端发送了一个SUB请求，则此时subEventChan分支被触发，此时subChannel被设置，且subEventChan之后再也不能被触发。此时客户端的状态为stateSubscribed； 接下来，上面的代码执行的仍然是第一个if分支，因为此时subChannel != nil，但是客户端仍未准备好接收消息，即客户端的ReadyCount属性还未初始化； 按正常情况，此时客户端应该会发送RDY命令请求，设置自己的ReadyCount，即表示客户端能够处理消息的数量。 接下来，上面的代码总算可以执行第二个if分支，终于初始化了memoryMsgChan和backendMsgChan两个用于发送消息的消息队列了，同时将flusherChan设置为nil，显然，此时不需要刷新缓冲区； 此时，ReadyStateChan分支会被触发，因为客户端的消息处理能力确实发生了改化； 但ReadyStateChan分支的执行不影响上面代码中被触发的if分支，执行第二个分支。换言之，此时程序中涉及到的各属性没有发生变化； 现在，按正常情况终于要触发了memoryMsgChan分支，即有生产者向此channel所关联的topic投递了消息，因此nsqd将channel内存队列的消息发送给订阅了此channel的消费者。此时flushed为false； 接下来，按正常情况（假设客户端还可以继续消息消息，且消息消费未超时），上面代码应该执行第三个if分支，即设置两个消息队列，并设置flusherChan，因为此时确实可能需要刷新缓冲区了。 一旦触发了flusherChan分支，则flushed又被设置成true。表明暂时不需要刷新缓冲区，直到nsqd发送了消息给客户端，即触发了memoryMsgChan或backendMsgChan分支； 然后可能又进入第二个if分支，然后发送消息，刷新缓冲区，反复循环… 假如某个时刻，消费者的消息处理能力已经变为0了，则此时执行第一个if分支，两个消息队列被重置，执行强刷。显然，此时考虑到消费者已经不能再处理消息了，因此需要“关闭”消息发送的管道。 至此，nsqd其为客户端提供的tcp请求处理器相关的处理逻辑已经阐述完毕。内容比较多，因为笔者也阐述的比较详细，尽可能希望读者能够清晰整个流程。下面阐述nsqd为客户端提供的http请求处理器的相关逻辑。 http 连接处理器http连接处理器则相对简单很多，因为大部分内容已经由httprouter这个请求路由器完成了。我们简单看一下http handler的创建过程。同nsqlookupd创建http handler完全一样。首先设置了httprouter一些重要属性，然后构建httpServer实例，最后，调用router.Handle添加特定请求的处理器。关于具体请求的处理逻辑，后面会单开一篇文章来阐述。这里只涉及处理过程的框架。相关代码如下： 123456789101112131415161718192021222324252627282930313233// 同 nsqlookupd.httpServer 类似，参考 nsqlookupd/http.go 的源码注释type httpServer struct &#123; ctx *context tlsEnabled bool tlsRequired bool router http.Handler&#125;func newHTTPServer(ctx *context, tlsEnabled bool, tlsRequired bool) *httpServer &#123; log := http_api.Log(ctx.nsqd.logf) router := httprouter.New() router.HandleMethodNotAllowed = true router.PanicHandler = http_api.LogPanicHandler(ctx.nsqd.logf) router.NotFound = http_api.LogNotFoundHandler(ctx.nsqd.logf) router.MethodNotAllowed = http_api.LogMethodNotAllowedHandler(ctx.nsqd.logf) s := &amp;httpServer&#123; ctx: ctx, tlsEnabled: tlsEnabled, tlsRequired: tlsRequired, router: router, &#125; router.Handle("GET", "/ping", http_api.Decorate(s.pingHandler, log, http_api.PlainText)) router.Handle("GET", "/info", http_api.Decorate(s.doInfo, log, http_api.V1)) // v1 negotiate router.Handle("POST", "/pub", http_api.Decorate(s.doPUB, http_api.V1)) // only v1 router.Handle("POST", "/topic/create", http_api.Decorate(s.doCreateTopic, log, http_api.V1)) router.Handle("POST", "/channel/create", http_api.Decorate(s.doCreateChannel, log, http_api.V1)) // ... // debug router.HandlerFunc("GET", "/debug/pprof/", pprof.Index) // ... return s&#125; // /nsq/nsqd/http.go 至此，nsqd服务启动相关的源码已经解析完毕了。整个文章非常长，读者能够看到这里实属不易。希望看完全文读者能够有所收获，源码分析也并不难。 最后，简单小结，本文从五个方面对nsqd服务启动相关的流程进行分析。具体地，其一，先以nsqd命令为切入点，简述服务启动流程；其二，紧追nsqd启动流程，进一步分析初始化过程中NSQ的创建及初始化相关逻辑；接下来，详细阐述nsqd异步开启nsqlookupd查询过程；其四，详细阐述了nsqd和nsqlookupd交互的主循环的逻辑。即第四点和第五点阐述的是nsqd与nsqlookupd交互部分；最后，分析了nsqd建立tcp和http请求处理器相关逻辑。其中，重点分析了nsqd为客户端（生产者和消费者）建立的tcp请求处理器，主要包括两个大的方面：IOLoop主循环主要是读取连接请求，调用对应的处理函数处理请求。另一个则是messagePump方法，其包含了nsqd处理消息发送的核心逻辑——即nsqd所维护的channel将消息发送给各个订阅了它的客户端，其涉及到的流程最为复杂。更详细内容可以参考笔者简要注释的源码。 参考文献 [1]. https://github.com/nsqio/nsq[2]. https://nsq.io/overview/quick_start.html]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nsq nsqlookupd 源码简析]]></title>
    <url>%2F2019%2F05%2F12%2Fnsq-nsqlookupd-%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章简要介绍了nsq以及它的几个值得注意的特性，以对nsq有大体认识，并在整体上把握nsq的工作流程。这对于后面的源码分析工作会有帮助。本文重点阐述nsqlookup组件设计实现相关的源码。nsqlookupd是一个中心管理服务，负责管理集群(nsqd)的拓扑信息，并且为客户端（消费者）提供最终一致性的发现服务。具体而言，nsqlookupd用作解耦消费者和生产者(nsqd)。因为nsqd节点会将topic和channel的信息通过tcp广播到nsqlookupd节点（允许水平扩展）以实施服务注册，而客户端或nsqadmin通过http接口查询nsqlookupd来发现指定topic的生产者。引入nsqlookupd使得整个系统的模块更加清晰，且维护起来更加方便。值得注意的是，nsqlookupd本身对存储在其上的数据不做任何持久化。 考虑到nsqlookupd本身所提供的功能比较简单，代码结构并不复杂，因此以nsqlookupd为分析入口。个人建议，查看或分析源码最好从某个业务逻辑流程切入，这样更具针对性，忽略某些旁支或细节，先从宏观上把握整个流程。按照惯例，读者可以自己clone源码进行分析。本文分析nsqlookupd关键流程，较为完整的nsq源码注释可在这里找到，其注释源码版本为v1.1.0，仅供参考。 本文主要从三个方面来阐述nsqlookupd：其一，以nsqlookupd命令为切入点，介绍其启动流程；其二，通过启动流程，继续追溯到NSQLookupd的创建及初始化过程。最后，阐述初始化过程中tcp和http请求处理器相关逻辑，并示例分析几个典型请求的详细处理逻辑，比如，nsqd通过tcp协议订阅topic。另外，nsqd通过http协议请求nsqlookupd执行topic的创建过程，以及客户端（消费者）请求nsqlookupd执行topic的查询过程。本文所涉及到源码主要为/nsq/apps/nsqlookupd/、/nsq/nsqlookupd/和/nsq/internal/下的若干子目录，/nsq/apps目录是官方提供的一些工具包，而/nsq/nsqlookupd会对应具体的实现，/nsq/internal则为nsq内部的核心（公共）库，目录结构比较简单，不多阐述。 当我们在命令行执行nsqlookupd命令时（同时可指定参数），相当于运行了nsq/apps/nsqlookupd程序的main方法。此方法启动了一个进程（服务），并且通过创建NSQLookupd并调用其Main方法执行启动逻辑。 利用 svc 启动进程具体而言，其利用 svc的Run方法启动一个进程（守护进程或服务），在 svc.Run 方法中依次调用 Init 和 Start 方法，Init 和 Start 方法都是 no-blocking的；Run 方法会阻塞直到接收到 SIGINT(程序终止信号，如ctrl+c)或SIGTERM（程序结束信号，如kill -15 PID），然后调用 stop方法后退出，这通过传递一个channel及感兴趣的信号集(SIGINT&amp;SGITERM)给 signal.Notify 方法实现；Run方法中阻塞等待从channel中接收消息，一旦收到消息，则调用stop方法返回，进程退出。更多可以查看 golang 标准包的signal.Notify以及svc包是如何协助启动一个进程。相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869type program struct &#123; // 代表此进程结构，包装了一个 nsqlookupd 实例 once sync.Once nsqlookupd *nsqlookupd.NSQLookupd&#125;// nsqlookupd 服务程序执行入口func main() prg := &amp;program&#123;&#125; // 1. 利用 svc 启动一个进程，在 svc.Run 方法中会依次调用 Init 和 Start 方法，Init 和 Start 方法都是 no-blocking的； // 2. Run 方法会阻塞直到接收到 SIGINT(程序终止)或SIGTERM(程序结束信号)，然后调用stop方法后退出； // 3. 这是通过传递一个 channel 及感兴趣信号集(SIGINT&amp;SGITERM)给 signal.Notify 方法实现； // 4. Run方法中阻塞等待从 channel 中接收消息，一旦收到消息，则调用 stop 方法返回，进程退出。 if err := svc.Run(prg, syscall.SIGINT, syscall.SIGTERM); err != nil &#123; logFatal("%s", err) &#125;&#125;func (p *program) Init(env svc.Environment) error &#123;// 初始化函数没有做实质性工作 if env.IsWindowsService() &#123; dir := filepath.Dir(os.Args[0]) return os.Chdir(dir) &#125; return nil&#125;// Start 方法包含进程启动的主要执行逻辑func (p *program) Start() error &#123; // 1. 默认初始化 nsqlookupd 配置参数 opts := nsqlookupd.NewOptions() // 2. 根据命令行传递的参数更新默认参数 flagSet := nsqlookupdFlagSet(opts) flagSet.Parse(os.Args[1:]) // 3. 输出版本号并退出 if flagSet.Lookup("version").Value.(flag.Getter).Get().(bool) &#123; fmt.Println(version.String("nsqlookupd")) os.Exit(0) &#125; // 4. 解析配置文件获取用户设置参数 var cfg map[string]interface&#123;&#125; configFile := flagSet.Lookup("config").Value.String() if configFile != "" &#123; _, err := toml.DecodeFile(configFile, &amp;cfg) if err != nil &#123; logFatal("failed to load config file %s - %s", configFile, err) &#125; &#125; options.Resolve(opts, flagSet, cfg) // 5. 合并默认参数及配置文件中的参数 nsqlookupd, err := nsqlookupd.New(opts) // 6. 创建 nsqlookupd 进程 if err != nil &#123; logFatal("failed to instantiate nsqlookupd", err) &#125; p.nsqlookupd = nsqlookupd go func() &#123; // 7. 执行 nsqlookupd 的主函数 err := p.nsqlookupd.Main() if err != nil &#123; p.Stop() os.Exit(1) &#125; &#125;() return nil&#125;// 进程退出方法，注意使用 sync.Once 来保证 nsqlookupd.Exit 方法只被执行一次func (p *program) Stop() error &#123; p.once.Do(func() &#123; p.nsqlookupd.Exit() &#125;) return nil&#125; // /nsq/apps/nsqlookupd.go 123456789101112131415161718// 使用 Run 方法来开启一个进程func Run(service Service, sig ...os.Signal) error &#123; env := environment&#123;&#125; if err := service.Init(env); err != nil &#123; // 1.初始化环境，没什么实质性内容 return err &#125; if err := service.Start(); err != nil &#123; // 2. 调用上面的 Start 方法来执行进程启动逻辑 return err &#125; if len(sig) == 0 &#123;// 3. 默认响应 SIGINT和SIGTERM信号 sig = []os.Signal&#123;syscall.SIGINT, syscall.SIGTERM&#125; &#125; signalChan := make(chan os.Signal, 1) // 4. 调用 signalNotify 方法，使用阻塞等待信号产生 signalNotify(signalChan, sig...) &lt;-signalChan // 5. 在进程退出之前，调用 Stop 方法做清理工作 return service.Stop()&#125; // /svc/svc/svc_other.go 代码中注释已经介绍得比较清晰，这里简单阐述下Start方法的逻辑：首先会通过默认参数创建配置参数实例opts，然后合并命令行参数及配置文件参数（若存在），接下来创建nsqlookupd实例，并调用nsqlookupd.Main函数，这是最关键的步骤。下面展开分析。 NSQLookupd 启动初始化在介绍构造方法前，先贴出NSQLookupd结构： 123456789type NSQLookupd struct &#123; sync.RWMutex // 读写锁 opts *Options // 参数配置信息 tcpListener net.Listener // tcp 监听器用于监听 tcp 连接 httpListener net.Listener // 监听 http 连接 // sync.WaitGroup 增强体，功能类似于 sync.WaitGroup，一般用于等待所有的 goroutine 全部退出 waitGroup util.WaitGroupWrapper DB *RegistrationDB // 生产者注册信息(topic、channel及producer) DB&#125; 构造方法中并没有太多逻辑，启用了日志输出，并构建NSQLookupd结构实例，然后开启了tcp和http连接的监听。代码如下所示（省去了一些错误处理及其它无关紧要的代码片段）： 123456789101112131415161718192021// 创建 NSQLookupd 实例func New(opts *Options) (*NSQLookupd, error) &#123; var err error // 1. 启用日志输出 if opts.Logger == nil &#123; opts.Logger = log.New(os.Stderr, opts.LogPrefix, log.Ldate|log.Ltime|log.Lmicroseconds) &#125; // 2. 创建 NSQLookupd 实例 l := &amp;NSQLookupd&#123; opts: opts, // 配置参数实例 DB: NewRegistrationDB(), // topic、channel及producer的存储，一个 map 实例 &#125; // 3. 版本号等信息 l.logf(LOG_INFO, version.String("nsqlookupd")) // 4. 开启 tcp 和 http 连接监听 l.tcpListener, err = net.Listen("tcp", opts.TCPAddress) // ... l.httpListener, err = net.Listen("tcp", opts.HTTPAddress) // ... return l, nil&#125; 下面重点介绍其Main启动方法，即启动一个NSQLookupd实例，具体逻辑比较简单：构建了一个Context实例，它纯粹只是一个NSQLookupd实例的wrapper。然后注册了进程退出前需要执行的hook函数。关键步骤为创建用于处理tcp和http连接的handler，同时异步开启tcp和http连接的监听动作，最后通过一个channel阻塞等待方法退出，详细代码逻辑如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243// 启动 NSQLookupd 实例func (l *NSQLookupd) Main() error &#123; // 1. 构建 Context 实例， Context 是 NSQLookupd 的一个 wrapper ctx := &amp;Context&#123;l&#125; // 2. 创建进程退出前需要执行的 hook 函数 exitCh := make(chan error) var once sync.Once exitFunc := func(err error) &#123; once.Do(func() &#123; if err != nil &#123; l.logf(LOG_FATAL, "%s", err) &#125; exitCh &lt;- err &#125;) &#125; // 3. 创建用于处理 tcp 连接的 handler，并开启 tcp 连接的监听动作 // tcp协议处理函数其实是 LookupProtocolV1.IOLoop, 它支持 IDENTIFY、REGISTER及UNREGISTER等命令请求的处理 tcpServer := &amp;tcpServer&#123;ctx: ctx&#125; l.waitGroup.Wrap(func() &#123; // 3.1 在 protocol.TCPServer 方法中统一处理监听 exitFunc(protocol.TCPServer(l.tcpListener, tcpServer, l.logf)) &#125;) // 4. 创建用于处理 http 连接的 handler，并开启 http 连接的监听动作 // 而 http连接处理，它利用了 httpServer，一个高效的请求路由库 httpServer := newHTTPServer(ctx) l.waitGroup.Wrap(func() &#123; exitFunc(http_api.Serve(l.httpListener, httpServer, "HTTP", l.logf)) &#125;) // 5. 阻塞等待错误退出 err := &lt;-exitCh return err&#125;// NSQLookupd 服务退出方法中，关闭了网络连接，并且需等待 hook 函数被执行func (l *NSQLookupd) Exit() &#123; if l.tcpListener != nil &#123; l.tcpListener.Close() &#125; if l.httpListener != nil &#123; l.httpListener.Close() &#125; l.waitGroup.Wait()&#125; // /nsq/nsqlookupd/nsqlookupd.go tcp &amp; http 请求处理Main方法中关键代码为构建tcp及http请求的handler，并异步调用它们以处理请求。 客户端 tcp 请求处理其中tcp请求的handler比较简单：直接使用标准库的tcp相关函数，在一个单独的goroutine中启用了tcp handler。对于监听到每一个连接，开启一个额外的goroutine来处理请求。相关代码如下： 12345678910111213141516// tcp 连接处理器，只是一个统一的入口，当 accept 到一个连接后，将此连接交给对应的 handler 处理type TCPHandler interface &#123; Handle(net.Conn)&#125;func TCPServer(listener net.Listener, handler TCPHandler, logf lg.AppLogFunc) error &#123; logf(lg.INFO, "TCP: listening on %s", listener.Addr()) for &#123; clientConn, err := listener.Accept() // ... // 针对每一个连接到 nsqd 的 client，会单独开启一个 goroutine 去处理 // 实际上是由 /nsq/nsqlookupd/tcp.go 中的 tcpServer.Handle 方法处理 go handler.Handle(clientConn) &#125; logf(lg.INFO, "TCP: closing %s", listener.Addr()) return nil&#125; // /nsq/internal/protocol/tcp_server.go 对于accpet到的每一个连接，都交给了tcpServer.Handle方法异步处理。需要注意的是tcpServer.Handle只是对连接进行初步处理，不涉及到具体的业务逻辑。其主要是验证客户端使用的协议版本，然后就调用lookup_protocol_v1.go中的LookupProtocolV1.IOLoop方法处理。相关代码如下： 123456789101112131415161718192021222324252627282930// tcp 连接 handler。 Context/NSQLookupd 的一个 wrappertype tcpServer struct &#123; ctx *Context&#125;func (p *tcpServer) Handle(clientConn net.Conn) &#123; p.ctx.nsqlookupd.logf(LOG_INFO, "TCP: new client(%s)", clientConn.RemoteAddr()) // 在 client 同 NSQLookupd 正式通信前，需要发送一个 4byte 的序列号，以商定协议版本 buf := make([]byte, 4) _, err := io.ReadFull(clientConn, buf) // ... protocolMagic := string(buf) p.ctx.nsqlookupd.logf(LOG_INFO, "CLIENT(%s): desired protocol magic '%s'", clientConn.RemoteAddr(), protocolMagic) var prot protocol.Protocol switch protocolMagic &#123; // 构建 LookupProtocolV1 来真正处理连接的业务请求 case " V1": prot = &amp;LookupProtocolV1&#123;ctx: p.ctx&#125; default: // 只支持V1版本，否则发送 E_BAD_PROTOCOL，关闭连接 protocol.SendResponse(clientConn, []byte("E_BAD_PROTOCOL")) clientConn.Close() // ... return &#125; // 调用 prot.IOLoop 方法循环处理指定连接的请求 err = prot.IOLoop(clientConn) // ...&#125; 在LookupProtocolV1.IOLoop方法中，它开启了一个循环，为每一个连接创建对应的client（/nsq/nsqlookupd/client_v1的ClientV1）实例，然后读取请求内容，解析请求参数，并调用Exec方法执行请求，最后将结果返回，而在连接关闭时，它会清除client（其实代指的是nsqd实例）在NSQLookupd注册的信息（包括topic、channel和producer等）。相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445// LookupProtocolV1： Context/NSQLookupd 的一个 wrapper。是 protocol.Protocol 的一个实现。// nsqd 使用 tcp 接口来广播type LookupProtocolV1 struct &#123; ctx *Context&#125;func (p *LookupProtocolV1) IOLoop(conn net.Conn) error &#123; var err error var line string // 1. 先创建客户端实例，并构建对应的 reader client := NewClientV1(conn) reader := bufio.NewReader(client) for &#123; // 2. 读取一行内容，并分离出参数信息 line, err = reader.ReadString('\n') // ... line = strings.TrimSpace(line) params := strings.Split(line, " ") // 3. 调用 Exec 方法获取响应内容 var response []byte response, err = p.Exec(client, reader, params) // 4. Exec 方法执行失败，返回对应的异常信息 // ... // 5. 执行成功，则返回响应内容 if response != nil &#123; _, err = protocol.SendResponse(client, response) // ... &#125; &#125; // 6. 连接关闭时，清除 client(nsqd) 在 NSQLookupd 注册的信息 conn.Close() p.ctx.nsqlookupd.logf(LOG_INFO, "CLIENT(%s): closing", client) if client.peerInfo != nil &#123; registrations := p.ctx.nsqlookupd.DB.LookupRegistrations(client.peerInfo.id) for _, r := range registrations &#123; if removed, _ := p.ctx.nsqlookupd.DB.RemoveProducer( r, client.peerInfo.id); removed &#123; p.ctx.nsqlookupd.logf(LOG_INFO, "DB: client(%s) UNREGISTER category:%s key:%s subkey:%s", client, r.Category, r.Key, r.SubKey) &#125; &#125; &#125; return err&#125; 先了解Exec如何处理请求的。其实比较简单，对于通过tcp连接所发送请求，只支持PING、IDENTIFY、REGISTER和UNREGISTER这四种类型。针对每一种类型的请求，分别调用它们所关联的请求处理函数。如下： 123456789101112131415func (p *LookupProtocolV1) Exec(client *ClientV1, reader *bufio.Reader, params []string) ([]byte, error) &#123; switch params[0] &#123; case "PING": return p.PING(client, params) case "IDENTIFY": return p.IDENTIFY(client, reader, params[1:]) case "REGISTER": return p.REGISTER(client, reader, params[1:]) case "UNREGISTER": return p.UNREGISTER(client, reader, params[1:]) &#125; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", fmt.Sprintf("invalid command %s", params[0]))&#125; 在介绍具体的请求处理逻辑前，先介绍一下在构建NSQLookupd实例时，构建的RegistrationDB实例，因为它代表了客户端往nsqlookupd所注册信息的存储或容器。其实质上是一个map结构，其中key为Registration，值为ProduceMap(map[string]*Producer)。且Registration主要包含了topic和channel的信息，而ProduceMap则包含了生产者(nsqd)的信息。因此，围绕RegistrationDB的操作也比较简单，即对相关的数据的CRUD操作。相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// NSQLookupd 的注册信息DB，即为 nqsd 的注册信息type RegistrationDB struct &#123; sync.RWMutex // guards registrationMap registrationMap map[Registration]ProducerMap&#125;type Registration struct &#123; Category string // client|channel|topic Key string // topic SubKey string // channel&#125;type Registrations []Registration// PeerInfo 封装了 client/sqsd 中与网络通信相关的字段，即client 在 NSQLookupd 端的逻辑视图type PeerInfo struct &#123; lastUpdate int64 // nsqd 上一次向 NSQLookupd 发送心跳的 timestamp id string // nsqd 实例的 id RemoteAddress string `json:"remote_address"` // ip 地址 Hostname string `json:"hostname"` // 主机名 BroadcastAddress string `json:"broadcast_address"` // 广播地址 TCPPort int `json:"tcp_port"` // TCP 端口 HTTPPort int `json:"http_port"` // http 端口 Version string `json:"version"` // nsqd 版本号&#125;type Producer struct &#123; peerInfo *PeerInfo // client/nsqd 的 PeerInfo tombstoned bool // 标记 nsqd 是否被逻辑删除 tombstonedAt time.Time // 若被逻辑删除，则记录时间戳&#125;type Producers []*Producertype ProducerMap map[string]*Producer// /nsq/nsqlookupd/registration_db.go// 当 nsqd 创建topic或channel时，需要将其注册到 NSQLookupd 的 DB 中func (r *RegistrationDB) AddRegistration(k Registration) // 添加一个 Producer 到 registration 集合中，返回此 Producer 之前是否已注册func (r *RegistrationDB) AddProducer(k Registration, p *Producer) bool // 从 Registration 对应的 ProducerMap 移除指定的 client/peerfunc (r *RegistrationDB) RemoveProducer(k Registration, id string) (bool, int) // 删除 DB 中指定的 Registration 实例（若此 channel 为 ephemeral，// 则当其对应的 producer/client 集合为空时，会被移除）所对应的 producerMapfunc (r *RegistrationDB) RemoveRegistration(k Registration)// 根据 category、key和 subkey 来查找 Registration 集合。注意 key 或 subkey 中可能包含 通配符*func (r *RegistrationDB) FindRegistrations(category string, key string, subkey string) Registrations // 根据 category、key和 subkey 来查找 Producer 集合。注意 key 或 subkey 中可能包含 通配符*func (r *RegistrationDB) FindProducers(category string, key string, subkey string) Producers// 根据 peer id 来查找 Registration 集合。func (r *RegistrationDB) LookupRegistrations(id string) Registrations // /nsq/nsqlookupd/registration_db.go 接下来具体介绍各具体的命令请求是如下处理的，其中PING命令用于维持nsqd与nsqlookupd实例之间的连接通信，其处理也比较简单，更新一下此客户端的活跃时间lastUpdate，并回复OK。当我们使用nsqd --lookupd-tcp-address=127.0.0.1:4160启动一个nsqd实例时，它会在它的Main方法中使用一个额外的goroutine来开启lookupd扫描。当第一次执行时，它会向它知道的nsqlookupd地址（通过配置文件或命令行指定）建立连接。当nsqd与nsqlookupd连接建立成功后，会向nsqlookupd发送一个MagicV1的命令请求以校验目前自己所使用的协议版本，然后，会向nsqlookupd发送一个IDENTIFY命令请求，以认证自己身份，在此处理方法中会将客户端构造成Producer添加到RegistrationDB，并且返回自己的一些信息，当nsqd收到这些信息后，会遍历自己所有的topic，针对每一个topic，若其没有关联的channel，则发送只包含topic的REGISTER命令请求，否则还会遍历topic所关联的channel集合，针对每一个channel，发送一个包含topic和channel的REGISTER命令。所谓的REGISTER命令请求表示nsqd向 nsqlookupd 发送注册 topic的请求，当nsqlookupd收到REGISTER命令请求时，且若消息中带有channel时，会为此客户端会注册两个producer，即分别针对channel和topic构建。注意，在这里个人对nsqd启动后与nsqdlookupd建立连接以及REGISTER的过程阐述得比较详细，希望读者能够对一个二者的交互有一个全局的把握，但这里面涉及到nsqd启动的过程，会在后续的文章中详细阐述。而各命令请求处理逻辑则比较简单： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107// PING 消息： client 在发送其它命令之前，可能会先发送一个 PING 消息。func (p *LookupProtocolV1) PING(client *ClientV1, params []string) ([]byte, error) &#123; if client.peerInfo != nil &#123; // we could get a PING before other commands on the same client connection cur := time.Unix(0, atomic.LoadInt64(&amp;client.peerInfo.lastUpdate)) now := time.Now() // 打印 PING 日志 p.ctx.nsqlookupd.logf(LOG_INFO, "CLIENT(%s): pinged (last ping %s)", client.peerInfo.id, now.Sub(cur)) // 更新上一次PING的时间 atomic.StoreInt64(&amp;client.peerInfo.lastUpdate, now.UnixNano()) &#125; // 回复 OK return []byte("OK"), nil&#125; // /nsq/nsqlookup/lookup_protocol_v1.go// client 向 NSQLookupd 发送认证身份的消息。 注意在此过程中会将客户端构造成Producer添加到 Registration DB中。func (p *LookupProtocolV1) IDENTIFY(client *ClientV1, reader *bufio.Reader, params []string) ([]byte, error) &#123; var err error // 1. client 不能重复发送 IDENTIFY 消息 if client.peerInfo != nil &#123; return nil, protocol.NewFatalClientErr(err, "E_INVALID", "cannot IDENTIFY again") &#125; // 2. 读取消息体的长度 var bodyLen int32 err = binary.Read(reader, binary.BigEndian, &amp;bodyLen) // ... // 3. 读取消息体内容，包含生产者的信息 body := make([]byte, bodyLen) _, err = io.ReadFull(reader, body) // ... // 4. 根据消息体构建 PeerInfo 实例 peerInfo := PeerInfo&#123;id: client.RemoteAddr().String()&#125; err = json.Unmarshal(body, &amp;peerInfo) // ... peerInfo.RemoteAddress = client.RemoteAddr().String() // 5. 检验属性不能为空，同时更新上一次PING的时间 if peerInfo.BroadcastAddress == "" || peerInfo.TCPPort == 0 || peerInfo.HTTPPort == 0 || peerInfo.Version == "" &#123; return nil, protocol.NewFatalClientErr(nil, "E_BAD_BODY", "IDENTIFY missing fields") &#125; atomic.StoreInt64(&amp;peerInfo.lastUpdate, time.Now().UnixNano()) // 6. 将此 client 构建成一个 Producer 注册到 DB中 client.peerInfo = &amp;peerInfo if p.ctx.nsqlookupd.DB.AddProducer(Registration&#123;"client", "", ""&#125;, &amp;Producer&#123;peerInfo: client.peerInfo&#125;) &#123; p.ctx.nsqlookupd.logf(LOG_INFO, "DB: client(%s) REGISTER category:%s key:%s subkey:%s", client, "client", "", "") &#125; // 7. 构建响应消息，包含 NSQLookupd 的 hostname、port及 version data := make(map[string]interface&#123;&#125;) data["tcp_port"] = p.ctx.nsqlookupd.RealTCPAddr().Port data["http_port"] = p.ctx.nsqlookupd.RealHTTPAddr().Port data["version"] = version.Binary hostname, err := os.Hostname() if err != nil &#123; log.Fatalf("ERROR: unable to get hostname %s", err) &#125; data["broadcast_address"] = p.ctx.nsqlookupd.opts.BroadcastAddress data["hostname"] = hostname response, err := json.Marshal(data) // ... return response, nil&#125; // /nsq/nsqlookup/lookup_protocol_v1.go// Client 向 NSQLookupd 发送取消注册/订阅 topic 的消息。即为 REGISTER 的逆过程func (p *LookupProtocolV1) UNREGISTER(client *ClientV1, reader *bufio.Reader, params []string) ([]byte, error) &#123; // 1. 必须先要发送 IDENTIFY 消息进行身份认证 if client.peerInfo == nil &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "client must IDENTIFY") &#125; // 2. 获取 client 注册的 topic 和 channel(若有的话) topic, channel, err := getTopicChan("UNREGISTER", params) // ... // 3. 若 channel 不为空，则在 DB 中移除一个 Producer 实例， // 其键(Category)为 channel 类型的 Registration。 if channel != "" &#123; key := Registration&#123;"channel", topic, channel&#125; removed, left := p.ctx.nsqlookupd.DB.RemoveProducer(key, client.peerInfo.id) // ... // 对于 ephemeral 类型的 channel， // 若它未被任何 Producer 订阅，则需要移除此 channel 代表的 Registration 对象 // for ephemeral channels, remove the channel as well if it has no producers if left == 0 &amp;&amp; strings.HasSuffix(channel, "#ephemeral") &#123; p.ctx.nsqlookupd.DB.RemoveRegistration(key) &#125; &#125; else &#123; // 4. 取消注册 topic。因此它会删除掉 类型(Category)为 channel // 且 Key 为 topic 且 subKey不限的 Registration 集合； // 也会删除 Category 为 topic 且 Key 为 topic且 subKey为""的 Registration集合 registrations := p.ctx.nsqlookupd.DB.FindRegistrations("channel", topic, "*") for _, r := range registrations &#123; removed, _ := p.ctx.nsqlookupd.DB.RemoveProducer(r, client.peerInfo.id) // ... key := Registration&#123;"topic", topic, ""&#125; removed, left := p.ctx.nsqlookupd.DB.RemoveProducer(key, client.peerInfo.id) // ... // 同样，对于 ephemeral 类型的 topic，若它没有被任何 Producer 订阅， // 则需要移除此 channel 代表的 Registration 对象。 if left == 0 &amp;&amp; strings.HasSuffix(topic, "#ephemeral") &#123; p.ctx.nsqlookupd.DB.RemoveRegistration(key) &#125; &#125; return []byte("OK"), nil&#125; // /nsq/nsqlookup/lookup_protocol_v1.go tcp 请求 REGISTER 处理过程这里简要阐述REGISTER请求处理的过程：当nsqlookupd收到REGISTER命令请求后，它首先确认对方是否已经发送过IDENTIFY命令请求，确认完成后，解析请求中的topic名称和channel名称，然后，进一步检查topic和channel命名的合法性，最后若channel不为空，则向RegistrationDB中添加一个Producer 实例，其Category为channel类型的Registration。同样，若topic不为空，则还需要向RegistrationDB中添加一个Producer实例，其Category为topic 类型的 Registration。最后返回OK。 123456789101112131415161718192021222324252627282930// Client 向 NSQLookupd 发送注册 topic 的消息。注意，当消息中带有 channel 时，// 对于此 client会注册两个 producer，分别针对 channel 和 topicfunc (p *LookupProtocolV1) REGISTER(client *ClientV1, reader *bufio.Reader, params []string) ([]byte, error) &#123; // 1. 必须先要发送 IDENTIFY 消息进行身份认证。 if client.peerInfo == nil &#123; return nil, protocol.NewFatalClientErr(nil, "E_INVALID", "client must IDENTIFY") &#125; // 2. 获取 client 注册的 topic 和 channel(若有的话) topic, channel, err := getTopicChan("REGISTER", params) // ... // 3. 若 channel 不为空，则向 DB 中添加一个 Producer 实例，其键(Category)为 channel 类型的 Registration if channel != "" &#123; key := Registration&#123;"channel", topic, channel&#125; if p.ctx.nsqlookupd.DB.AddProducer(key, &amp;Producer&#123;peerInfo: client.peerInfo&#125;) &#123; p.ctx.nsqlookupd.logf( LOG_INFO, "DB: client(%s) REGISTER category:%s key:%s subkey:%s", client, "channel", topic, channel) &#125; &#125; // 4. 若 topic 不为空，则还需要向 DB 中添加一个 Producer 实例，其键(Category)为 topic 类型的 Registration key := Registration&#123;"topic", topic, ""&#125; if p.ctx.nsqlookupd.DB.AddProducer(key, &amp;Producer&#123;peerInfo: client.peerInfo&#125;) &#123; p.ctx.nsqlookupd.logf( LOG_INFO, "DB: client(%s) REGISTER category:%s key:%s subkey:%s", client, "topic", topic, "") &#125; // 5. 返回 OK return []byte("OK"), nil&#125; // /nsq/nsqlookup/lookup_protocol_v1.go 需要注意的是，这几个接口都是tcp连接请求的对应的处理函数，并非是http请求（可以通过命令行的方式发起）所对应的处理函数。nsq官方提供了一个go-nsq的客户端库。我们可以通过这个库来显式调试这些命令请求处理函数，当然源码包下也有对应的测试文件/nsq/nsqlookupd/nsqlookupd_test.go。到此为止通过tcp协议发起的请求的处理逻辑已经阐述完毕。下面介绍http协议的请求处理逻辑。 客户端 http 请求处理前面提到，在NSQLookupd.Main方法中，同样创建了一个http请求处理器httpServer，并设置了请求的监听器http_api.Serve。它们的功能及用法可以参考这里。我们先来简单了解http请求的监听器是怎样工作的。很简单，它同样使用的是标准库中的http相关的接口，即调用server.Serve函数监听连接请求，但其采用的是自定义的handler，即前方所提到的httprouter来作为请求路由处理器。相关代码如下： 123456789101112// http 连接处理器，类似于 tcp_server 只是一个统一的入口，具体监听动作是在标准包 http.Server.Serve 方法中完成func Serve(listener net.Listener, handler http.Handler, proto string, logf lg.AppLogFunc) error &#123; logf(lg.INFO, "%s: listening on %s", proto, listener.Addr()) server := &amp;http.Server&#123; Handler: handler, ErrorLog: log.New(logWriter&#123;logf&#125;, "", 0), &#125; err := server.Serve(listener) // ... logf(lg.INFO, "%s: closing %s", proto, listener.Addr()) return nil&#125; 而http请求处理的重点在于http请求的路由器，即/nsq/nsqlookupd/http.go中所定义的httpServer，其仅仅是httprouter的一个wrapper。关于httprouter具体工作原理，读者可以阅读源码（比较短）或参考其它文章。这里简要介绍一下httpServer实例化的过程，首先会创建httprouter实例，然后设置参数信息，比如对于403、404和500等错误的handler。接下来，构建httpServer实例，然后通过httprouter实例来添加特定的路由规则。各具体的请求处理器也比较简单，纯粹就调用RegistrationDB相关接口，不多阐述，读者可深入源码查看。最后，值得学习的是，程度采用装饰者模式构建强大且灵活的请求处理器。相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596// http 连接的 handler。// 客户端（消费者）使用这些 http 接口来发现和管理。// 实现了 http.Handler 接口，实现了 ServeHTTP(ResponseWriter, *Request) 处理函数type httpServer struct &#123; ctx *Context router http.Handler&#125;// http 请求处理器构造函数func newHTTPServer(ctx *Context) *httpServer &#123; log := http_api.Log(ctx.nsqlookupd.logf) // 1. 创建 httprouter 实例。httprouter是一个高效的请求路由器，使用了一个后缀树来存储路由信息。 // 更多 https://github.com/julienschmidt/httprouter 或参考博客 https://learnku.com/articles/27591 router := httprouter.New() // 2. 配置参数信息 router.HandleMethodNotAllowed = true router.PanicHandler = http_api.LogPanicHandler(ctx.nsqlookupd.logf) router.NotFound = http_api.LogNotFoundHandler(ctx.nsqlookupd.logf) router.MethodNotAllowed = http_api.LogMethodNotAllowedHandler(ctx.nsqlookupd.logf) // 3. 对此 httprouter 进行包装，构建 httpServer 实例 s := &amp;httpServer&#123; ctx: ctx, router: router, &#125; // 4. 为 httprouter 实例添加特定路由规则 // 对于 PING 请求而言，其 handler 为调用 Decorate 后的返回值， // 以 pingHandler 作为被装饰函数，可变的装饰参数列表为 log, http_api.PlainText（用于返回纯文本内容） // 后面的程序结构类似 router.Handle("GET", "/ping", http_api.Decorate(s.pingHandler, log, http_api.PlainText)) router.Handle("GET", "/info", http_api.Decorate(s.doInfo, log, http_api.V1)) // v1 negotiate router.Handle("GET", "/debug", http_api.Decorate(s.doDebug, log, http_api.V1)) router.Handle("GET", "/lookup", http_api.Decorate(s.doLookup, log, http_api.V1)) router.Handle("GET", "/topics", http_api.Decorate(s.doTopics, log, http_api.V1)) router.Handle("GET", "/channels", http_api.Decorate(s.doChannels, log, http_api.V1)) router.Handle("GET", "/nodes", http_api.Decorate(s.doNodes, log, http_api.V1)) // only v1 router.Handle("POST", "/topic/create", http_api.Decorate(s.doCreateTopic, log, http_api.V1)) router.Handle("POST", "/topic/delete", http_api.Decorate(s.doDeleteTopic, log, http_api.V1)) router.Handle("POST", "/channel/create", http_api.Decorate(s.doCreateChannel, log, http_api.V1)) router.Handle("POST", "/channel/delete", http_api.Decorate(s.doDeleteChannel, log, http_api.V1)) router.Handle("POST", "/topic/tombstone", http_api.Decorate(s.doTombstoneTopicProducer, log, http_api.V1)) // debug router.HandlerFunc("GET", "/debug/pprof", pprof.Index) // ... return s&#125; // /nsq/nsqlookupd/http.gofunc (s *httpServer) ServeHTTP(w http.ResponseWriter, req *http.Request) &#123; s.router.ServeHTTP(w, req)&#125;// PING 请求处理器func (s *httpServer) pingHandler(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error) // INFO（版本信息）查询请求处理器func (s *httpServer) doInfo(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error)// 查询所有 Category 为 topic的 Registration 的集合所包含的 Key 集合（topic名称集合）func (s *httpServer) doTopics(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error)// 查询所有 Category 为 channel，// 且 topic 为请求参数中指定的 topic 的 Registration 的集合包含的 SubKey 集合（channel名称集合）func (s *httpServer) doChannels(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error) // 查询所有 Category 为 topic的 channels 的集合以及producers集合。func (s *httpServer) doLookup(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error) // 根据 topic 来添加注册信息 Registrationfunc (s *httpServer) doCreateTopic(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error)// 根据 topic 来删移除注册信息 Registrationfunc (s *httpServer) doDeleteTopic(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error) // 为指定 topic 关联的 producer 设置为 tombstone 状态。func (s *httpServer) doTombstoneTopicProducer(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error) // 根据 topic 和 channel 的名称添加注册信息func (s *httpServer) doCreateChannel(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error)// 根据 topic 和 channel 的名称移除注册信息func (s *httpServer) doDeleteChannel(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error) // /nsq/nsqlookupd/http.go http 请求 topic 创建/查询处理过程最后，笔者简要介绍，消费者请求nsqlookupd的完成的请求服务。比如，当消费者需要查询某个topic在哪些nsqd上时，它可以通过/lookup的http GET请求来查询结果，nsqlookupd所提供的查询的指定topic信息的接口为：curl &#39;http://127.0.0.1:4161/lookup?topic=test-topic&#39;，从返回结果为此topic所关联的channels列表和producer列表。其具体处理流程不再阐述，比较简单，其详细代码如下： 12345678910111213141516171819202122// 查询所有 Category 为 topic的 channels 的集合以及producers集合。func (s *httpServer) doLookup(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error) &#123; reqParams, err := http_api.NewReqParams(req) // 1. 解析请求参数 // ... topicName, err := reqParams.Get("topic") // 2. 获取请求查询的 topic // ... // 3. 根据 topic 查询 registration registration := s.ctx.nsqlookupd.DB.FindRegistrations("topic", topicName, "") // ... // 4. 根据 topic 查询 channel 列表 channels := s.ctx.nsqlookupd.DB.FindRegistrations("channel", topicName, "*").SubKeys() // 5. 根据 topic 查询 producer 列表 producers := s.ctx.nsqlookupd.DB.FindProducers("topic", topicName, "") // 6. 过滤掉那些 inActive 的 producers，同时也过滤那些 tombstone 状态的 producers producers = producers.FilterByActive(s.ctx.nsqlookupd.opts.InactiveProducerTimeout, s.ctx.nsqlookupd.opts.TombstoneLifetime) return map[string]interface&#123;&#125;&#123; // 7. 返回 topic 所关联的 channel 和 producer 列表 "channels": channels, "producers": producers.PeerInfo(), &#125;, nil&#125; // /nsq/nsqlookupd/http.go 同样，也介绍一个nsqd请求nsqlookupd通过http协议完成的请求服务。比如，当nsqd需要请求nsqlookupd注册topic信息时，其可通过/topic/create的http POST请求来创建，而参数为topic名称，对应的请求处理函数为doCreateTopic，且没有返回值，处理函数的具体逻辑即为通过topic创建一个Registration实例，然后添加到RegistrationDB中。相关代码如下： 12345678910111213// 根据 topic 来添加注册信息 Registrationfunc (s *httpServer) doCreateTopic(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error) &#123; reqParams, err := http_api.NewReqParams(req) // 1. 解析请求参数 // ... topicName, err := reqParams.Get("topic") // 2. 获取请求创建的 topic // ... s.ctx.nsqlookupd.logf(LOG_INFO, "DB: adding topic(%s)", topicName) key := Registration&#123;"topic", topicName, ""&#125; // 3. 构建一个 Registration // 4. 将其添加到 RegistrationDB（value 为空的 map） s.ctx.nsqlookupd.DB.AddRegistration(key) return nil, nil&#125;// /nsq/nsqlookupd/http.go 至此，关于nsqlookupd相关的逻辑的源码已经分析完毕。相比nsqd要简单，没有复杂的流程。 简单小结，本文以执行nsqlookupd命令为切入点，先是简要分析了nsqlookupd其利用svc启动一个进程的过程。进而分析了NSQLookupd的Main方法的执行流程，其核心逻辑为创建了tcp及http请求的处理器，并注册了监听函数。本文的重点在于分析tcp请求处理器的详细内容，附带阐述了nsqd实例启动后与nsqlookupd实例的一个交互过程，具体包括IDENTIFY、REGISTER及PING等命令请求。然后，对于http请求处理器也进行了简要分析，侧重于处理器的创建过程。最后，对于http请求的方式，以两个示例分别阐述了客户端（消费者）及nsqd请求nsqlookupd完成topic查询和topic注册过程。更详细内容可以参考笔者简要注释的源码。 参考文献 [1]. https://github.com/nsqio/nsq[2]. https://nsq.io/overview/quick_start.html]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nsq 简介和特性理解]]></title>
    <url>%2F2019%2F05%2F11%2Fnsq-%E7%AE%80%E4%BB%8B%E5%92%8C%E7%89%B9%E6%80%A7%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前段时间写的分布式系统相关lab，都是使用go语言，还挺有趣的这门语言，但至今也没系统了解过它。不得不说，纯粹了解或学习语言特性，长时间下来会有点枯燥。个人更习惯阅读开源项目，以更深入了解其在实际工程项目的最佳实践（但似乎不能对语言特性本质有太多的了解，尴尬）。很自然地选择了一个对于初学者比较经典的starter project，一个分布式实时消息队列——nsq。大家比较熟悉的消息中间件可能是像RabbitMQ、ActiveMQ、RocketMQ以及kafaka之类的。我个人对这些消息中间件并没有实践经验，但感觉nsq更轻量级（代码量也少），而且正如官方对其的定义，其最主要优势在于分布式和实时。具体对比，可以参考golang 2017开发者大会上的一张图。nsq遵循的是一个去中心化的的拓扑结构，其具备高可用性（无单点故障SPOF）、可扩展（支持水平扩展节点）、低延迟（采用push的方式）、可靠消息传递（消息可被持久化和失败重入队列）、组合式负载均衡（消息推送为随机负载均衡）以及消费者自动发现和连接生产者（通过服务发现与注册系统实现）等特性。笔者打算后面通过一系列博文，深入到nsq源码，简析其各模块（组件）的设计实现。 按照惯例，先声明下阅读本文章的前提。本文不会详细介绍nsq基本特性，也不会过多阐述消息队列的基本功能和使用场景，也不会涉及到nsq如何使用及其最佳实践。换言之，阅读本文前最好对这些有初步了解，以在整体上有所把握和体会。关于nsq可参考官方文档的文献[1]、[2]、[3]和[4]，或者其它中文文章[4]。本文先简要介绍nsq各组件及基本工作流程，然后详细阐述nsq几个值得注意的特性。若有理解错误的地方，欢迎指正！ nsq 基本介绍在本文最前面，简要阐述了nsq的几个重要特性（官方文档列举的更全），我们会重点阐述这些特性。关于它们中的一些，我们深入到代码以了解其是如何实现的。而其它的典型的包括：易于部署，因为它没有太多依赖配置（通过命令行就可以配置）；支持安全传输层协议 (TLS)；还提供了多种语言的客户端功能库；最后附带一个简易的集群管理界面nsqadmin。nsq主要包括两个组件nsqd和nsqlookupd，和一个管理界面nsqdadmin： nsqd是一个负责接收、排队、转发消息到客户端的守护进程； nsqlookupd是一个管理集群(nsqd)拓扑信息并提供最终一致性的服务注册与发现的守护进程； nsqadmin是一个管理界面，提供实时的集群中topic、channel和message的统计信息，还提供与这些实体相关的各种管理接口。 在这里简单阐述整个系统的典型流程。除了上述几个组件外，另外两个核心的概念为主题(topic)和通道channel，当然消息(message)也是重要的。它们的简单阐述如下： message即代表的是数据，它被生产者创建并发送到指定的topic，而消费者可以从指定的topic和channel接收并消费消息。且每个被nsqd接收到的消息至少会被发送一次给消费者，因为消息消费超时会触发重入队过程，且消息附带有一个投递次数属性，以使得客户端对投递次数过大的消息灵活处理。 topic代表生产者投递消息的一个逻辑键值，它可以将消息进行分类。一个nsqd上可包含多个topic，且它们可以不必在生产者投递消息前就创建，换言之，topic可在其第一次接收到生产者投递的消息时创建。 channel代表消费者订阅某个nsqd上的topic的消息。你可以仍旧将它视作一个消息队列，只不过它与消费者相关。每当生产者将消息发布到一个topic上，消息会被拷贝（深拷贝，即构建一个消息副本）到与topic关联的所有的channel。而且，多个消费者可以订阅同一个channel，channel会将其接收到的消息随机（即随机负载均衡）发送到与其关联的一个客户端。同topic类似，channel也可不用提前创建，消费者在第一次订阅消息（在指定topic的某个channel）的时候会创建此channel（若其不存在）。最后，channel从topic接收的消息首先会在内存中排队，当达到内存队列长度上限，就被写到持久化存储。 nsqd在启动时通常会建与各nsqlookupd的连接。nsqlookupd为其提供目录服务，即nsqd可将topic、channel及nsqd(生产者)的信息注册到nsqlookupd，并且可通过与nsqlookupd之间的TCP连接动态更新自己的状态信息（topic及channel信息等）。而消费者可通过http请求连接到nsqlookupd，以查找其感兴趣的nsqd的地址。正是通过这种设计使得消费者与生产者分离，从而降低系统复杂性。在获取到生产者(nsqd)的地址后（准确而言，还包括topic及channel） ，消费者（nsq客户端）可直接连接到nsqd，以等待对方发送消息（不是主动去拉到消息，而是等被对方推送消息）。另外，为了提供更好的吞吐量，nsq允许消费者显式地向nsqd声明其能够处理的消息数量，这可以提高消息的吞吐量（避免消费者积压消息的情况）。针对每一个消息，当消费者消费完成后，其需要回复nsqd一个FIN消息，否则消息会被重入队，然后再一次被发送给消费者。最后，有一点需要注意，客户端（消费者）一次性只能订阅一个channel。官方文档提供的消息流动图如下： 关键特性个人在阅读官方文档之后，对于其描述的一些特性比较模糊，甚至有一些误解，在后面的进一步了解过程（甚至有些东西未深入到源码是不太能理解的），因此个人总结下如下几点是需要特别理解和注意的： nsq官方所声称的去中心化分布式特征，指的是，因为各节点(nsqd甚至也包括nsqlookupd)之间不会产生状态（数据）共享或依赖，因此可以直接通过添加节点来提升系统处理能力（即系统可通过水平扩展来来线性提升处理能力），且单个节点宕机不会影响其它节点的功能。因此，正是因为节点之间没有状态共享，没有数据冗余或副本的概念，使得它也不需要使用复杂的一致性算法来保证数据的一致； 虽然官方声称其是具体可靠消息传递，但是需要注意的是，其的确也存在消息丢失的情形，因为虽然它通过提供消息持久化来缓解这一问题，但并没有根本解决这一问题，一旦宕机，内存中的消息会丢失，当然你可以将内存队列长度mem_queue_size参数设置地更小一些以缓解此问题。甚至，你可以节点作冗余操作，以保证数据丢失的可能性在实际生产中几乎不可能发生（注意不是不可能发生），但对于那些需要高可靠性的消息发布的应用场景，nsq是无法保证的。 还有，官方文档提到一点，nsq并没有kafaka那么强大，它不能保证消息的严格顺序，换言之，生产者创建的消息可以随时（不确定性）地以任何顺序进入到nsqd的消息队列。典型地，官方推荐将消息生产者与nsqd实例协同部署，即部署在同一台机器上，这样即使发生网络分区，也不会影响生产者消息的投递，明显在此种情况下消息投递的效率理论上会被其它情况下的消息投递效率。但官方提供了一种解决方案，即将消息打上时间戳。尽管如此，它仍然不适合需要保证严格消息顺序的情况下。 nsq保证的消息至少一次会被发送给生产者。换言之，消息可能被多次发送给消费者。因此，消费者应该能够识别消息重复(message de-duplicate)，或者保证消息所涉及的操作幂等性(idempotent)。更具体地，造成消息被多次发送的原因包括，客户端连接断开或消息超时时间内未返回响应，这些都会导致消息的重入队操作(requeue)。 关于nsqlookupd，其也是可水平扩展的，但各节点实例之间也是没有任何联系的，结合业务逻辑来阐述，即每个nsqdlookupd实例可分别接收部分或全部的nsqd实例的服务注册请求，这只需要客户端在连接到nsqlookupd获取所有的生产者(及topic和channel)信息后，对获取的数据进行一个union操作即可，便可得到整个系统中生产者的拓扑信息（这即是官方文档中所描述的将发现服务设计成eventually consistent）。 最后，不得不说的一点是，正是因为上面所描述的各种不足，造就了nsq的精简设计。简单实在是太重要了！因为简单意味着容易进行故障或bug查找，同时，容易部署和使用方便。不得不说，当满足了应用的基本功能之后，简单往往是最重要的一个因素。 小结一下，本文先简要介绍了nsq的各个组件及基本工作流程，然后重点阐述了几个值得关注的特性，这些特性（包括未讨论的）以及nsq的各个组件，会在后面的博文中阐述。为了更好地理解nsq，个人建议，先仔细阅读官方文档，并简单实践，若有兴趣，深入源码查看会收获更多。 参考文献 [1]. nsq Features &amp; Guarantees[2]. nsq Design[3]. nsq Quick Start[4]. nsq - NYC Golang Meetup[5]. 消息中间件NSQ深入与实践]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 内存模型]]></title>
    <url>%2F2019%2F03%2F07%2FJava-%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[并发编程中，需要处理两个关键问题：线程之间如何通信及线程之间如何同步。通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。Java语言是采用共享内存的方式来完成线程间通信的。为了保证共享内存的正确性（可见性、有序性、原子性），内存模型定义了共享内存系统中多线程程序读写操作行为的规范。因此，Java内存模型（Java Memory Model ,JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。 并发编程中为了保证数据的安全，需要满足以下三个特性：(a) 原子性是指在一个操作中就是 cpu 不可以在中途暂停然后再调度，既不被中断操作，要不执行完成，要不就不执行。(b) 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。(c) 有序性即程序执行的顺序按照代码的先后顺序执行。且实际上，缓存一致性问题其实就是可见性问题。处理器优化可以导致原子性问题。指令重排会导致有序性问题。为了保证共享内存的正确性（可见性、有序性、原子性），内存模型定义了共享内存系统中多线程程序读写操作行为的规范。 Java内存模型（Java Memory Model ,JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。(a) Java内存模型规定所有的变量都存储在主内存中，每条线程还有自己的工作内存（本地内存），线程的工作内存中保存了该线程中是用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存。不同线程之间也无法直接访问对方工作内存中的变量，线程间变量的传递需要自己的工作内存和主存之间进行数据同步进行。本地内存是 JMM 的一个抽象概念，并不真实存在。它涵盖缓存，写缓冲区，寄存器及其他硬件和编译器优化。(b) JMM就作用于工作内存和主存之间数据同步过程。它规定了如何做数据同步以及什么时候做数据同步。因此，它决定一个线程对共享变量的写入何时对另一个线程可见。 重排序。从源代码到最终的指令序列，经历的重排序过程如下：源代码 -&gt; 编译器优化重排序 -&gt; 指令级并行重排序 -&gt; 内存系统重排序 -&gt; 最终执行的指令序列。(a) 对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 java 编译器在生成指令序列时，插入特定类型的内存屏障指令，通过内存屏障指令来禁止特定类型的处理器重排序。(b) 现代的多处理器大都支持StoreLoad屏障（其他 3 种类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中。 happens-before。通过happens-before概念来阐述操作之间的内存可见性。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在 happens-before 关系。与程序员密切相关的happens-before规则如下（另外的规则可以参考文献[2]或[3]）：(1) 程序顺序规则：一个线程中的每个操作，happens-before 于该线程中的任意后续操作。(2) 监视器锁规则：对一个监视器锁的解锁，happens-before 于随后对这个监视器锁的加锁。(3) volatile 变量规则：对一个 volatile 域的写，happens-before 于任意后续对这个 volatile 域的读。(4) 传递性：如果 A happens-before B，且 B happens-before C，那么 A happens-before C。注意，两个操作之间具有 happens-before 关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。 重排序。如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性（读写、写写 和 写读三种情况）。(a) 针对单个处理器中执行的指令序列和单个线程中执行的操作，编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。(b) 不管怎么重排序（为了提高并行度），（单线程）程序的执行结果不能被改变。即编译器，runtime 和处理器都必须遵守 as-if-serial 语义。为了遵守 as-if-serial 语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。(c) 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 顺序一致性。顺序一致性内存模型为程序员提供了极强的内存可见性保证：(1) 一个线程中的所有操作必须按照程序的顺序来执行。(2) （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。(a) JMM 对正确同步的多线程程序的内存一致性做了如下保证：如果程序是正确同步的，程序的执行将具有顺序一致性—— 即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同。换言之，未正确同步程序在 JMM 中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。(b) 在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在 JMM 中，临界区内的代码可以重排序，以尽可能执行编译器和处理器优化（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。(c) 对于未同步或未正确同步的多线程程序，JMM 只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值。JMM不保证未同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。 volatile。对一个 volatile 变量的单个读 / 写操作，与对一个普通变量的读 / 写操作使用同一个监视器锁来同步，它们之间的执行效果相同。简而言之，volatile 变量自身具有下列特性：(1) 可见性。对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入。(2) 原子性：对任意单个 volatile 变量的读 / 写具有原子性，但类似于 volatile++ 这种复合操作不具有原子性。(a) 从内存语义的角度来说，volatile 与监视器锁有相同的效果：volatile 写和监视器的释放有相同的内存语义；volatile 读与监视器的获取有相同的内存语义。(b) volatile 写的内存语义如下：当写一个 volatile 变量时，JMM 会把该线程对应的本地内存中的共享变量刷新到主内存。 volatile 读的内存语义如下：当读一个 volatile 变量时，JMM 会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。(c) 若我们把 volatile 写和 volatile 读这两个步骤综合起来看的话，在读线程 B 读一个 volatile 变量后，写线程 A 在写这个 volatile 变量之前所有可见的共享变量的值都将立即变得对读线程 B 可见。(d) JSR-133 专家组决定增强 volatile 的内存语义：严格限制编译器和处理器对 volatile 变量与普通变量的重排序，确保 volatile 的写 - 读和监视器的释放 - 获取一样，具有相同的内存语义。(e) JMM 针对编译器制定的 volatile 重排序规则表：(1) 当第二个操作是 volatile 写时，不管第一个操作是什么，都不能重排序。这个规则确保 volatile 写之前的操作不会被编译器重排序到 volatile 写之后。(2) 当第一个操作是 volatile 读时，不管第二个操作是什么，都不能重排序。这个规则确保 volatile 读之后的操作不会被编译器重排序到 volatile 读之前。(3) 当第一个操作是 volatile 写，第二个操作是 volatile 读时，不能重排序。 锁。当线程释放锁时，JMM 会把该线程对应的本地内存中的共享变量刷新到主内存中。当线程获取锁时，JMM 会把该线程对应的本地内存置为无效，从而使得被监视器保护的临界区代码必须要从主内存中去读取共享变量。(a) CAS 同时具有 volatile 读和 volatile 写的内存语义。(b) AQS，非阻塞数据结构和原子变量类（java.util.concurrent.atomic 包中的类）都是基于 volatile变量的读/写及 CAS 操作实现。而前三者又构成了java并发包中的同步器的基础。 final。与前面介绍的锁和 volatile 相比较，对 final 域的读和写更像是普通的变量访问。对于 final 域，编译器和处理器要遵守两个重排序规则：(1) 在构造函数内对一个 final 域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。(2) 初次读一个包含 final 域的对象的引用，与随后初次读这个final 域，这两个操作之间不能重排序。(a) 写 final 域的重排序规则会要求译编器在 final 域的写之后，构造函数 return 之前，插入一个 StoreStore 障屏。读 final 域的重排序规则要求编译器在读 final 域的操作前面插入一个 LoadLoad 屏障。(b) 当final 域为一个引用类型，比如它引用一个 int 型的数组对象。此时写final域的重排序规则对编译器和处理器增加了如下约束：在构造函数内对一个 final 引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。(c) JSR-133 专家组增强了 final 的语义。通过为 final 域增加写和读重排序规则，可以为 java 程序员提供初始化安全保证：只要对象是正确构造的（被构造对象的引用在构造函数中没有“逸出”），那么不需要使用同步（指 lock 和 volatile 的使用），就可以保证任意线程都能看到这个 final 域在构造函数中被初始化之后的值。 总结。(a) 由于常见的处理器内存模型比 JMM 要弱，java 编译器在生成字节码时，会在执行指令序列的适当位置插入内存屏障来限制处理器的重排序。同时，由于各种处理器内存模型的强弱并不相同，为了在不同的处理器平台向程序员展示一个一致的内存模型，JMM 在不同的处理器中需要插入的内存屏障的数量和种类也不相同。(b) 顺序一致性内存模型是一个理论参考模型，JMM 和处理器内存模型在设计时通常会把顺序一致性内存模型作为参照。(c) JMM 是一个语言级的内存模型，处理器内存模型是硬件级的内存模型，顺序一致性内存模型是一个理论参考模型。(d) 从 JMM 设计者的角度来说，在设计 JMM 时，需要考虑两个关键因素：(1) 程序员对内存模型的使用。程序员希望内存模型易于理解，易于编程。程序员希望基于一个强内存模型来编写代码。(2) 编译器和处理器对内存模型的实现。编译器和处理器希望内存模型对它们的束缚越少越好，这样它们就可以做尽可能多的优化来提高性能。编译器和处理器希望实现一个弱内存模型。由于这两个因素互相矛盾，所以 JSR-133 专家组在设计 JMM 时的核心目标就是找到一个好的平衡点：一方面要为程序员提供足够强的内存可见性保证；另一方面，对编译器和处理器的限制要尽可能的放松。(e) Java 程序的内存可见性保证按程序类型可以分为下列三类：(1) 单线程程序。单线程程序不会出现内存可见性问题。编译器，runtime 和处理器会共同确保单线程程序的执行结果与该程序在顺序一致性模型中的执行结果相同。(2) 正确同步的多线程程序。正确同步的多线程程序的执行将具有顺序一致性（程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同）。这是 JMM 关注的重点，JMM 通过限制编译器和处理器的重排序来为程序员提供内存可见性保证。(3) 未同步 / 未正确同步的多线程程序。JMM 为它们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值。 以上内容大部分自文献[1]，并整合了个人的思考。 参考文献 [1]. 深入理解Java内存模型 程晓明[2]. JSR-133_JavaTM Memory Model and Thread Specification[3]. 周志明. 深入理解 Java 虚拟机一 JVM 高级特性与最佳实践[J]. 2014.]]></content>
      <categories>
        <category>Java并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd-raft LeaseRead 线性一致性源码简析]]></title>
    <url>%2F2019%2F01%2F16%2Fetcd-raft-LeaseRead-%E7%BA%BF%E6%80%A7%E4%B8%80%E8%87%B4%E6%80%A7%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上篇文章阐述的是etcd-raft基于ReadIndex实现线性一致性的相关逻辑，这包括上层应用程序对客户端读请求的控制，以及底层协议库实现ReadIndex线性一致性的逻辑，另外，也简单阐述了线性一致性相关理论，包括顺序一致性及严格一致性。本文的主题同样是线性一致性，但是是etcd-raft提供的另一种实现方式：LeaseRead。相比于基于ReadIndex的实现，它性能更好，因为它没有heartbeat开销，但它却不能保证绝对意义上的线性一致性读，这依赖于机器时钟，工程实现只能尽可能保证在实际运行中不出错。基于lease的线性一致性读的原理和实现都比较简单。 上篇文章谈到过，基于ReadIndex实现的线性一致性的一个关键步骤即为leader通过广播心跳来确保自己的领导地位，显然这会带来网络开销（虽然实际中这种开销已经很小了）。因此可以考虑进一步优化。在Raft论文中提到了一种通过clock + heartbeat的lease read的优化方法，即每次当leader发送心时，先记录一个时间start，当quorum节点回复leader心跳消息时，它就可以将此lease续约到start + election timeout时间点，当然实际上还要考虑时钟偏移（clock drift），其中的原理也比较简单，因为任何时候若follower节点想发起新的一轮选举，必须等到election timeout后才能进行，这也就间接保证了在这段时间内无论什么情况（比如网络分区），leader都绝对拥有领导地位。再次强调，这依赖于机器的时钟飘移速率，换言之，若各机器之间的时钟差别过在，则此种基于lease的机制就可能出现问题。 下面结合etcd-raft的源码来简单梳理这个过程。我们重点关注两个逻辑：其一，在lease有效期内，leader如何处理读请求。其二，leader如何更新（续约）其lease。 基于 lease read 的线性一致性基于lease read同基于ReadIndex实现的线性一致性在应用程序层的逻辑是一致的，不作多阐述。重点了解协议库是如何处理的。同样，我们定位到leader的stepLeader()函数，同样是MsgReadIndex分支： 123456789101112131415161718192021222324252627282930313233func stepLeader(r *raft, m pb.Message) error &#123; switch m.Type &#123; // ... case pb.MsgReadIndex: if r.quorum() &gt; 1 &#123; // 1. 如果 leader 在当前任期内没有提交过日志，则直接返回，不处理此 ReadIndex 请求 // 否则会造成 过期读 甚至不正确的读 if r.raftLog.zeroTermOnErrCompacted(r.raftLog.term(r.raftLog.committed)) != r.Term &#123; // Reject read only request when this leader has not committed any log entry at its term. return nil &#125; // 2. 判断线性一致性读的实现方式 switch r.readOnly.option &#123; case ReadOnlySafe: // 3. 采用 ReadIndex 实现 // ... case ReadOnlyLeaseBased: // 4. 采用 leaseBase 实现 ri := r.raftLog.committed // 4.1 获取当前的 commit index // 4.2 如果是本地的请求，则直接将 ReadState 追加到数组中，里面包含了 leader 的 commit index 及 ctx（请求唯一标识） if m.From == None || m.From == r.id &#123; // from local member r.readStates = append(r.readStates, ReadState&#123;Index: r.raftLog.committed, RequestCtx: m.Entries[0].Data&#125;) &#125; else &#123; // 4.3 如果 follower 节点转发的，则直接向其回复 MsgReadIndexResp 消息，并带上commit index 及 Entries r.send(pb.Message&#123;To: m.From, Type: pb.MsgReadIndexResp, Index: ri, Entries: m.Entries&#125;) &#125; &#125; &#125; else &#123; r.readStates = append(r.readStates, ReadState&#123;Index: r.raftLog.committed, RequestCtx: m.Entries[0].Data&#125;) &#125; return nil &#125; // ...&#125; // /etcd/raft/raft.go 上面的流程很简单，不多阐述。需要注意的一点是，若是 follower 收到读请求，其基于lease read的处理逻辑同基于ReadIndex一致，即要先向leader查询commit index。下面重点了解lease的续约逻辑。 lease 续约在阐述lease具体的续约准则之前，我们先了解下在etcd-raft中，触发检测lease是否过期的相关代码，因为leader要确保自己的领导地位，因此它必须周期性地检查自己是否具备领导地位。它通过周期性地向自己发送MsgCheckQuorum类型消息来验证自己是否具备领导地位（即此次lease是否能成功续约）。代码如下： 1234567891011121314151617// leader 会周期性地给自己发送 MsgCheckQuorum 消息func (r *raft) tickHeartbeat() &#123; r.heartbeatElapsed++ r.electionElapsed++ // 若达到了 electionTimeout 的时间（并非 heartbeat timeout），则需要向自己发送消息 if r.electionElapsed &gt;= r.electionTimeout &#123; r.electionElapsed = 0 if r.checkQuorum &#123; r.Step(pb.Message&#123;From: r.id, Type: pb.MsgCheckQuorum&#125;) &#125; // If current leader cannot transfer leadership in electionTimeout, it becomes leader again. if r.state == StateLeader &amp;&amp; r.leadTransferee != None &#123; r.abortLeaderTransfer() &#125; &#125; // ...&#125; // /etcd/raft/raft.go leader同样在stepLeader()函数中处理MsgCheckQuorum类型的消息： 123456789101112131415161718192021222324252627282930func stepLeader(r *raft, m pb.Message) error &#123; switch m.Type &#123; // ... case pb.MsgCheckQuorum: if !r.checkQuorumActive() &#123; // 检查是否 quorum 节点仍然活跃 r.logger.Warningf("%x stepped down to follower since quorum is not active", r.id) r.becomeFollower(r.Term, None) &#125; return nil &#125; // ...&#125; // /etcd/raft/raft.gofunc (r *raft) checkQuorumActive() bool &#123; var act int // 循环检查 leader 维护的 progress 对象数组，来判断对应的节点是否活跃 r.forEachProgress(func(id uint64, pr *Progress) &#123; if id == r.id &#123; // self is always active act++ return &#125; if pr.RecentActive &amp;&amp; !pr.IsLearner &#123; act++ &#125; // 并且在每次检查完毕后，都要重置它，以确保下一次检查不会受到此次结果的影响 pr.RecentActive = false &#125;) // 若存在 quorum 节点活跃，则返回 true return act &gt;= r.quorum()&#125; // etc/raft/raft.go 关于lease续约不同的系统可能有不能的实现，但其目的只有一个：确认follower依然遵从leader的领导地位。这可以从几个方面体现出来，其一，如果每次leader发送的心跳消息(MsgHeartbeat)，节点都响应了，则证明此节点依然受到leader的领导。其二，若每次leader发送的日志同步消息(MsgApp)，节点都响应了，则同样能够证明leader的领导地位。最后，其实在节点刚刚加入集群时，也标记其接受leader的领导。这就是RecentActive所被标记为true的地方。因此，每当触发了election timeout事件，leader都需要重新检查自己是否仍然具备领导地位，实质上就是检查每个节点的RecentActive是否被设置，如果具备，则表明成功续约了lease，因此可以不经额外的处理，就能够直接返回自身的commit index作为ReadIndex的响应。整个流程比较简单，其中原理也较容易理解。 简单小结，本文先是简单阐述了关于lease read实现线性一致性（比基于ReadIndex的实现更有效率）的基本原理，然后结合etcd-raft的实现来进一步细化理解整个过程，这包括两方面：其一是基于lease read线性一致性的处理逻辑（在lease有效期内）。其二是lease的续约过程，即何时触发续约事件，以及续约的条件是什么。值得一提的是，不同的系统的对续约条件的实现可能不同，而且为了尽可能保证基于lease实现的线性一致性的正确性，会加入一些优化动作。 参考文献 [1]. https://github.com/etcd-io/etcd/tree/master/raft[2]. etcd-raft的线性一致读方法二：LeaseRead]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>线性一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd-raft ReadIndex 线性一致性读源码简析]]></title>
    <url>%2F2019%2F01%2F15%2Fetcd-raft-ReadIndex-%E7%BA%BF%E6%80%A7%E4%B8%80%E8%87%B4%E6%80%A7%E8%AF%BB%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上篇文章阐述了etcd-raft集群配置变更源码的相关逻辑，同时，在理论层面，从正反两个方面简要y论述了一次集群配置变更只能涉及到一个节点的原因。本文的主题为etcd-raft使用ReadIndex来实现线性一致性(linearizability)的实现原理。这包括两个方面，首先简要阐述线性一致性的理论知识，其次是结合etcd-raft的源码来简单梳理其如何使用ReadIndex来保证读请求的线性一致性。线性一致性广泛被应用于分布式应用中，是用于衡量一个并发系统是否正确的重要标准，我们通常谈论的CAP中的C指的即为线性一致性。需要说明的是，etcd是基于raft来提供一致性保证，虽然共识算法被用于保证状态的一致性，但并不代表实现共识算法的系统就自动具备了线性一致性，这是两个概念，换言之，etcd-raft必须在实现raft的基础上额外增加一些逻辑来保证系统具备线性一致性。 线性一致性简单而言，线性一致性是针对单个对象的单个操作的一种保证，它对单个对象的一系列操作（读和写）提供了一种实时的保证，即它们可以按照时间进行排序。不精确地说，linearizability可以保证： 一旦写操作写入了某个值，后面的（由wall-clock定义）读操作应该至少能够返回之前写的最新的值，换言之，它也可以返回之后的写操作所写入的值（注意不一定是读操作之前的最新的写的值） 一旦读操作返回了某个值，后面的读操作应该返回前一个读操作所返回的值，或者返回之后的写操作所写入的值。（注意不一定的是读之前最新的值） 并且线性一致性是可组合的(composable)，如果系统中每一个对象上的操作都符合线性一致性，那么系统中的所有操作都符合线性一致性。 另外，顺序一致性(serializability)很容易同线性一致性混淆。但实际上二者有较大的区别。且不严谨地说，线性一致性比顺序一致性提供更强的一致性保障语义。另外，不同于线性一致性属于分布式系统（并发编程系统）的概念，而顺序一致性是数据库领域的概念，它是对事务的一种保证，或者说，顺序一致性是针对一个或多个对象的一个或多个操作的一种保证。具体而言，它保证了多个事务（每个都可能包含了一组对于不同对象的读或写操作）的执行的效果等同于对这些事务的某一个顺序执行的效果。 顺序一致性是ACID中的I，且若每个事务都保证了正确性（ACID中的C），则这些事务的顺序执行也会保证正确性，可见，顺序一致性是数据库关于事务执行正确性的一种保证。不同于线性一致性，顺序一致性不会对事务执行的顺序强加任何实时的约束，换言之，其不需要事务的所有操作按照真实时间（应用程序指定的）严格排序的，只需要存在一个满足条件的顺序执行的顺序即可。最后顺序一致性也是不可组合的(composable)。 将线性一致性同顺序一致性结合起来，便是严格一致性(serializability)，即事务执行的行为等同于某一个顺序（串行）执行的效果，且这些串行的顺序对应实时的顺序。举一个简单的例子，如果存在两个事务T1及T2，我们先执行T1，T1中包含写x的操作，最后提交T1。我们然后执行T2，包含了读x的操作，然后提交它。若一个数据库系统满足严格一致性，则其会先执行T1并提交，然后才执行T2提交T2，因此T2能够读到T1中写入x的值，但如果数据库系统只提供顺序一致性，则其可能会将T2排序到T1之前。因此，可以将线性一致性看成是严格一致性的一种特殊情况，即一次执行只针对单个对象的单个操作。 另外论文 Linearizability: Correctness Condition for Concurrent Objects中给出了线性一致性的定义： Linearizability is a correctness condition for concurrent objects that provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object’s operations can be given by pre- and post-conditions 因此，为了提供线性一致性，一个系统应该保证存在这样一个时间点，在这个时间点之后，系统需要被提交到一个新的状态，并且绝不能返回到之前旧的状态。而且，这样的转变是瞬时的，具备原子性。最后，概括而言，线性一致性必须提供三个方面的保证：a) 瞬间完成（保证原子性），b) 发生在invocation 和response`两个事件之间，c) 能够反映出”最新的”值（特别注意这个最新的意义）。 关于此处对顺序一致性的顺序简单（可能不是完全精度）的阐述来源于 线性一致性和 Raft、On Ways To Agree, Part 2: Path to Atomic Broadcast 、 Linearizability versus Serializability 以及Strong consistency models。当然你也可以参照 论文。其中文献[1]举例了一个非常通俗易理解的实例来帮助解读对线性一致性理解的普遍的误区。特别地，文献[2]的comment讨论了关于”实时性”以及”可组合”更精确的含义。 etcd-raft ReadIndex 线性一致性简析在etcd-raft实现中，所有的写请求都会由leader执行并将请求日志同步到follower节点，且若follower节点收到客户端的写请求，则一般是把写请求转发给leader。那么对于读请求又如何处理呢？虽然etcd-raft能够对日志提供一致性保证，但若不加以协调，两个原因导致在etcd-raft中从不同节点读数据可能会出现不一致： leader节点与follower节点存在状态差，因为日志是从leader节点同步至follower节点，但不能保证任何时刻 ，二者的日志完全相同，即follower完全有可能落后于leader。另外follower之间同样如此，即也不能保证所有的follower节点的日志完全一致。因此必须对读操作进行协调。 如果限制只能从leader节点读取（至少leader状态机中最有可能包含最新的数据），这样仍然存在一个问题：若网络发生分区，则包含quorum节点的分区可能选举出一个新leader代替了旧leader，而旧leader却仍然以为自己是作法的leader，并依然处理客户端的读请求，则此时其可能会返回过期的数据，即与从包含quorum节点的分区读到的数据很有可能不同。 由此可见，必须对读请求作出限制，首先总结etcd-raft针对leader完成ReadIndex线性一致性读所作的协调处理的大致过程： leader需要同集群中quorum节点通信，以确保自己仍然是合法的leader。这一点容易理解，在上面的举例当中，若leader处于网络分区中的非quorum中，则其很可能会被取代，因此必须让leader确保自己仍然是leader。 等待状态机至少已经应用ReadIndex记录的日志。注意此处的至少两个字，简单而言，若状态机应用到RedaIndex/commit index之后的状态也能够使请求满足线性一致性，这同上文对线性一致性的解释中所强调的是一致的。需要这一条保证的原因是，虽然应用状态机的状态能达成一致，但不能保证多个节点会同时将同一个日志应用到状态机，换言之，各个节点的状态机所处的状态不能实时一致。因此，必须根据commit index对请求进行排序，以保证每个请求都至少能反映出状态机在执行完前一请求后的状态，因此，可以认为commit决定了读（也包括写）请求发生的顺序。日志是全局有序的，那么自然而然读请求也被严格排序了。因此这能保证线性一致性。 下文结合源码我们来了解etcd-raft是如何协调处理的。 首先简单了解相关数据结构，相关注释已概述了各结构的含义，主要涉及的代码目录为：/etcd/raft/。 12345678910111213141516171819202122// ReadState 负责记录每个客户端的读请求的状态// ReadState 最终会被打包放在 Ready 结构中以返回给应用，具体由应用负责处理客户端的读请求// 即根据 commit index 确定何时才能从状态机中读对应的数据返回给客户端type ReadState struct &#123; Index uint64 // 读请求对应的当前节点的 commit index RequestCtx []byte // 请求唯一标识，etcd 使用的是 8 位的请求 ID (/etcd/pkg/idutil/id.go)&#125; // /etcd/raft/read_only.go// readIndexStatus 用来记录 follower 对 leader 的心跳消息的响应type readIndexStatus struct &#123; req pb.Message // 原始 ReadIndex 请求，是应用在处理客户端读请求时向底层协议加发送的请求。 index uint64 // leader 当前的 commit index，在收到此读请求时 acks map[uint64]struct&#123;&#125; // 记录了 follower 对 leader 的心跳的响应消息， // map 的键为 follower 节点的 ID，值是一个空的 struct，没有意义&#125; // /etcd/raft/read_only.go// readyOnly 负责全局的 ReadIndex 请求type readOnly struct &#123; option ReadOnlyOption // 表示为 ReadOnlySafe 或者 ReadOnlyLeaseBased // （两种不同的实现线性一致性的方式，官方推荐前者，也是默认的处理方式） pendingReadIndex map[string]*readIndexStatus // 为一个保存所有待处理的 ReadIndex 请求的 map， // 其中的 key 表示请求的唯一标识（转换成了字符串），而 value 为 readIndexStatus 结构实例 readIndexQueue []string // 请求标识 (RequestCtx) 的数组，同样转换成了 string 进行保存&#125; // /etcd/raft/read_only.go 了解这些数据结构，能够基本感知到它们会被使用在什么地方，或者说它们各自的作用是什么。下面来梳理下etcd-raft所实现的ReadIndex线性一致性的关键流程。 关键流程我们仍然从客户端接收请求入手，但由于raftexample中并没有示例读请求的线性一致性的处理流程，因此，只能选择etcd-server来示例（要比 raftexample更复杂，但我们只关注与ReadIndex线性一致性相关逻辑，其它的不作多阐述）。整个过程包括两个大的部分：应用程序处理读请求（对读请求进行协调）以及底层协议库处理ReadIndex请求。 应用程序处理读请求此部分相关逻辑涉及到的代码目录为/etcd/etcdserver/。在etcd-server在启动创建是会执行Start()函数，以进行一些在接收并处理请求之前的初始化工作，Start()函数会开启若干个go routine来处理初始化任务，其代码如下所示，其中关键的代码为s.goAttach(s.linearizableReadLoop)，顾名思义，其会开启一个协程来循环处理线性一致性读请求。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104// 循环处理线性一致性读请求func (s *EtcdServer) linearizableReadLoop() &#123; var rs raft.ReadState for &#123; // 1. 构建 requestCtx 即请求 ID，且为全局唯一，具体查看 /etcd/pkg/idutil/id.go ctxToSend := make([]byte, 8) id1 := s.reqIDGen.Next() binary.BigEndian.PutUint64(ctxToSend, id1) // 2. 判断是否发生了 leader change 事件，若是，则重新执行 leaderChangedNotifier := s.leaderChangedNotify() select &#123; case &lt;-leaderChangedNotifier: continue // 3. 等待 readwaitc 管道中 pop 出通知， // 显然，即为等待客户端发起读请求，具体是在函数 linearizableReadNotify 中 push 通知的 case &lt;-s.readwaitc: case &lt;-s.stopping: return &#125; // 4. 创建一个 notifier，替换原有的。它类似于一个 condition 并发语义 nextnr := newNotifier() s.readMu.Lock() nr := s.readNotifier s.readNotifier = nextnr s.readMu.Unlock() lg := s.getLogger() // 这里构建一个可取消的机制 cctx, cancel := context.WithTimeout(context.Background(), s.Cfg.ReqTimeout()) // 5. 一旦收到一个客户端的读请求，则向底层协议库发送 ReadIndex 请求 // 底层协议库会构建一个类库 MsgReadIndex 的消息，并将 ctxToSend 作为 Message 的 Entry.Data if err := s.r.ReadIndex(cctx, ctxToSend); err != nil &#123; // ... &#125; cancel() var ( timeout bool done bool ) // 6. 设置了超时处理 for !timeout &amp;&amp; !done &#123; select &#123; // 7. 若从 readStateC 收到了 ReadState 通知，则说明底层协议库已经处理完成。 // 事实上，上层应用程序（在此处是/etcd/etcdserver/raft.go）当收到底层协议库的 Ready 通知时， // 并且 Ready 结构中包含的 ReadState 不为空，则会向 readStateC 管道中压入 ReadState 实例， // 此处就能 pop 出 ReadState 实例。总而言之，ReadIndex 请求执行至此处表示底层协议库已经处理完毕 // 只需要等待状态机至少已经应用 ReadIndex 的日志记录即可 case rs = &lt;-s.r.readStateC: done = bytes.Equal(rs.RequestCtx, ctxToSend) // ... case &lt;-leaderChangedNotifier: // ... case &lt;-time.After(s.Cfg.ReqTimeout()): // ... case &lt;-s.stopping: return &#125; &#125; if !done &#123; continue &#125; // 8. 获取 appliedIndex，判断其是否小于 ReadIndex，若是，则要继续等待，说明状态机此时仍未应用 ReadIndex 处日志 if ai := s.getAppliedIndex(); ai &lt; rs.Index &#123; select &#123; // 9. 等待被调用 s.applyWait.Trigger(index)，那些在index之前的索引上调用的 Wait，都会收到通知而返回。 // 具体而言，在 server.go 中的 start() -&gt; applyAll() 触发了通知 // 且 触发调用 applyAll() 是由 etcdsever/raft.go 中 start() 函数往 applyc 中 push 了通知 case &lt;-s.applyWait.Wait(rs.Index): case &lt;-s.stopping: return &#125; &#125; // unblock all l-reads requested at indices before rs.Index // 8. 否则，说明状态机已经至少应用到了 ReadIndex 日志，表明此时可以读取状态机中的内容，返回给客户端 nr.notify(nil) &#125;&#125; // /etcd/etcdserver/v3_server.go// 用于触发 linearizableReadLoop() 函数执行一遍循环中的等待处理 ReadIndex 请求的逻辑func (s *EtcdServer) linearizableReadNotify(ctx context.Context) error &#123; // 1. 获取 notifier s.readMu.RLock() nc := s.readNotifier s.readMu.RUnlock() // signal linearizable loop for current notify if it hasn't been already // 2. 向 readwaitc 管道中 push 一个空结构，以通知有 ReadIndex 请求到达 select &#123; case s.readwaitc &lt;- struct&#123;&#125;&#123;&#125;: default: &#125; // wait for read state notification // 3. 等待 notifier 的通知，即等待 linearizableReadLoop() 调用 notifier.notify() 函数 // 一旦触发 notifier 管道中 pop 的信号，则表明已经 ReadIndex 请求的准备工作已全部完毕 // 这包含两个部分：其一是底层协议库的工作，leader 确认自己仍旧是 leader // 其二，等待节点的状态机至少已经应用到 ReadIndex 处的日志 // 此时，就可以正式从状态机中读取对应的请求的内容 select &#123; case &lt;-nc.c: return nc.err case &lt;-ctx.Done(): return ctx.Err() case &lt;-s.done: return ErrStopped &#125;&#125; // /etcd/etcdserver/v3_server.go 总结而言，上述两个函数 linearizableReadNotify()及linearizableReadLoop()相当于锁的功能（此锁中包含多个条件等待操作），底层协议库未走完ReadIndex请求之前，或者应用层还未将ReadIndex应用到状态机之前，这把锁保证应用不会从状态机中读取请求数据，因此也不会返回对客户端读请求的响应。另外，顺便提一名，linearizableReadNotify()是当应用收到客户端的读请求时调用的，即在函数Range()中被调用，关键部分代码如下： 123456789101112func (s *EtcdServer) Range(ctx context.Context, r *pb.RangeRequest) (*pb.RangeResponse, error) &#123; var resp *pb.RangeResponse // ... if !r.Serializable &#123; err = s.linearizableReadNotify(ctx) if err != nil &#123; return nil, err &#125; &#125; // ... return resp, err&#125; // // /etcd/etcdserver/v3_server.go 关于ReadIndex请求在应用程序层（服务端层）被处理的过程已经解析完毕。下文阐述底层协议库的处理。 底层协议库处理 ReadIndex 请求底层协议库提供处理ReadIndex请求的一个接口：ReadIndex()，上层应用程序也正是调用此函数来使用协议库的协调功能。其中完整的调用栈为ReadIndex() -&gt; step() -&gt; stepWithWaitOption，然后通过将消息压入recvc管道，使得在run()函数从管道中收到消息，然后调用raft.Step()函数，经过一系列的检查之后，进入了stepLeader()函数，对应leader节点的处理流程，重要的代码如下所示： 12345// 接收上层应用程序的 ReadIndex 请求func (n *node) ReadIndex(ctx context.Context, rctx []byte) error &#123; // 创建一个 MsgReadIndex 的消息，其中 message 中的 entry 的 data 为请求的标识 return n.step(ctx, pb.Message&#123;Type: pb.MsgReadIndex, Entries: []pb.Entry&#123;&#123;Data: rctx&#125;&#125;&#125;)&#125; // /etcd/raft/node.go 进入到stepLeader()函数后，随即根据消息类型进行MsgReadIndex分支。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859func stepLeader(r *raft, m pb.Message) error &#123; switch m.Type &#123; // ... case pb.MsgReadIndex: if r.quorum() &gt; 1 &#123; // 1. 如果 leader 在当前任期内没有提交过日志，则直接返回，不处理此 ReadIndex 请求 // 否则会造成 过期读 甚至不正确的读 if r.raftLog.zeroTermOnErrCompacted(r.raftLog.term(r.raftLog.committed)) != r.Term &#123; // Reject read only request when this leader has not committed any log entry at its term. return nil &#125; // 2. 判断线性一致性读的实现方式 switch r.readOnly.option &#123; case ReadOnlySafe: // 3. 采用 ReadIndex 实现 // 3.1 使用 leader 节点当前的 commit index 及 ReadIndex 消息 m 构造一个 readIndexStatus，并追加到 pendingReadIndex 中 r.readOnly.addRequest(r.raftLog.committed, m) // 3.2 将此请求的ID(rctx)作为参数，并向集群中的节点广播心跳消息 r.bcastHeartbeatWithCtx(m.Entries[0].Data) case ReadOnlyLeaseBased: // 4. 采用 leaseBase 实现 // ... &#125; &#125; else &#123; r.readStates = append(r.readStates, ReadState&#123;Index: r.raftLog.committed, RequestCtx: m.Entries[0].Data&#125;) &#125; return nil &#125; // ...&#125; // /etcd/raft/raf.go// 并负责更新 pendingReadIndex(当前正在被处理的 ReadIndex 请求)，以及 readIndexQueuefunc (ro *readOnly) addRequest(index uint64, m pb.Message) &#123; ctx := string(m.Entries[0].Data) if _, ok := ro.pendingReadIndex[ctx]; ok &#123; return &#125; ro.pendingReadIndex[ctx] = &amp;readIndexStatus&#123;index: index, req: m, acks: make(map[uint64]struct&#123;&#125;)&#125; ro.readIndexQueue = append(ro.readIndexQueue, ctx)&#125; // /etcd/raft/read_only.go// 向集群中所有的节点发送心跳消息func (r *raft) bcastHeartbeatWithCtx(ctx []byte) &#123; r.forEachProgress(func(id uint64, _ *Progress) &#123; if id == r.id &#123; return &#125; r.sendHeartbeat(id, ctx) &#125;)&#125; // /etcd/raft/raf.go// 向指定节点发送心跳消息，并带上 ctxfunc (r *raft) sendHeartbeat(to uint64, ctx []byte) &#123; commit := min(r.getProgress(to).Match, r.raftLog.committed) m := pb.Message&#123; To: to, Type: pb.MsgHeartbeat, Commit: commit, Context: ctx, &#125; r.send(m)&#125; // /etcd/raft/raf.go 当消息经网络传输到达follower节点后，follower收到此心跳消息时，其相关的处理如下所示： 12345678910111213141516func stepFollower(r *raft, m pb.Message) error &#123; switch m.Type &#123; // ... case pb.MsgHeartbeat: r.electionElapsed = 0 r.lead = m.From r.handleHeartbeat(m) // ... &#125; return nil&#125; // /etcd/raft/raf.go// 处理心跳消息的逻辑也很简单，应用 commit index，然后发送心跳消息响应，并带上消息中的 ctxfunc (r *raft) handleHeartbeat(m pb.Message) &#123; r.raftLog.commitTo(m.Commit) r.send(pb.Message&#123;To: m.From, Type: pb.MsgHeartbeatResp, Context: m.Context&#125;)&#125; 同样，当leader收到心跳消息响应的处理逻辑如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func stepLeader(r *raft, m pb.Message) error &#123; // ... switch m.Type &#123; // ... case pb.MsgHeartbeatResp: // 1. 更新 leader 为消息中的节点的 progress 对象实例 pr.RecentActive = true pr.resume() // free one slot for the full inflights window to allow progress. if pr.State == ProgressStateReplicate &amp;&amp; pr.ins.full() &#123; pr.ins.freeFirstOne() &#125; // 2. 若发现节点日志落后，则进行日志同步 if pr.Match &lt; r.raftLog.lastIndex() &#123; r.sendAppend(m.From) &#125; // 3. 只有 ReadOnlySafe 类型的消息需要针对性处理，且其 ctx 不能为空 if r.readOnly.option != ReadOnlySafe || len(m.Context) == 0 &#123; return nil &#125; // 4. 更新 pendingReadIndex 中的 ack 字典（因为收到了 follower 的响应），并查看此时心跳响应是否达到 quorum ackCount := r.readOnly.recvAck(m) // 5. 若没达到 quorum 的心跳响应，则直接返回，说明此时流程还未走完 if ackCount &lt; r.quorum() &#123; return nil &#125; // 6. 在 pendingReadIndex 中返回 m 以前的所有的 readIndexStatus，一个 slice // 因为此请求依次顺序处理的，若此请求满足了底层协议库的条件，那么此请求之前的消息也会满足。 rss := r.readOnly.advance(m) // 7. 循环 readIndexStatus，并将其追加到 for _, rs := range rss &#123; req := rs.req // 7.1 若是节点本地的 ReadIndex 请求，则直接将其追加到 ReadState 结构中，最后会打包到 Ready 结构，由 node 返回给上层应用程序 if req.From == None || req.From == r.id &#123; // from local member r.readStates = append(r.readStates, ReadState&#123;Index: rs.index, RequestCtx: req.Entries[0].Data&#125;) &#125; else &#123; // 7.2 否则此消息则是来源于 follower，则向 follower 发送 MsgReadIndexResp 类型的消息 r.send(pb.Message&#123;To: req.From, Type: pb.MsgReadIndexResp, Index: rs.index, Entries: req.Entries&#125;) &#125; &#125; &#125; return nil&#125; // /etcd/raft/raf.go// 通知 readonly 结构，leader 节点收到了 follower 节点的心跳消息响应（此心跳消息是针对 ReadIndex 请求而发送的）func (ro *readOnly) recvAck(m pb.Message) int &#123; rs, ok := ro.pendingReadIndex[string(m.Context)] if !ok &#123; return 0 &#125; rs.acks[m.From] = struct&#123;&#125;&#123;&#125; // 返回此时 leader 已经收到的响应消息的数量 return len(rs.acks) + 1&#125; // /etcd/raft/read_only.go 至此，关于leader节点处理ReadIndex请求的流程已经阐述完毕，总的流程比较简单，即leader通过一轮心跳消息来确认自己仍然是leader。另外，若客户端将读请求发送给了follower节点，etcd-raft的实现是：应用层会调用协议的核心库的ReadIndex()方法，然后让follower节点先将ReadIndex消息发送给leader，接下来leader同样走一圈上面的流程，在确认自己依旧为leader后，将确认的ReadIndex通过MsgReadIndexResp消息发送给follower节点，最后同样，follower节点将构造ReadState并记录commit index，最后由上层应用收到Ready结构后，从中取出ReadState。因此，综合来看，若ReadIndex请求发送给了follower，则follower先要去问leader查询commit index，然后同样构造ReadState返回给上层应用，这和leader的处理是一样的。关于follower收到MsgReadIndex消息的核心代码如下： 1234567891011121314func stepFollower(r *raft, m pb.Message) &#123; switch m.Type &#123; ...... case pb.MsgReadIndex: if r.lead == None &#123; return &#125; m.To = r.lead r.send(m) // 先将消息转发给 leader case pb.MsgReadIndexResp: // 最后收到 leader 的 MsgReadIndexResp 消息回复后， // 追加到其 ReadStates 结构中，以通过 Ready 返回给上层应用程序 r.readStates = append(r.readStates, ReadState&#123;Index: m.Index, RequestCtx: m.Entries[0].Data&#125;) ......&#125; // /etcd/raft/raf.go 至此，关于etcd-raft如何处理ReadIndex线性一致性读的相关逻辑已经分析完毕。 最后，简单提一点，当节点刚被选举成为leader时，如果其未在新的term中提交过日志，那么其所在的任期内的commit index是无法得知的，因此，在etcd-raft具体实现中，会在leader刚选举成功后，马上提交追加提交一个no-op的日志（代码如下所示），在这之前所有客户端（应用程序）发送的读请求都会被阻塞（写请求肯定不会，其实若是有写请求了，也就不用提交空日志了）。通过此种方式可以确定新的term的commit index。 12345678910111213141516func (r *raft) becomeLeader() &#123; r.step = stepLeader r.reset(r.Term) r.tick = r.tickHeartbeat r.lead = r.id r.state = StateLeader r.prs[r.id].becomeReplicate() r.pendingConfIndex = r.raftLog.lastIndex() // 提交了一条 no-op 日志 emptyEnt := pb.Entry&#123;Data: nil&#125; if !r.appendEntry(emptyEnt) &#123; r.logger.Panic("empty entry was dropped") &#125; r.reduceUncommittedSize([]pb.Entry&#123;emptyEnt&#125;) r.logger.Infof("%x became leader at term %d", r.id, r.Term)&#125; // /etcd/raft/raft.go 简单小结，本文先是从理论角度阐述了什么是线性一致性，并且它具备什么特征。相比于顺序一致性，它们的不同点在哪里，最后线性一致性结合顺序一致性，即为严格一致性。关于这部分理论内容，读者若有兴趣，可以参考参考文献的[1]-[5]，讲得更完备和精确。后一部分内容就结合etcd 的代码阐述了其具体如何保证ReadIndex线性一致性，大概的流程为：先执行应用程序层对读请求的控制，它类似于一把锁的功能，在底层协议库未完成线性一致性相关的逻辑处理之前，会阻塞应用的读请求的处理，直至底层协议库走一圈后返回，才能继续处理，然后继续判断此时ReadIndex处的日志是否有被应用（若状态机已应用到ReadIndex之后的日志也完全可以），直至ReadIndex日志被提交才能返回，即才能释放锁，允许应用程序读取状态机。 参考文献 [1]. 线性一致性和 Raft[2]. On Ways To Agree, Part 2: Path to Atomic Broadcast[3]. Linearizability versus Serializability[4]. Strong consistency models[5]. Linearizability: Correctness Condition for Concurrent Objects[6]. https://github.com/etcd-io/etcd]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>线性一致性</tag>
        <tag>顺序一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd-raft 集群配置变更源码简析]]></title>
    <url>%2F2019%2F01%2F15%2Fetcd-raft-%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E5%8F%98%E6%9B%B4%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章阐述了etcd-raft snopshot相关逻辑，并从整体上把握etcd-raft snapshot的设计规范。本文的主题是集群配置变更的理论及实际的相关流程，即etcd-raft如何处理集群配置变更，且在配置变更前后必须保证集群任何时刻只存在一个leader。在raft论文中提出每次只能处理一个节点的变更请求，若一次性处理多个节点变更请求（如添加多个节点），可能会造成某一时刻集群中存在两个leader，但这是raft协议规范所不允许的。而etcd-raft的实现同样只允许每次处理一个配置变更请求。大概地，etcd-raft首先将配置变更请求同普通请求日志进行类似处理，即复制到quorum节点，然后提交配置变更请求。一旦quorum节点完配置变更日志的追加操作后，便触发leader节点所维护的集群拓扑信息变更（此时原集群拓扑所包含的节点才知道新的集群拓扑结构），而其它节点在收到leader的消息后，也会更新其维护的集群拓扑。 集群配置信息变更在结合代码阐述集群配置信息变更的流程之前，先简单了解论文中所阐述的集群配置变更理论。为什么一次集群配置信息变更（此处以增加新节点示例）只能增加一个节点？这包含两个部分：其一解释若一次增加两个节点会使得集群在某一时刻存在两个leader的情况。其二，阐述若一次只允许增加一个节点，则不会出现某一时刻存在两个两个leader的情况。 如图（论文原图）所示，集群配置变更前集群中节点数量为 3（包括s1、s2及s3），且假设最最初的leader为s3。假设集群配置变更时允许 2 个节点同时加入到集群中，那么原来的 3 个节点在知道新的集群拓扑结构前（即集群配置变更的请求日志被提交之前），它们认为集群中只包含 3 个节点。而当新加入的节点（s4及s5）提出加入请求时，leader节点开始对节点加入请求日志进行同步复制，假设在s1及s2提交日志之前，s3、s4于它们之前收到日志并成功回复，那么leader此时收到了 quorum个回复（s1、s4及s5），因此可以提交节点请求加入的日志，换言之，此时节点s1、s4及s5认为集群中存在 5 个节点，而s2和s3仍然认为集群中只包含 3 个节点（因为它们还未提交配置变更请求）。此时假设某种网络原因，s1与s3（leader节点）失联，则s1重新发起选举，并成功收到s2的回复（s2可以给s1投票的），因此s1成功选举为leader（因为它它认为自己收到了quorum=3/2+1=2节点的投票）。而s3此时也同样发起选举，它可以获得s3、s4及s5的选票，因此它也能成功当选为leader（它认为自己收到了quorum=5/2+1=3节点的投票）。此时，集群中存在两个leader，是不安全且不允许的（显然，两个leader会导致对于同一索引处的日志不同，违反一致性）。 那为什么每次只入一个节点就能保证安全性呢（即任何时刻都只能有一个leader存在）？同样，假设我们最初的集群中包含三个节点（s1、s2及s3），且最初的leader为s1，但此时只有一个节点加入（假设为s4）。那么我们从三个方面来讨论为什么能保证任意时刻只存在一个leader： 配置变更请求日志提交前。即此时原集群的节点（s1、s2及s3）都只知道原始集群拓扑结构信息，不知道新加入的节点信息（其quorum=3/2+1=2）。但新加入的节点认为集群中存在 4 个节点（因此其quorum=4/2+1=3）。因此，在s1、s2或s3当中任意一个或多个发起选举时，它们最多只能产生 1 个leader（与原始集群的选举一致，因为它们的集群拓扑视角均未变化）。而s4发起选举时，它不能得到s1、s2或s3任何一张选票（因为很明显它的日志比它们的要旧）。 配置变更请求日志提交中。即此时配置变更请求的日志已经被leader提交了，但并不是所有的节点都提交了。比如，s1及s2成功提交了日志，则此时若s4发起选举，它不能获取quorum=3/1+1=3张选票，因为它的日志要比s1和s2的要更旧，即只能获取s3的选票（不能成功当选 ），若s3发起选举的结果也类似（注意，其此刻不知道s4的存在，因此其quorum=2）。总而言之，已提交了日志的节点能够获取quorum张选票，而未提交日志的节点因为日志不够新因此不能获得quorum张选票。 配置变更请求日志提交后。这种情况比较简单，当配置变更请求已经提交了，集群中任意一个节点当选的条件必须是获得quorum张选票，且任意两个quorum存在交集，但一个节点只能投出一张选票（在一个term内），因此不可能存在两个节点同时当选为leader。 至此，关于论文中的理论已经阐述完毕。而etcd-raft也只允许一次只能存在一个配置变更的请求。下面来简单了解etcd-raft是如何处理配置变更请求。 关键流程我们同样从raftexample着手，当客户端发起配置变更请求（这里以加入一个新节点作为示例）时，etcd-raft是如何处理的。上文提过，这主要包含两个过程：其一，配置变更请求日志的同步过程（同普通的日志请求复制流程类似）。其二，在日志提交之后，节点正式更新集群拓扑信息，直至此时，原集群中的节点才知道新节点的存在。主要涉及的代码的目录为：/etcd/contribe/raftexample及/etcd/raft。 在阐述配置变更相关流程逻辑前，我们简要帖出核心数据结构，比较简单： 1234567891011121314151617type ConfChange struct &#123; // ID 为节点变更的消息id ID uint64 `protobuf:"varint,1,opt,name=ID" json:"ID"` // 配置信息变更的类型，目前包含四种 Type ConfChangeType `protobuf:"varint,2,opt,name=Type,enum=raftpb.ConfChangeType" json:"Type"` // 配置信息变更所涉及到的节点的 ID NodeID uint64 `protobuf:"varint,3,opt,name=NodeID" json:"NodeID"` Context []byte `protobuf:"bytes,4,opt,name=Context" json:"Context,omitempty"`&#125; // /etc/raft/raftpb/raft.pb.gotype ConfChangeType int32const ( ConfChangeAddNode ConfChangeType = 0 ConfChangeRemoveNode ConfChangeType = 1 ConfChangeUpdateNode ConfChangeType = 2 ConfChangeAddLearnerNode ConfChangeType = 3) // /etc/raft/raftpb/raft.pb.go 我们知道，关于raftexample示例，它可以通过两种方式加入集群，其一是集群节点信息初始化，即在集群启动时便知道存在哪些节点，这不属于我们本文讨论的范围。其二是集群正常运行过程中，一个节点要加入集群，它可以通过向客户端发出一个 HTTP POST 请求以加入集群： 12curl -L http://127.0.0.1:12380/4 -XPOST -d http://127.0.0.1:42379 raftexample --id 4 --cluster http://127.0.0.1:12379,http://127.0.0.1:22379,http://127.0.0.1:32379,http://127.0.0.1:42379 --port 42380 --join 在应用的 HTTP 处理器模块接收到请求后，会构建一个配置变更对象，通过confChangeC管道将其传递给raftNode模块，由raftNode进一步调用node实例的ProposeConfChange()函数。相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839func (h *httpKVAPI) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; key := r.RequestURI switch &#123; case r.Method == "PUT": // ... case r.Method == "GET": // ... case r.Method == "POST": url, err := ioutil.ReadAll(r.Body) // ... // 解析参数 nodeId, err := strconv.ParseUint(key[1:], 0, 64) if err != nil &#123; log.Printf("Failed to convert ID for conf change (%v)\n", err) http.Error(w, "Failed on POST", http.StatusBadRequest) return &#125; // 构建 ConfChang 对象 cc := raftpb.ConfChange&#123; Type: raftpb.ConfChangeAddNode, NodeID: nodeId, Context: url, &#125; // 将对象放入管道，通知 raftNode h.confChangeC &lt;- cc w.WriteHeader(http.StatusNoContent) case r.Method == "DELETE": nodeId, err := strconv.ParseUint(key[1:], 0, 64) // ... cc := raftpb.ConfChange&#123; Type: raftpb.ConfChangeRemoveNode, NodeID: nodeId, &#125; h.confChangeC &lt;- cc // As above, optimistic that raft will apply the conf change w.WriteHeader(http.StatusNoContent) // ... &#125;&#125; // /etcd/contribe/raftexample/httpapi.go 1234567891011121314151617181920212223func (rc *raftNode) serveChannels() &#123; go func() &#123; confChangeCount := uint64(0) for rc.proposeC != nil &amp;&amp; rc.confChangeC != nil &#123; select &#123; case prop, ok := &lt;-rc.proposeC: // ... // 收到客户端的配置变更请求 case cc, ok := &lt;-rc.confChangeC: if !ok &#123; rc.confChangeC = nil &#125; else &#123; confChangeCount++ cc.ID = confChangeCount // 调用底层协议核心来处理配置变更请求（实际上即追加配置变更日志） rc.node.ProposeConfChange(context.TODO(), cc) &#125; &#125; &#125; // client closed channel; shutdown raft if not already close(rc.stopc) &#125;()&#125; // /etcd/contribe/raftexample/raft.go 在底层协议收到此调用请求后，会构建一个MsgProp类型的日志消息（这同普通的日志请求的类型是一致的），但消息中的Entry类型为EntryConfChange。通过一系列的函数调用，会将此请求消息放入proc管道，而在node的run()函数中会将消息从管理中取出，然后调用底层协议的核心处理实例raft的Step()函数，进而在最后调用其stepLeader()函数。部分代码如下（完整的函数调用栈为：ProposeConfChange() -&gt; Step() -&gt; step() -&gt; stepWithWaitOption() -&gt; r.Step() -&gt; r.stepLeader()）： 1234567func (n *node) ProposeConfChange(ctx context.Context, cc pb.ConfChange) error &#123; data, err := cc.Marshal() if err != nil &#123; return err &#125; return n.Step(ctx, pb.Message&#123;Type: pb.MsgProp, Entries: []pb.Entry&#123;&#123;Type: pb.EntryConfChange, Data: data&#125;&#125;&#125;)&#125; // /etcd/raft/node.go 123456789101112131415161718192021222324252627282930313233343536373839404142func stepLeader(r *raft, m pb.Message) error &#123; switch m.Type &#123; case pb.MsgBeat: r.bcastHeartbeat() return nil case pb.MsgCheckQuorum: // ... case pb.MsgProp: // 配置变更请求消息也走这里，因此其处理流程同普通的日志请求是类似的 if len(m.Entries) == 0 &#123; r.logger.Panicf("%x stepped empty MsgProp", r.id) &#125; if _, ok := r.prs[r.id]; !ok &#123; return ErrProposalDropped &#125; if r.leadTransferee != None &#123; r.logger.Debugf("%x [term %d] transfer leadership to %x is in progress; dropping proposal", r.id, r.Term, r.leadTransferee) return ErrProposalDropped &#125; for i, e := range m.Entries &#123; if e.Type == pb.EntryConfChange &#123; // 若为配置变更请求消息，先判断其 pendingConfIndex（它限制了一次只能进行一个节点的变更） // 并且保证其不能超过 appliedIndex，因为只有一个变更请求被 pending，因此其肯定还未提交，因此正常情况下必须小于 appliedIndex if r.pendingConfIndex &gt; r.raftLog.applied &#123; r.logger.Infof("propose conf %s ignored since pending unapplied configuration [index %d, applied %d]", e.String(), r.pendingConfIndex, r.raftLog.applied) m.Entries[i] = pb.Entry&#123;Type: pb.EntryNormal&#125; &#125; else &#123; // 否则，若符合条件，则更新 pendingConfIndex 为对应的索引 r.pendingConfIndex = r.raftLog.lastIndex() + uint64(i) + 1 &#125; &#125; &#125; // 追加配置变更消息到节点的 unstable if !r.appendEntry(m.Entries...) &#123; return ErrProposalDropped &#125; // 广播配置变更消息到 follower 节点 r.bcastAppend() return nil &#125; &#125;&#125; // /etcd/raft/raft.go 关于bcastAppend()之后的逻辑，这里不再重复阐述，其同正常的日志消息的逻辑是一致的。因此，当上层应用调用网络传输组件将配置变更消息转发到集群其它节点时，其它节点同样会完成配置变更日志追加操作（同普通的日志请求消息追加的流程一致），而且leader节点处理响应同样与同步普通日志的响应的逻辑一致，这里也不再重复阐述。 最后，我们来了解当配置变更请求已经被同步到quorum节点后，准备提交的相关逻辑。这包括两个部分：其一是上层应用程序准备应用配置变更请求日志到状态机，然后会触发底层协议正式更新集群拓扑结构信息。 步骤一的相关代码如下（完整调用栈为：serverChannels() -&gt; publishEntries()）： 12345678910111213141516171819202122232425262728293031323334353637383940414243// whether all entries could be published.func (rc *raftNode) publishEntries(ents []raftpb.Entry) bool &#123; for i := range ents &#123; switch ents[i].Type &#123; // 准备应用普通的日志 case raftpb.EntryNormal: // ... // 若为配置变更请求日志 case raftpb.EntryConfChange: var cc raftpb.ConfChange // 1. 反序列化 cc.Unmarshal(ents[i].Data) // 2. 调用 node 的 ApplyConfChange 正式更新对应节点所维护的集群拓扑结构信息 // 即更新 progress 结构信息，这可能包括 learners 信息 // 并且会返回集群的配置信息，即各节点的具体角色 rc.confState = *rc.node.ApplyConfChange(cc) switch cc.Type &#123; // 3. 调用网络传输组件变更对应的代表节点网络传输实例的信息 case raftpb.ConfChangeAddNode: if len(cc.Context) &gt; 0 &#123; rc.transport.AddPeer(types.ID(cc.NodeID), []string&#123;string(cc.Context)&#125;) &#125; case raftpb.ConfChangeRemoveNode: if cc.NodeID == uint64(rc.id) &#123; log.Println("I've been removed from the cluster! Shutting down.") return false &#125; rc.transport.RemovePeer(types.ID(cc.NodeID)) &#125; &#125; // 4. 更新当前已应用的日志索引 rc.appliedIndex = ents[i].Index // special nil commit to signal replay has finished if ents[i].Index == rc.lastIndex &#123; select &#123; case rc.commitC &lt;- nil: case &lt;-rc.stopc: return false &#125; &#125; &#125; return true&#125; // /etcd/contrib/raftexample/raft.go 而底层协议会执行具体的更新集群拓扑（包括更换已有节点的角色）的操作。相关代码如下： 1234567891011121314func (n *node) ApplyConfChange(cc pb.ConfChange) *pb.ConfState &#123; var cs pb.ConfState select &#123; // 将配置变更请求实例放入 confc 管道，n.run() 函数会循环从 confc 管道中取 case n.confc &lt;- cc: case &lt;-n.done: &#125; select &#123; // 从 confstatec 管道中取出集群配置信息实例，返回给上层应用 raftNode case cs = &lt;-n.confstatec: case &lt;-n.done: &#125; return &amp;cs&#125; // /etcd/raft/node.go 12345678910111213141516171819202122232425262728293031323334353637383940func (r *raft) addNode(id uint64) &#123; r.addNodeOrLearnerNode(id, false)&#125; // etcd/raft/raft.gofunc (r *raft) addLearner(id uint64) &#123; r.addNodeOrLearnerNode(id, true)&#125; // etcd/raft/raft.gofunc (r *raft) addNodeOrLearnerNode(id uint64, isLearner bool) &#123; // 1. 获取此新加入节点的 progress 实例 pr := r.getProgress(id) // 2. 若为空，则表示为新加入的节点，设置其 progress 对象信息 if pr == nil &#123; r.setProgress(id, 0, r.raftLog.lastIndex()+1, isLearner) &#125; else &#123; // 3. 否则节点已存在，可能是更新节点的具体的角色 if isLearner &amp;&amp; !pr.IsLearner &#123; // can only change Learner to Voter r.logger.Infof("%x ignored addLearner: do not support changing %x from raft peer to learner.", r.id, id) return &#125; if isLearner == pr.IsLearner &#123; return &#125; // change Learner to Voter, use origin Learner progress // 3.1 考虑从 Learner 切换到 Voter 的角色（Voter 角色的节点保存在 prs 数组） delete(r.learnerPrs, id) pr.IsLearner = false r.prs[id] = pr &#125; // 4. 如果当前节点即为新加入的节点，则设置是否是 Learner if r.id == id &#123; r.isLearner = isLearner &#125; // When a node is first added, we should mark it as recently active. // Otherwise, CheckQuorum may cause us to step down if it is invoked // before the added node has a chance to communicate with us. // 5. 当节点第一次被加入时，需要标记节点最近为 活跃，否则在节点正式与 leader 通信前，可能会导致 leader 节点下台 pr = r.getProgress(id) pr.RecentActive = true&#125; // etcd/raft/raft.go 123456789101112131415161718// 从集群中移除指定节点func (r *raft) removeNode(id uint64) &#123; // 1. 从当前节点维护的其它节点的 progress 对象数组中移除欲删除节点的信息 r.delProgress(id) // do not try to commit or abort transferring if there is no nodes in the cluster. if len(r.prs) == 0 &amp;&amp; len(r.learnerPrs) == 0 &#123; return &#125; // 2. 节点删除操作，更新了 quorum 的大小，因此需要检查是否有 pending 的日志项已经达到提交的条件了 if r.maybeCommit() &#123; // 2.1 若确实提交了日志项，则将此消息进行广播 r.bcastAppend() &#125; // 3. 如果当前被移除的节点是即将当选为 Leader 的节点则中断此 Leader 交接过程 if r.state == StateLeader &amp;&amp; r.leadTransferee == id &#123; r.abortLeaderTransfer() &#125;&#125; // /etcd/raft/raft.go 至此，集群配置信息的变更的相关流程源码已经简单分析完毕。 简单小结，本文主要从两个方面阐述集群配置变更：首先结合论文从理论角度阐述为什么一次集群配置变更只能涉及到单个节点，从正反两个方面进行简单讨论证明。其次，结合etcd-raft中集群配置信息变更的代码具体叙述其中的流程，流程的第一阶段大部分已略过，这同普通日志的提交、追加、同步及响应过程类似，流程的第二阶段为节点执行集群拓扑配置信息的更新过程，直至此时，原集群中的节点，才能感知到新加入节点的存在，因此会更新其quorum。 参考文献 [1]. Ongaro D, Ousterhout J K. In search of an understandable consensus algorithm[C]//USENIX Annual Technical Conference. 2014: 305-319.[2]. https://github.com/etcd-io/etcd]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式协调服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd-raft snapshot实现源码简析]]></title>
    <url>%2F2019%2F01%2F14%2Fetcd-raft-snapshot%E5%AE%9E%E7%8E%B0%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章阐述了etcd-raft存储模块相关逻辑源码，准确而言是与日志存储相关，主要是围绕raftLog、unstable以及Storage/MemoryStorage展开，涉及流程较多，且结合流程逻辑阐述得比较详细。本文主题是snapshot，快照也属于存储的范畴，因此本文内容与上一篇文章存在重叠。不同的是，本文是围绕snapshot展开相关逻辑的分析。具体而言，首先简要介绍snapshot数据结构及重要接口实现，然后重点分析snapshot的全局逻辑（大部分源码已在上篇文章中分析），这主要包括如下四个子问题：其一，leader节点何时执行snapshot同步复制，其二，（应用程序）何时触发snapshot操作及，其三，（应用程序）如何应用snapshot数据，最后，follower节点何时以及如何应用snapshot数据事实上，第一、四两点是从底层协议的角度阐述与snapshot的相关操作，而第二、三点是从应用程序的角度来阐述snapshot相关操作（这其实涵盖了所有节点的操作）。但总的原则不变，目的是从整体上把握snapshot的逻辑，希望读者不要混淆。 需要注意，etcd-raft中关于存储的组件unstable、Storage以及WAL都包含快照，其中前二者的日志项包括快照存储在内存中，WAL将日志项以及快照数据存储在磁盘上。所谓快照实际上表示的是某一时刻系统的状态数据，那么在此时刻之前所保留的日志可以清除，因此它明显具有压缩日志项、节约磁盘空间的作用（在unstable及Storage中仍旧存储在内存）。但WAL与前二者不同，实际上它存储的snapshot数据是指存储它的元数据信息（原因是进行日志重放时，只要从快照元数据记录的日志索引开始即可，在【etcd-raft WAL日志管理源码简析】章节详述），并且每次构建快照数据，它不会覆盖已有的快照数据，而unstable及Storage在更新快照时则会进行替换。另外，etcd-raft还提供一个Snapshotter组件来构建Snapshot数据，它也属于快照数据，而且是增量更新并保存并持久化到磁盘的快照数据目录下的。下面介绍的Snapshot数据结构指的便是此Snapshot类型数据。因为它们相互关联，但作用不同。因此希望读者不要将这几种类型的snapshot混淆，仔细理解每一处的含义。 数据结构Snapshot的数据结构及其相关接口的实现较为简单，大致了解下即可，其中数据结构相关代码的主要目录为/etcd/etcdserver/api/snap/。 SnapshotSnapshot的数据结构如下所示： 123456789101112131415161718type Snapshot struct &#123; Data []byte `protobuf:"bytes,1,opt,name=data" json:"data,omitempty"` Metadata SnapshotMetadata `protobuf:"bytes,2,opt,name=metadata" json:"metadata"`&#125; // raft.pb.gotype SnapshotMetadata struct &#123; ConfState ConfState `protobuf:"bytes,1,opt,name=conf_state,json=confState" json:"conf_state"` // 系统构建快照时，最后一条日志项的索引值 Index uint64 `protobuf:"varint,2,opt,name=index" json:"index"` Term uint64 `protobuf:"varint,3,opt,name=term" json:"term"`&#125; // raft.pb.gotype ConfState struct &#123; // 表示集群中的节点的信息， Nodes 表示 leader及follower的id数组， // 而 Learners 表示集群中 learner 的 id 数组 Nodes []uint64 `protobuf:"varint,1,rep,name=nodes" json:"nodes,omitempty"` Learners []uint64 `protobuf:"varint,2,rep,name=learners" json:"learners,omitempty"`&#125; // raft.pb.go Snapshot的数据结构比较简单。我们下面简单了解其几个关键接口实现，首先是创建快照（文件）： 1234567891011121314151617181920212223242526272829303132// 构建快照文件，应用程序使用此接口来创建持久化的快照文件func (s *Snapshotter) SaveSnap(snapshot raftpb.Snapshot) error &#123; if raft.IsEmptySnap(snapshot) &#123; return nil &#125; return s.save(&amp;snapshot)&#125; // snapshotter.gofunc (s *Snapshotter) save(snapshot *raftpb.Snapshot) error &#123; start := time.Now() // 1. snapshot 文件命令规则：Term-Index.snap fname := fmt.Sprintf("%016x-%016x%s", snapshot.Metadata.Term, snapshot.Metadata.Index, snapSuffix) // 2. 序列化快照结构体数据 b := pbutil.MustMarshal(snapshot) // 3. 生成 crc 检验数据 crc := crc32.Update(0, crcTable, b) // 4. 生成快照 pb 结构数据 snap := snappb.Snapshot&#123;Crc: crc, Data: b&#125; // 5. 序列化 d, err := snap.Marshal() if err != nil &#123; return err &#125; snapMarshallingSec.Observe(time.Since(start).Seconds()) // 6. 构建快照文件路径 spath := filepath.Join(s.dir, fname) fsyncStart := time.Now() // 7. 快照文件存盘 err = pioutil.WriteAndSyncFile(spath, d, 0666) // ... return nil&#125; // snapshotter.go 再简单也解加载快照文件的过程： 12345678910111213141516// 加载快照文件，应用程序使用此接口来加载已存盘的快照文件func (s *Snapshotter) Load() (*raftpb.Snapshot, error) &#123; // 1. 获取快照文件目录下的所有快照谁的，并排序 names, err := s.snapNames() // ... // 2. 遍历快照文件集合，并加载每一个快照文件到内存，形成*raftpb.Snapshot实例 // load 的过程也比较简单，为创建的逆过程，包括反序列化及校验 crc var snap *raftpb.Snapshot for _, name := range names &#123; if snap, err = loadSnap(s.lg, s.dir, name); err == nil &#123; break &#125; &#125; // ... return snap, nil&#125; // snapshotter.go 快照数据结构比较简单，而且其相关接口的实现也比较简单，不多阐述。 关键流程上文提到，snapshot是系统某时刻状态的数据，在etcd-raft中会在多个地方存储snapshot数据，这包括unstable、Storage/MemoryStorage、WAL以及snap日志文件。正是因为涉及到多个存储的结构，因此整个关于snapshot的逻辑也稍显啰嗦。此部分代码的主要目录为：/etcd/raft/、/etcd/contrib/raftexample/。 snapshot 相关逻辑总结大概地，关于unstable日志存储，它与底层协议库直接交互，当leader节点发现follower节点进度过慢时（这也包括节点新加入的情形），会尝试发送MsgSnap，以加快节点状态同步（关于leader如何知道follower节点的日志过旧的原因是leader为每个follower维护了其目前的日志进度视图，这通过progress.go实现）。更准确来说，leader节点在发现本节点的日志过长时（MemoryStorage的实现规则是将长度大于 10000 ），会将更早的日志compact掉以节约内存（这在应用每次收到raft协议核心的Ready通知时，都会检查是否可以触发构建快照）。因此，若leader在给follower节点同步日志时，其可能发现对应的（需要从哪一项日志开始同步）日志项不存在，那么它会认为对应的日志已经被compact掉，因此尝试使用同步快照来代替（即发送MsgSnap消息）。换言之，unstable中的snapshot是来自于leader节点的同步（若follower节点允许直接执行快照同步，会将unstable中的快照直接进行替换）。 而关于Storage日志存储中的快照的来源，则可能来自两处，其一是节点自身主动构建snapshot，即应用程序发现达到快照构建条件时，便触发快照创建，所以这部分快照所对应的数据已存在于节点的应用状态机中，因此也不需要被重放，其主要目的是进行日志压缩。其二是leader节点通过MsgSnap将快照同步到follower节点的unstable中，然后follower的会生成Ready结构并传递给上层应用（里面封装了unstable的snapshot数据），因此最终由follower节点的应用将unstable中的snapshot应用到节点的Storage中。此处的快照的作用使用同步快照数据来代替同步日志项数据，因此减少了网络及 IO 开销，并加速了节点状态的同步。 对比unstable和Storage中快照数据的来源可知，unstable中的快照数据也必须交给上层应用，由上层应用进行WAL持久化、保存snap日志并应用到Storage中。而Storage中的快照数据的另外一个来源则由节点应用层自身直接创建，当然，此时也要作WAL持久化并且记录snap日志。由此可见，unstable与Storage中的日志存储的内容差别较大。另外需要强调的是，WAL日志中的快照部分存储snapshot元信息。而snap的数据存储方法由使用etcd-raft的应用实现，这取决于应用存储的数据类型（在etcd-raft中使用的是Snapshot数据结构来存储）。 综上，基本涵盖了整个关于snapshot流程的逻辑。下面结合代码更详细地阐述各个逻辑，本文将它分为四个方面进行叙述：其一，leader节点何时执行snapshot同步复制，其二，（应用程序）何时触发snapshot操作及，其三，（应用程序）如何应用snapshot数据，最后，follower节点何时以及如何应用snapshot数据。（这四点其实可以串联在一起叙述，但本文还是将它们分开叙述，希望读者能够理清并串联好整个逻辑） leader 节点执行 snapshot 同步复制上文提到当leader节点发现follower节点日志过旧时会使用同步snapshot复制来代替普通的日志同步（即发送MsgSnap而非MsgApp消息），这leadaer节点之所以能够发现follower节点的日志进度过慢的原因是，它使用为此follower节点保存的当前已同步日志索引来获取其unstable（也包括Storage）中的日志项（集合）时，发现不能成功获取对应的日志项，由此说明对应的日志项已经被compact掉了，即已经创建了快照。（关于如何创建快照，在下小节详述），因此，leader节点会向follower节点发送MsgSnap消息。相关代码及部分关键注释如下（下面只展示了关键函数的代码，整个流程为：stepLeader() -&gt; bcastAppend() -&gt; sendAppend() -&gt; maybeSendAppend()）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445func (r *raft) maybeSendAppend(to uint64, sendIfEmpty bool) bool &#123; // 1. 获取 id 为 to 的 follower 的日志同步进度视图（具体查看 progress.go） pr := r.getProgress(to) // 2. 若对应 follower 节点未停止接收消息（停止的原因可能是在执行一个耗时操作，如应用快照数据） if pr.IsPaused() &#123; return false &#125; // 3. 构建消息实例 m := pb.Message&#123;&#125; m.To = to // 4. 通过 Next(为节点维护的下一个需要同步的日志索引)查找对应的 term 及 ents // 注意：1. maxMsgSize 是作为控制最大的传输日志项数量 // 2. 其在查找对应的 term 及 ents 时，也会查找 Storage 中的日志项集合 term, errt := r.raftLog.term(pr.Next - 1) ents, erre := r.raftLog.entries(pr.Next, r.maxMsgSize) if len(ents) == 0 &amp;&amp; !sendIfEmpty &#123; return false &#125; // 5. 如果查找失败，则考虑发送 MsgSnap 消息 if errt != nil || erre != nil &#123; // send snapshot if we failed to get term or entries // 此处记录对应节点最近是活跃 if !pr.RecentActive &#123; r.logger.Debugf("ignore sending snapshot to %x since it is not recently active", to) return false &#125; m.Type = pb.MsgSnap // 5.1 通过 raftLog 获取 snapshot 数据，若 unstable 中没有，则从 Storage 中获取 snapshot, err := r.raftLog.snapshot() // ... m.Snapshot = snapshot sindex, sterm := snapshot.Metadata.Index, snapshot.Metadata.Term r.logger.Debugf("%x [firstindex: %d, commit: %d] sent snapshot[index: %d, term: %d] to %x [%s]", r.id, r.raftLog.firstIndex(), r.raftLog.committed, sindex, sterm, to, pr) // 5.2 更新对应节点的 progress 实例对象 pr.becomeSnapshot(sindex) r.logger.Debugf("%x paused sending replication messages to %x [%s]", r.id, to, pr) &#125; else &#123; // 6. 否则进行日志同步，即发送正常的 MsgApp 消息 m.Type = pb.MsgApp // ... &#125; // 此处会将此消息打包到 raft.msgs 中，进一步会由 node 将其打包到 Ready 结构中，并转发给上层应用程序， // 由应用程序调用启用网络传输的组件，将消息发送出去（在上一篇文章中已经详述） r.send(m) return true&#125; // /etcd/raft/raft.go 何时触发 snapshot 操作事实上，触发snapshot操作是由上层应用程序完成的（并非底层raft协议核心库的功能）。触发构建快照的规则是：Storage中的日志条目的数量大于 10000，一旦达到此条件，则会将日志项索引不在过去 10000 条索引范围内的日志执行compact操作，并创建对应的快照数据，记录到WAL日志文件，以及snap快照文件中。相关代码及部分关键注释如下（下面只展示了关键函数的代码，整个流程为：startRaft() -&gt; serveChannels() -&gt; maybeTriggerSnapshot()）： 12345678910111213141516171819202122232425262728293031323334353637// 针对 memoryStorage 触发快照操作（如果满足条件）（注意这是对 memoryStorage 中保存的日志信息作快照）func (rc *raftNode) maybeTriggerSnapshot() &#123; // 0. 判断是否达到创建快照（compact）的条件 if rc.appliedIndex-rc.snapshotIndex &lt;= rc.snapCount &#123; return &#125; log.Printf("start snapshot [applied index: %d | last snapshot index: %d]", rc.appliedIndex, rc.snapshotIndex) // 1. 加载状态机中当前的状态数据（此方法由应用程序提供，在 kvstore 中） data, err := rc.getSnapshot() if err != nil &#123; log.Panic(err) &#125; // 2. 利用上述快照数据、以及 appliedIndex 等为 memoryStorage 实例创建快照（它会覆盖/更新 memoryStorage 已有的快照信息） snap, err := rc.raftStorage.CreateSnapshot(rc.appliedIndex, &amp;rc.confState, data) if err != nil &#123; panic(err) &#125; // 3. 保存快照到 WAL 日志（快照的索引/元数据信息） // 以及到 snap 日志文件中（它包含所有信息，一般而言，此 snap 结构由应用程序决定，etcd-raft 的实现包含了元数据及实际数据） if err := rc.saveSnap(snap); err != nil &#123; panic(err) &#125; // 4. 如果满足日志被 compact 的条件（防止内存中的日志项过多），则对内存中的日志项集合作 compact 操作 // compact 操作会丢弃 memoryStorage 日志项集中 compactIndex 之前的日志 compactIndex := uint64(1) if rc.appliedIndex &gt; snapshotCatchUpEntriesN &#123; compactIndex = rc.appliedIndex - snapshotCatchUpEntriesN &#125; if err := rc.raftStorage.Compact(compactIndex); err != nil &#123; panic(err) &#125; log.Printf("compacted log at index %d", compactIndex) // 5. 更新应用程序的快照位置(进度)信息 rc.snapshotIndex = rc.appliedIndex&#125; // /etcd/contrib/raftexample/raft.go 如何应用 snapshot 数据本小节所涉及的如何应用snapshot数据亦是针对应用程序而言（因为follower节点的unstable也会由协议库来应用leader节点发送的快照数据）。大概地，应用程序应用快照数据包含两个方面：其一，在节点刚启动时（宕机后重启）会进行日志重放，因此在重放过程中，若快照数据不为空（由snap存盘的快照数据，包括元信息及实际数据），则加载快照数据，并将其应用到Storage的快照中，而且会重放快照数据后的WAL日志项数据，并将其追加到Storage的日志项集。如此以来，节点便能重构其状态数据。其相关代码如下（实际完整调用为：startRaft() -&gt; serveChannels()）： 12345678910111213141516171819202122232425262728293031323334func (rc *raftNode) replayWAL() *wal.WAL &#123; log.Printf("replaying WAL of member %d", rc.id) // 1. 从 snap 快照文件中加载 快照数据（包含元信息及实际数据） snapshot := rc.loadSnapshot() // 2. 从指定日志索引位置打开 WAL 日志，以准备读取快照之后的日志项 w := rc.openWAL(snapshot) // 3. 读取指定索引位置后的所有日志 _, st, ents, err := w.ReadAll() if err != nil &#123; log.Fatalf("raftexample: failed to read WAL (%v)", err) &#125; // 4. 应用程序创建一个 MemoryStorage 实例 rc.raftStorage = raft.NewMemoryStorage() // 5. 若快照数据不为空，则将快照数据应用到 memoryStorage 中，替换掉已有的 snapshot 实例 if snapshot != nil &#123; rc.raftStorage.ApplySnapshot(*snapshot) &#125; // 6. 设置 HardState 到 memoryStorage 实例 rc.raftStorage.SetHardState(st) // append to storage so raft starts at the right place in log // 7. 将 WAL 重放的日志项集追加到 memoryStorage 实例（显然，此日志项不包含已经快照的日志项） rc.raftStorage.Append(ents) // send nil once lastIndex is published so client knows commit channel is current if len(ents) &gt; 0 &#123; // 8. 如果在快照后，仍存在日志项记录，则设置 lastIndex rc.lastIndex = ents[len(ents)-1].Index &#125; else &#123; // 9. 通知 kvstore，日志重放已经完毕，因此 kvstore 状态机也会从 snap 快照文件中加载数据 // 参见下面的代码片段 rc.commitC &lt;- nil &#125; return w&#125; // /etcd/raftexample/raft.go 12345678910111213141516171819202122232425262728293031323334353637383940func (s *kvstore) readCommits(commitC &lt;-chan *string, errorC &lt;-chan error) &#123; // raftNode 会将日志项 或 nil 放入 commitC 管道 for data := range commitC &#123; if data == nil &#123; // done replaying log; new data incoming // OR signaled to load snapshot // 从 snap 快照文件中加载数据，这包括两种情形： // 一是重启时重放日志， // 二是当 leader 向 follower 同步 snapshot 数据时，节点会将其应用到 unstable 及 Storage 中，同样会保存到 WAL 及 snap 文件 // 因此让状态机重新加载 snap 快照数据 snapshot, err := s.snapshotter.Load() // ... if err := s.recoverFromSnapshot(snapshot.Data); err != nil &#123; log.Panic(err) &#125; continue &#125; // 有新的数据已经被提交，因此将其应用到状态机中 var dataKv kv dec := gob.NewDecoder(bytes.NewBufferString(*data)) if err := dec.Decode(&amp;dataKv); err != nil &#123; log.Fatalf("raftexample: could not decode message (%v)", err) &#125; s.mu.Lock() s.kvStore[dataKv.Key] = dataKv.Val s.mu.Unlock() &#125;// ...&#125; // /etcd/raftexample/kvstore.gofunc (s *kvstore) recoverFromSnapshot(snapshot []byte) error &#123; var store map[string]string if err := json.Unmarshal(snapshot, &amp;store); err != nil &#123; return err &#125; s.mu.Lock() s.kvStore = store s.mu.Unlock() return nil&#125; // /etcd/raftexample/kvstore.go 其二，当leader节点同步快照数据给follower节点时，协议库会将快照数据应用到unstable（如果合法的话），然后，将Ready实例返回给应用程序，应用程序会检测到Ready结构中包含快照数据，因此，会将快照数据应用到Storage中。其相关代码如下（实际完整调用为：startRaft() -&gt; serveChannels()）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func (rc *raftNode) serveChannels() &#123; // 节点刚启动时，通过加载 snap 快照 &amp;&amp; 重放 WAL 日志，以将其应用到 memoryStorage 中 // 因此可以从 memoryStorage 中取出相关数据 snap, err := rc.raftStorage.Snapshot() rc.confState = snap.Metadata.ConfState rc.snapshotIndex = snap.Metadata.Index rc.appliedIndex = snap.Metadata.Index // ... go func() &#123; // ... // 应用程序状态机更新的事件循环，即循环等待底层协议库的 Ready 通知 for &#123; select &#123; case &lt;-ticker.C: rc.node.Tick() // 1. 收到底层协议库的 Ready 通知，关于 Ready 的结构已经在介绍 raftexample 文章中简要介绍 case rd := &lt;-rc.node.Ready(): // 2. 先将 Ready 中需要被持久化的数据保存到 WAL 日志文件（在消息转发前） rc.wal.Save(rd.HardState, rd.Entries) // 3. 如果 Ready 中的需要被持久化的快照不为空 // 此部分快照数据的来源是 leader 节点通过 MsgSnap 消息同步给 follower 节点 if !raft.IsEmptySnap(rd.Snapshot) &#123; // 3.1 保存快照到 WAL 日志（快照的索引/元数据信息）以及到 // snap 日志文件中（由应用程序来实现 snap 的数据结构，etcd-raft 的实现包含了快照的元信息及实际数据） // snap 日志文件会作为 状态机 (kvstore) 加载快照数据的来源（重启时加载，以及快照更新时重新加载） rc.saveSnap(rd.Snapshot) // 3.2 将快照应用到 memoryStorage 实例，替换掉其 snapshot 实例 rc.raftStorage.ApplySnapshot(rd.Snapshot) // 3.3 更新应用程序保存的快照信息 // 这包括更新 snapshotIndex、appliedIndex以及confState // 另外，还会通知 kvstore 重新加载 snap 文件的快照数据 rc.publishSnapshot(rd.Snapshot) &#125; // 4. 追加 Ready 结构中需要被持久化的信息（在消息转发前） rc.raftStorage.Append(rd.Entries) // 5. 转发 Ready 结构中的消息 rc.transport.Send(rd.Messages) // 6. 将日志应用到状态机（如果存在已经提交，即准备应用的日志项） // 会更新 appliedIndex if ok := rc.publishEntries(rc.entriesToApply(rd.CommittedEntries)); !ok &#123; rc.stop() return &#125; // 7. 触发快照操作（如果满足条件） rc.maybeTriggerSnapshot() // 8. 通知底层 raft 协议库实例 node，即告知当前 Ready 已经处理完毕，可以准备下一个 rc.node.Advance() // ... &#125; &#125;&#125; // /etcd/contrib/raftexample/raft.go follower 节点何时以及如何应用 snapshot 数据 最后，我们来简单了解follower节点收到leader节点的MsgSnap消息时，如何应用snapshot数据。其大致的逻辑为：当follower节点收到MsgSnap消息时，会判断此快照是否合法，若合法，则将共应用到unstable，并且更新相关的记录索引（如offset等），返回快照应用成功的消息。否则，返回快照已应用的消息（事实上回复消息没有明显区分应用失败还是成功，实际上是以lastIndex及commited来区分，这足以使得leader节点获悉follower节点日志进度）。同时follower节点还会更新集群的拓扑结构信息。再提醒一次，其最后调用的send()函数使得节点的上层应用程序将snapshot应用到Storage，并且作WAL日志以及snap快照。其相关代码为（实际完整调用为：stepFollower() -&gt; handleSnapshot() -&gt; restore()）： 12345678910111213141516func (r *raft) handleSnapshot(m pb.Message) &#123; sindex, sterm := m.Snapshot.Metadata.Index, m.Snapshot.Metadata.Term // 1. 若应用成功，则发送当前 raftLog 中（包括 unstable 及 Storage）的最后一项日志（之前的日志已作为快照数据存储） if r.restore(m.Snapshot) &#123; r.logger.Infof("%x [commit: %d] restored snapshot [index: %d, term: %d]", r.id, r.raftLog.committed, sindex, sterm) // 1.1 此 send 函数会将消息最终放入 Ready 结构中，node 会将 Ready 实例进行打包，以发送给节点上层应用程序 // 上层应用程序收到 Ready 通知后，检查到此消息中包含 snapshot 数据，则应用到 Storage，并作 WAL日志以及 snap 快照记录 r.send(pb.Message&#123;To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.lastIndex()&#125;) &#125; else &#123; // 1.2 否则说明此快照数据已应用，则发送目前已提交的日志项的索引给 leader r.logger.Infof("%x [commit: %d] ignored snapshot [index: %d, term: %d]", r.id, r.raftLog.committed, sindex, sterm) r.send(pb.Message&#123;To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.committed&#125;) &#125;&#125; // /etcd/raft/raft.go 12345678910111213141516171819202122232425262728293031323334353637func (r *raft) restore(s pb.Snapshot) bool &#123; // 1. 若快照消息中的快照索引小于已提交的日志项的日志索引，则不能应用此快照（之前已应用） if s.Metadata.Index &lt;= r.raftLog.committed &#123; return false &#125; // 2. 否则，若此索引与任期匹配 if r.raftLog.matchTerm(s.Metadata.Index, s.Metadata.Term) &#123; r.logger.Infof("%x [commit: %d, lastindex: %d, lastterm: %d] fast-forwarded commit to snapshot [index: %d, term: %d]", r.id, r.raftLog.committed, r.raftLog.lastIndex(), r.raftLog.lastTerm(), s.Metadata.Index, s.Metadata.Term) // 2.1 则更新 raftLog 中的 commited 字段，因为 committed 之前的日志代表已经提交 r.raftLog.commitTo(s.Metadata.Index) return false &#125; // The normal peer can't become learner. // 3. 这里是更新当前节点的集群的拓扑结构信息，即集群中包含哪些节点，它们各自的角色是什么 if !r.isLearner &#123; for _, id := range s.Metadata.ConfState.Learners &#123; if id == r.id &#123; r.logger.Errorf("%x can't become learner when restores snapshot [index: %d, term: %d]", r.id, s.Metadata.Index, s.Metadata.Term) return false &#125; &#125; &#125; r.logger.Infof("%x [commit: %d, lastindex: %d, lastterm: %d] starts to restore snapshot [index: %d, term: %d]", r.id, r.raftLog.committed, r.raftLog.lastIndex(), r.raftLog.lastTerm(), s.Metadata.Index, s.Metadata.Term) // 4. 更新 raftLog 的 commited 为快照消息中的索引，以及更换 unstable 中的 snapshot 实例为快照消息中的快照实例 // 并更新 unstable 的 offset 为 快照消息索引+1，更新 ents 字段为空 r.raftLog.restore(s) // 5. 以下同样是重构节点的拓扑信息 r.prs = make(map[uint64]*Progress) r.learnerPrs = make(map[uint64]*Progress) r.restoreNode(s.Metadata.ConfState.Nodes, false) r.restoreNode(s.Metadata.ConfState.Learners, true) return true&#125; // /etcd/raft/raft.go 至此，关于snapshot的逻辑已经阐述完毕。 简单小结，本文先是简单介绍了Snapshot的数据结构及接口实现（该Snapshot为重启的快照数据加载来源，并配合WAL日志重放记录，以重构节点宕机前的状态），然后围绕unstable及Storage总结了关于snapshot的流程逻辑，以在总体上把握snapshot的核心设计流程。最后，结合代码分析从四个方面梳理snapshot的相关流程，目的是加深读者对整个系统中如何使用snapshot的印象，并且需要理解为何如此设计。 参考文献 [1]. https://github.com/etcd-io/etcd[2]. etcd-raft snapshot实现分析]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式系统协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>snapshot管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd-raft 存储模块源码简析]]></title>
    <url>%2F2019%2F01%2F12%2Fetcd-raft-%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9D%97%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章简单分析了etcd-raft WAL日志管理模块相关的源码。文章集中在WAL库提供的相关接口的阐述，而未将其与raft协议核心库关联起来，即尚未阐述raft协议核心库如何使用WAL日志库，并且上一篇文章虽然是以应用程序使用WAL库为切入点分析，但并没有阐述清楚WAL、Storage以及unstable三者的关联，鉴于三者提供日志存储的功能。本文的重点是分析etcd-raft 存储模块，它包括Storage及其实现memoryStorage（etcd为应用程序提供的一个Storage实现的范例）、unstable以及raftLog三个核心数据结构。另外，阐述Storage同应用程序的交互的细节以及raft协议库与raftLog的交互相关的逻辑，后者包括raftLog重要接口的实现，以及raft协议库的一个典型的简单的日志追加流程（即从leader追加日志，然后广播给follower节点，然后follower节点同样进行日志项的追加，最后leader节点处理follower节点的响应各个环节中日志追加的具体逻辑）。 （需要提醒的是，整篇文章较长，因此读者可以选择部分小节进行针对性参考，每个小节的最开始都有概括该小节的内容，各小节的分析是独立进行的。）同样我们先重点了解几个数据结构，主要包括raftLog、unstable以及Storage &amp; MemoryStorage，读者可以深入源码文件仔细查看相关字段及逻辑（主要涉及的目录/etcd/raft/，也有示例应用的部分代码/etcd/contrib/raftexample）。通过了解相关数据结构，就能大概推测出其相关功能。 数据结构raftLograftLog为raft协议核心处理日志复制提供接口，raft协议库对日志的操作都基于raftLog实施。换言之，协议核心库不会直接同Storage及WAL直接交互。raftLog的数据结构如下： 12345678910111213141516type raftLog struct &#123; // 包含从上一次快照以来的所有已被持久化的日志项集合 storage Storage // 包含所有未被持久化（一旦宕机便丢失）的日志项集合及快照 // 它们会被持久化到 storage unstable unstable // 已被持久化的最高的日志项的索引编号 committed uint64 // 已经被应用程序应用到状态机的最高手日志项索引编号 // 必须保证： applied &lt;= committed applied uint64 logger Logger // 调用 nextEnts 时，返回的日志项集合的最大的大小 // nextEnts 函数返回应用程序已经可以应用到状态机的日志项集合 maxNextEntsSize uint64&#125; // log.go 关于storage及unstable两个数据我们暂时不知道其具体作用，比如它们是如何被raftLog使用的，它们的区别是什么？我们先继续了解这两个数据结构的内容。 unstableunstable顾名思义，表示非持久化的存储。其数据结构如下： 1234567891011121314// unstable.entries[i] 存储的日志的索引为 i+unstable.offset// 另外，unstable.offset 可能会小于 storage.entries 中的最大的索引// 此时，当继续向 storage 同步日志时，需要先截断其大于 unstable.offset 的部分type unstable struct &#123; // the incoming unstable snapshot, if any. // unstable 包含的快照数据 snapshot *pb.Snapshot // 所有未被写入 storage 的日志 entries []pb.Entry // entries 日志集合中起始的日志项编号 offset uint64 logger Logger&#125; // log_unstable.go Storage &amp; MemoryStorageStorage表示etcd-raft提供的持久化存储的接口。应用程序负责实现此接口，以将日志信息落盘。并且，若在操作过程此持久化存储时出现错误，则应用程序应该停止对相应的 raft 实例的操作，并需要执行清理或恢复的操作。其数据结构如下： 12345678910111213141516171819202122232425// Storage 接口需由应用程序来实现，以从存储中以出日志信息// 如果在操作过程中出现错误，则应用程序应该停止对相应的 raft 实例的操作，并需要执行清理或恢复的操作type Storage interface &#123; // 返回 HardState 及 ConfState 数据 InitialState() (pb.HardState, pb.ConfState, error) // 返回 [lo, hi) 范围的日志项集合 Entries(lo, hi, maxSize uint64) ([]pb.Entry, error) // 返回指定日志项索引的 term Term(i uint64) (uint64, error) // 返回日志项中最后一条日志的索引编号 LastIndex() (uint64, error) // 返回日志项中最后第一条日志的索引编号，注意在其被创建时，日志项集合会被填充一项 dummy entry FirstIndex() (uint64, error) // 返回最近一次的快照数据，如果快照不可用，则返回出错 Snapshot() (pb.Snapshot, error)&#125; // storage.go// MemoryStorage 实现了 Storage 接口，注意 MemoryStorage 也是基于内存的type MemoryStorage struct &#123; sync.Mutex hardState pb.HardState snapshot pb.Snapshot // ents[i] 存储的日志项的编号为 i+snapshot.Metadata.Index，即要把快照考虑在内 ents []pb.Entry&#125; // storage.go 关键流程从上述数据结构中发现raftLog封装了storage及unstable。而且大概看一下raftLog中各个接口，发现主要不是同unstable进行交互（也有利用storage的数据）。所以，我们决定从两个方面来明晰主几个数据结构的作用。包括应用程序与Storage交互，以及raft协议核心同raftLog(unstable/storage)交互。希望通过从具体功能实现切入来摸索梳理相关逻辑，并结合数据结构，以达到由外至里尽可能把握其设计原理的效果。 应用程序与 Storage 交互为了让读者有更好的理解，本文仍旧从raftexample中的startRaft()开始追溯与上述三个数据结构相关的逻辑，以明晰它们三者的作用。我们从两个方面来阐述交互的大致逻辑，包括应用程序启动（此时raft实例也会被初始化）以及上层应用收到底层raft协议核心的通知(Ready)时所执行的相关操作。 应用初始化首先来看第一个：在startRaft()函数中，我们先深入日志重放代码rc.wal = rc.replayWAL()： 123456789101112131415161718192021222324252627282930313233// 重放 WAL 日志到 raft 实例func (rc *raftNode) replayWAL() *wal.WAL &#123; log.Printf("replaying WAL of member %d", rc.id) // 1. 从持久化存储中加载 快照数据 snapshot := rc.loadSnapshot() // 2. 从指定日志索引位置打开 WAL 日志，以准备读取日志 w := rc.openWAL(snapshot) // 3. 读取指定索引位置后的所有日志 _, st, ents, err := w.ReadAll() if err != nil &#123; log.Fatalf("raftexample: failed to read WAL (%v)", err) &#125; // 4. 应用程序创建一个 MemoryStorage 实例 rc.raftStorage = raft.NewMemoryStorage() // 5. 若快照数据不为空，则将快照数据应用到 memoryStorage 中 if snapshot != nil &#123; rc.raftStorage.ApplySnapshot(*snapshot) &#125; // 6. 设置 HardState 到 memoryStorage 实例 rc.raftStorage.SetHardState(st) // append to storage so raft starts at the right place in log // 7. 将日志项追加到 memoryStorage 实例，注意，此日志项不包含已经快照的日志项 rc.raftStorage.Append(ents) // send nil once lastIndex is published so client knows commit channel is current if len(ents) &gt; 0 &#123; // 8. 如果在快照后，仍存在日志项记录，则设置 lastIndex rc.lastIndex = ents[len(ents)-1].Index &#125; else &#123; // 9. 通知 kvstore，日志重放已经完毕 rc.commitC &lt;- nil &#125; return w&#125; // raft.go 我们重点关注与memoryStorage相关的逻辑。步骤 4 创建了一个memoryStorage实例，创建逻辑也比较简单： 1234567// NewMemoryStorage creates an empty MemoryStorage.func NewMemoryStorage() *MemoryStorage &#123; return &amp;MemoryStorage&#123; // When starting from scratch populate the list with a dummy entry at term zero. ents: make([]pb.Entry, 1), &#125;&#125; // storage.go 而步骤 5 将快照数据应用到了memoryStorage实例，其逻辑也较为简单： 12345678910111213141516// ApplySnapshot overwrites the contents of this Storage object with// those of the given snapshot.func (ms *MemoryStorage) ApplySnapshot(snap pb.Snapshot) error &#123; ms.Lock() defer ms.Unlock() //handle check for old snapshot being applied msIndex := ms.snapshot.Metadata.Index snapIndex := snap.Metadata.Index if msIndex &gt;= snapIndex &#123; return ErrSnapOutOfDate &#125; ms.snapshot = snap ms.ents = []pb.Entry&#123;&#123;Term: snap.Metadata.Term, Index: snap.Metadata.Index&#125;&#125; return nil&#125; // storage.go 从代码可以看出，其只是将快照直接进行替换，并将快照的当前索引及任期存入日志项集合。而步骤 6 较为简单，在此略过。简单了解一下步骤 7，它往memoryStorage的日志项集合中追加日志项集合，其代码如下： 12345678910111213141516171819202122232425262728293031323334// 新追加的日志项必须是连续的，且 entries[0].Index &gt; ms.entries[0].Indexfunc (ms *MemoryStorage) Append(entries []pb.Entry) error &#123; if len(entries) == 0 &#123; return nil &#125; ms.Lock() defer ms.Unlock() first := ms.firstIndex() last := entries[0].Index + uint64(len(entries)) - 1 // shortcut if there is no new entry. if last &lt; first &#123; return nil &#125; // truncate compacted entries // 若已有的 ms.ents 被 compact 了，则新追加的日志项集有可能为被 compact 掉中的一部分 // 因此，需要将那一部进行移除，以免重复追加 if first &gt; entries[0].Index &#123; entries = entries[first-entries[0].Index:] &#125; // 判断新追加日志与已有日志是否有重叠，若是，则需要覆盖已有日志，否则直接追加到已有日志后面 offset := entries[0].Index - ms.ents[0].Index switch &#123; case uint64(len(ms.ents)) &gt; offset: ms.ents = append([]pb.Entry&#123;&#125;, ms.ents[:offset]...) ms.ents = append(ms.ents, entries...) case uint64(len(ms.ents)) == offset: ms.ents = append(ms.ents, entries...) default: raftLogger.Panicf("missing log entry [last: %d, append at: %d]", ms.lastIndex(), entries[0].Index) &#125; return nil&#125; // storage.go 日志追加流程基本符合逻辑，但需要注意如果已有日志项集合被compact，且追加的日志与已有日志重叠的情况。关于日志项被compact的相关逻辑，后面会叙述。现在作一个小结，上述逻辑发生在应用启动初始化时机，换言之，这包括两种情况，其一是整个集群刚启动，应用程序所在的节点没有任何持久化的快照记录；其二是此节点宕机，并且错过了部分日志的追加与快照操作，因此，应用程序需要恢复此节点对应的raft实例的memoryStorge信息以及增加快照数据（节点新加入时，也大致符合这种情况）。换言之，在有节点落后、刚重启、新加入的情况下，给这些节点的数据多数来自已落盘部分（持久化的快照及WAL日志）。 处理 raft 协议库 Ready 消息接下来，继续了解第二处交互逻辑：在serverChannels()函数中，应用等待接收底层raft协议库的通知： 123456789101112131415161718192021222324252627282930313233343536// 应用程序状态机更新的事件循环，即循环等待底层协议库的 Ready 通知for &#123; select &#123; case &lt;-ticker.C: rc.node.Tick() // store raft entries to wal, then publish over commit channel // 1. 收到底层协议库的 Ready 通知，关于 Ready 结构已经在介绍 raftexample 文章中简要介绍 case rd := &lt;-rc.node.Ready(): // 2. 先将 Ready 中需要被持久化的数据保存到 WAL 日志文件（在消息转发前） rc.wal.Save(rd.HardState, rd.Entries) // 3. 如果 Ready 中的需要被持久化的快照不为空 if !raft.IsEmptySnap(rd.Snapshot) &#123; // 3.1 保存快照到 WAL 日志（快照索引/元数据信息）以及到 snap (后面文章会介绍)中 rc.saveSnap(rd.Snapshot) // 3.2 将快照应用到 memoryStorage 实例 rc.raftStorage.ApplySnapshot(rd.Snapshot) // 3.3 更新应用程序保存的快照信息 rc.publishSnapshot(rd.Snapshot) &#125; // 4. 追加 Ready 结构中需要被持久化的信息（在消息转发前） rc.raftStorage.Append(rd.Entries) // 5. 转发 Ready 结构中的消息 rc.transport.Send(rd.Messages) // 6. 将日志应用到状态机（如果存在已经提交，即准备应用的日志项） if ok := rc.publishEntries(rc.entriesToApply(rd.CommittedEntries)); !ok &#123; rc.stop() return &#125; // 7. 触发快照操作（如果满足条件） rc.maybeTriggerSnapshot() // 8. 通知底层 raft 协议库实例 node，即告知当前 Ready 已经处理完毕，可以准备下一个 rc.node.Advance() // ... &#125;&#125; // raft.go 同样，重点关注与memoryStorage相关的逻辑（其余的逻辑在【etcd raftexample 源码简析】中已阐述）。在步骤 3 中，当Ready结构中的快照不为空时，需要保存快照至一系列地方。其中步骤 3.1 的调用代码如下： 1234567891011121314151617func (rc *raftNode) saveSnap(snap raftpb.Snapshot) error &#123; // must save the snapshot index to the WAL before saving the // snapshot to maintain the invariant that we only Open the // wal at previously-saved snapshot indexes. walSnap := walpb.Snapshot&#123; // 1. 构建快照索引（在【WAL 日志管理源码解析】文章有阐述）信息 Index: snap.Metadata.Index, Term: snap.Metadata.Term, &#125; // 2. 保存快照索引信息到 WAL 日志 if err := rc.wal.SaveSnapshot(walSnap); err != nil &#123; return err &#125; // 3. 保存快照完整数据到 snap（后面文章阐述） if err := rc.snapshotter.SaveSnap(snap); err != nil &#123; return err &#125; // 4. 更新 WAL 日志文件锁范围 return rc.wal.ReleaseLockTo(snap.Metadata.Index)&#125; // raft.go 而步骤 3.2 在上文已阐述过，即将快照替换到memoryStorage关联的快照实例。而最后 3.3 的相关代码如下： 1234567891011121314151617// 更新应用程序保存的快照位置信息，并且通知上层应用(kvstore)可以重新加载快照func (rc *raftNode) publishSnapshot(snapshotToSave raftpb.Snapshot) &#123; if raft.IsEmptySnap(snapshotToSave) &#123; return &#125; log.Printf("publishing snapshot at index %d", rc.snapshotIndex) defer log.Printf("finished publishing snapshot at index %d", rc.snapshotIndex) // 1. 检验快照数据 if snapshotToSave.Metadata.Index &lt;= rc.appliedIndex &#123; log.Fatalf("snapshot index [%d] should &gt; progress.appliedIndex [%d]", snapshotToSave.Metadata.Index, rc.appliedIndex) &#125; // 2. 通知上层应用(kvstore)可以重新加载快照 rc.commitC &lt;- nil // trigger kvstore to load snapshot // 3. 更新应用程序(raftNode)保存的快照位置信息，以及当前已应用到状态机的日志的索引信息 rc.confState = snapshotToSave.Metadata.ConfState rc.snapshotIndex = snapshotToSave.Metadata.Index rc.appliedIndex = snapshotToSave.Metadata.Index&#125; // raft.go 小结步骤 3 逻辑（包括 3.1-3.3）：若底层协议传来的Ready结构中包含的快照不为空，则首先将快照保存到WAL日志（索引信息），并保存完整快照信息到snap，然后将快照替换掉内存(memoryStorage)关联的快照实例，最后更新应用保存的快照位置信息及当前已应用日志位置信息，并触发应用（状态机）重新加载快照。 同样，步骤 4 已在上文阐述过。 123456789101112131415161718192021222324252627282930313233343536// 针对 memoryStorage 触发快照操作（如果满足条件）//（注意这是对 memoryStorage 中保存的日志信息作快照）func (rc *raftNode) maybeTriggerSnapshot() &#123; if rc.appliedIndex-rc.snapshotIndex &lt;= rc.snapCount &#123; return &#125; log.Printf("start snapshot [applied index: %d | last snapshot index: %d]", rc.appliedIndex, rc.snapshotIndex) // 1. 加载状态机中当前的信息（此方法由应用程序提供，在 kvstore 中） data, err := rc.getSnapshot() if err != nil &#123; log.Panic(err) &#125; // 2. 利用上述快照数据、以及 appliedIndex 等为 memoryStorage 实例创建快照（它会覆盖/更新 memoryStorage 已有的快照信息） snap, err := rc.raftStorage.CreateSnapshot(rc.appliedIndex, &amp;rc.confState, data) if err != nil &#123; panic(err) &#125; // 3. 保存快照到 WAL 日志（快照的索引/元数据信息）以及到 snap（后面文章会介绍）中 if err := rc.saveSnap(snap); err != nil &#123; panic(err) &#125;// 4. 若满足日志被 compact 的条件（防止内存中日志项过多），则对内存中日志项集合作 compact 操作 // compact 操作会丢弃 memoryStorage 日志项中 compactIndex 之前的日志 compactIndex := uint64(1) if rc.appliedIndex &gt; snapshotCatchUpEntriesN &#123; compactIndex = rc.appliedIndex - snapshotCatchUpEntriesN &#125; if err := rc.raftStorage.Compact(compactIndex); err != nil &#123; panic(err) &#125; log.Printf("compacted log at index %d", compactIndex) // 5. 更新应用程序的快照位置信息 rc.snapshotIndex = rc.appliedIndex&#125; // raft.go 上述代码逻辑比较简单，简单而言，它会从状态机中加载快照，然后覆盖raft实例关联的memoryStorage中的快照实例，而且，还会保存快照信息（上文已阐述），最后检查memoryStorage是否可以执行compact操作。其中memoryStorage的compact操作的逻辑也比较简单，即丢弃compactIndex之前的日志（注意：并不是丢弃 appliedIndex之前的日志，也不是丢弃snapshotIndex之前的日志）： 1234567891011121314151617181920// compact 操作会丢弃 compactIndex 之前的日志，// 应用程序应该检查 compactIndex 应该在 appliedIndex 之前，因为，只允许 compact 掉已经 applyfunc (ms *MemoryStorage) Compact(compactIndex uint64) error &#123; ms.Lock() defer ms.Unlock() offset := ms.ents[0].Index if compactIndex &lt;= offset &#123; return ErrCompacted &#125; if compactIndex &gt; ms.lastIndex() &#123; raftLogger.Panicf("compact %d is out of bound lastindex(%d)", compactIndex, ms.lastIndex()) &#125; i := compactIndex - offset ents := make([]pb.Entry, 1, 1+uint64(len(ms.ents))-i) ents[0].Index = ms.ents[i].Index ents[0].Term = ms.ents[i].Term ents = append(ents, ms.ents[i+1:]...) ms.ents = ents return nil&#125; // storage.go 至此，关于应用程序与memoryStorage/Storage的简单交互过程已经阐述完毕。作个简单小结：通过上述的分析，我们仔细关联各个流程，可以发现WAL中的日志项是已落盘的，而Storage则是etcd-raft提供的被应用程序访问已落盘数据的接口，memoryStorage实现了这个接口(Storage)（而且，从它的各个操作逻辑来看，它只是简单地将WAL已落盘的数据进行了拷贝，当然还有一个compact过程，如果满足条件的话），个人感觉似乎有一点多余（从网上查找资料发现，一般而言，Storage的实现应该是WAL与cache算法的组合，那显然，在这里的memoryStorage并没有实现某种cache算法）。另外值得注意的是，在etcd-raft的实现中，协议核心并不与memoryStorage直接交互，都是应用程序与memoryStorage交互。 raft 协议库与 raftLog 交互这部分内容包括两个部分：其一是先继续了解raftLog内部一些重要接口的实现，以更进一步理解直接与raft协议库交互的raftLog的实现原理。其二挑选一个简单的raft协议库的逻辑——日志追加操作以查看协议库使用raftLog的细节。 raftLog 接口实现逻辑首先了解raftLog相关接口的实现细则。在上文已经初步了解过raftLog的数据结构。其构造函数如下： 123456789101112131415161718192021222324252627func newLog(storage Storage, logger Logger) *raftLog &#123; return newLogWithSize(storage, logger, noLimit)&#125;// newLogWithSize returns a log using the given storage and max// message size.func newLogWithSize(storage Storage, logger Logger, maxNextEntsSize uint64) *raftLog &#123; if storage == nil &#123; // storage 不能为空！ log.Panic("storage must not be nil") &#125; // 利用应用传入的 storage 及 logger 以及 maxNextEntsSize（如果有的话）构建 raftLog 实例 log := &amp;raftLog&#123; storage: storage, logger: logger, maxNextEntsSize: maxNextEntsSize, &#125; firstIndex, err := storage.FirstIndex() lastIndex, err := storage.LastIndex() // 将 unstable 的 offset 初始化为 storage 的 lastIndex+1 log.unstable.offset = lastIndex + 1 log.unstable.logger = logger // Initialize our committed and applied pointers to the time of the last compaction. // 将 raftLog 的 commited 及 applied 初始化为 firstIndex-1，即 storage 中第一项日志的索引号， // 因为第一项日志为已经被提交的（也是已经被快照的），可以仔细察看 storage 的 ApplySnapshot 逻辑 log.committed = firstIndex - 1 log.applied = firstIndex - 1 return log&#125; // log.go 此构造函数在初始化raft结构时会被调用（具体可以查看代码）。从上述构造函数逻辑来看，unstable似乎是从storage最后一条日志后开始存储，换言之，从raft协议库的角度，unstable存储更新的日志。。我们可以从下面的几个函数来进一步证实这一点： 12345678910111213141516171819202122func (l *raftLog) snapshot() (pb.Snapshot, error) &#123; if l.unstable.snapshot != nil &#123; return *l.unstable.snapshot, nil &#125; return l.storage.Snapshot()&#125; // log.gofunc (l *raftLog) firstIndex() uint64 &#123; if i, ok := l.unstable.maybeFirstIndex(); ok &#123; return i &#125; index, err := l.storage.FirstIndex() return index&#125; // log.gofunc (l *raftLog) lastIndex() uint64 &#123; if i, ok := l.unstable.maybeLastIndex(); ok &#123; return i &#125; i, err := l.storage.LastIndex() return i&#125; // log.go 当raftLog都是先将unstable关联的数据返回给raft核心库。我们后面会来仔细了解这些函数如何被调用。我们继续了解两个较为重要的接口： 12345678910111213141516171819202122232425262728293031323334353637// 日志追加，返回(0, false)若日志项不能被追加，否则返回 (最后一条日志索引, true)func (l *raftLog) maybeAppend(index, logTerm, committed uint64, ents ...pb.Entry) (lastnewi uint64, ok bool) &#123; if l.matchTerm(index, logTerm) &#123; // 1. 检验 index 与 term 是否匹配 lastnewi = index + uint64(len(ents)) // 2. 最后一条日志索引 ci := l.findConflict(ents) // 3. 检查此次追加的日志项是否与已有的存在冲突（论文中有详述冲突情况） switch &#123; case ci == 0: // 3.1 没有冲突，则直接提交（如果可以提交的话） case ci &lt;= l.committed: // 3.2 冲突的索引不能比已经提交的索引还要小！ l.logger.Panicf("entry %d conflict with committed entry [committed(%d)]", ci, l.committed) default: // 3.3 否则，与已有日志（未提交的）有冲突 //（也有可能没有冲突，详情在 findConflict 函数中说明），并追加日志，最后提交 offset := index + 1 l.append(ents[ci-offset:]...) &#125; l.commitTo(min(committed, lastnewi)) return lastnewi, true &#125; return 0, false&#125; // log.go// 即检查追加的日志项集合与已有的日志项（包括已提交与未提交）是否存在冲突，返回第一次冲突的日志索引（如果有的话）// 另外，需要注意的是，要追加的日志必须要连续// 如果没有冲突，并且已有的日志包含了要追加的所有日志项，则返回 0// 如果没有冲突，并且要追加的日志包含有新日志项，则返回第一次新的日志项// 日志项冲突判定的条件是: 相同的 index 不同的 termfunc (l *raftLog) findConflict(ents []pb.Entry) uint64 &#123; for _, ne := range ents &#123; if !l.matchTerm(ne.Index, ne.Term) &#123; if ne.Index &lt;= l.lastIndex() &#123; l.logger.Infof("found conflict at index %d [existing term: %d, conflicting term: %d]", ne.Index, l.zeroTermOnErrCompacted(l.term(ne.Index)), ne.Term) &#125; return ne.Index &#125; &#125; return 0&#125; raft 协议库追加日志接下来，我们把重点放在/etcd/raft.log文件中，并梳理日志追加的整体逻辑（关于文件中的数据结构以及一些细节我们暂且忽略，重点关注其逻辑流程）。为了让读者更容易理解整个过程的来龙去脉，我们仍然从应用程序提交日志开始切入，以将整个流程梳理一遍（同时，下文所展示的代码大部分只包含关键的逻辑）。下面的逻辑分析会大致依据实际逻辑顺利展开，即从应用程序提交日志开始，到leader节点在本地追加日志（若是follower节点收到请求消息，则一般是转发给leader节点），然后到leader节点广播日志给follower节点，最后到follower节点的日志追加，以及leader如何处理follower节点日志追加的响应消息。 leader 节点追加日志我们从应用程序向raft协议库提交日志请求开始，当然，在应用启动初始化时，其实也涉及到raft协议库的初始化启动，如下代码所示： 1234567891011121314151617181920212223242526272829func (rc *raftNode) startRaft() &#123; // ... rpeers := make([]raft.Peer, len(rc.peers)) for i := range rpeers &#123; rpeers[i] = raft.Peer&#123;ID: uint64(i + 1)&#125; &#125; c := &amp;raft.Config&#123; ID: uint64(rc.id), ElectionTick: 10, HeartbeatTick: 1, Storage: rc.raftStorage, MaxSizePerMsg: 1024 * 1024, MaxInflightMsgs: 256, MaxUncommittedEntriesSize: 1 &lt;&lt; 30, &#125; if oldwal &#123; rc.node = raft.RestartNode(c) &#125; else &#123; startPeers := rpeers if rc.join &#123; startPeers = nil &#125; // 启动底层 raft 协议核心库，并将 Config 及集群中节点信息传入 rc.node = raft.StartNode(c, startPeers) &#125; // ... go rc.serveRaft() go rc.serveChannels()&#125; // /etcd/contrib/raftexample/raft.go 在raft.StartNode()函数中，创建node，它表示底层raft协议的实例，构建了raft实例（封装协议实现的核心逻辑），并且调用了n.run()以等待上层应用程序向node提交请求，关键代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091func StartNode(c *Config, peers []Peer) Node &#123; r := newRaft(c) // ... n := newNode() // ... go n.run(r) return &amp;n&#125; // node.gofunc (n *node) run(r *raft) &#123; // ... for &#123; if advancec != nil &#123; readyc = nil &#125; else &#123; rd = newReady(r, prevSoftSt, prevHardSt) if rd.containsUpdates() &#123; readyc = n.readyc &#125; else &#123; readyc = nil &#125; &#125; // ... select &#123; case pm := &lt;-propc: m := pm.m m.From = r.id err := r.Step(m) // 调用 Step 函数来进行处理 if pm.result != nil &#123; pm.result &lt;- err close(pm.result) &#125; case m := &lt;-n.recvc: // 此处的逻辑会在 follower 节点接收 leader 节点广播的消息时调用 // 具体地，会在下文的 【follower 节点追加日志】 小节涉及到 // filter out response message from unknown From. if pr := r.getProgress(m.From); pr != nil || !IsResponseMsg(m.Type) &#123; r.Step(m) &#125; // ... case readyc &lt;- rd: if rd.SoftState != nil &#123; prevSoftSt = rd.SoftState &#125; if len(rd.Entries) &gt; 0 &#123; prevLastUnstablei = rd.Entries[len(rd.Entries)-1].Index prevLastUnstablet = rd.Entries[len(rd.Entries)-1].Term havePrevLastUnstablei = true &#125; if !IsEmptyHardState(rd.HardState) &#123; prevHardSt = rd.HardState &#125; if !IsEmptySnap(rd.Snapshot) &#123; prevSnapi = rd.Snapshot.Metadata.Index &#125; if index := rd.appliedCursor(); index != 0 &#123; applyingToI = index &#125; r.msgs = nil r.readStates = nil r.reduceUncommittedSize(rd.CommittedEntries) advancec = n.advancec case &lt;-advancec: // ... // ... &#125; &#125;&#125; // node.gofunc newReady(r *raft, prevSoftSt *SoftState, prevHardSt pb.HardState) Ready &#123; rd := Ready&#123; Entries: r.raftLog.unstableEntries(), CommittedEntries: r.raftLog.nextEnts(), Messages: r.msgs, // Step 函数将消息进行广播实际上会发送到此 msg 结构中 &#125; if softSt := r.softState(); !softSt.equal(prevSoftSt) &#123; rd.SoftState = softSt &#125; if hardSt := r.hardState(); !isHardStateEqual(hardSt, prevHardSt) &#123; rd.HardState = hardSt &#125; if r.raftLog.unstable.snapshot != nil &#123; rd.Snapshot = *r.raftLog.unstable.snapshot &#125; if len(r.readStates) != 0 &#123; rd.ReadStates = r.readStates &#125; rd.MustSync = MustSync(r.hardState(), prevHardSt, len(rd.Entries)) return rd&#125; // node.go 从上面展示的三个函数，可以发现程序会开一个go routine通过channel来处理所有现应用程序（当然也有内部的一些逻辑）的交互。当node从propc管道中收到应用程序提交的请求后，它会将此请求交给Step函数处理，Step函数在经过一系列检查之后（比如检查term），会调用step函数（这里只考虑正常的MsgProp消息），step函数对于不同的角色的节点其实现不同，典型的，对于leader节点，其实现为stepLeader。另外，在循环中，程序会将打包好Ready结构通过readc的管道发送给应用程序，然后等待从advancec管道中接收应用程序的返回消息。下面，我们从stepLeader函数开始来一步步梳理leader的日志追加逻辑： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667func stepLeader(r *raft, m pb.Message) error &#123; // These message types do not require any progress for m.From. switch m.Type &#123; case pb.MsgBeat: // ... return nil case pb.MsgProp: // ... // 1. 追加日志 if !r.appendEntry(m.Entries...) &#123; return ErrProposalDropped &#125; // 2. 广播日志追加 r.bcastAppend() return nil case pb.MsgReadIndex: // ... return nil &#125; // ... return nil&#125; // /etcd/raft/raft.gofunc (r *raft) appendEntry(es ...pb.Entry) (accepted bool) &#123; // ... // use latest "last" index after truncate/append li = r.raftLog.append(es...) r.getProgress(r.id).maybeUpdate(li) // Regardless of maybeCommit's return, our caller will call bcastAppend. r.maybeCommit() return true&#125; // /etcd/raft/raft.gofunc (l *raftLog) append(ents ...pb.Entry) uint64 &#123; // ... l.unstable.truncateAndAppend(ents) return l.lastIndex()&#125; // log.gofunc (u *unstable) truncateAndAppend(ents []pb.Entry) &#123; after := ents[0].Index switch &#123; // 若需要追加的日志项集合中的第一条日志恰好是已有的日志的最后一条日志的后一条日志，则直接追加 case after == u.offset+uint64(len(u.entries)): // after is the next index in the u.entries // directly append u.entries = append(u.entries, ents...) // 若需要追加的日志项集合中的第一条日志，要比 unstable 中的 offset 还要小 //（即比 unstable 中日志项集合的开始日志的索引要小） // 则需要把重新设置 offset 索引，并且将 unstable 的日志项集合中的日志覆盖 case after &lt;= u.offset: u.logger.Infof("replace the unstable entries from index %d", after) // The log is being truncated to before our current offset // portion, so set the offset and replace the entries u.offset = after u.entries = ents default: // 否则，分段次进行日志追加 //（包含两种情况，u.offset &lt; after &lt; u.offset+len(u.entries) 或者 after &gt; u.offset+len(u.entries)） // 此种情况也可能涉及到 unstable 中已有日志的截断（前一种情况） // truncate to after and copy to u.entries // then append u.logger.Infof("truncate the unstable entries before index %d", after) u.entries = append([]pb.Entry&#123;&#125;, u.slice(u.offset, after)...) u.entries = append(u.entries, ents...) &#125;&#125; // log_unstable.go 在stepLeader函数中，首先调用 了appendEntry()函数，它会将日志项集合追加到raftLog中（实际上是调用了r.raftLog.append(es...)追加到unstable日志项集合），并且提交本地的日志项（如果满足条件的话）。 leader 节点向 follower 节点广播日志并且，在stepLeader()上会继续调用r.bcastAppend()函数向集群中其它节点广播日志，具体代码如下所示： 123456789101112func (r *raft) bcastAppend() &#123; r.forEachProgress(func(id uint64, _ *Progress) &#123; if id == r.id &#123; return &#125; r.sendAppend(id) &#125;)&#125; // /etcd/raft/raft.gofunc (r *raft) sendAppend(to uint64) &#123; r.maybeSendAppend(to, true)&#125; // /etcd/raft/raft.go 而sendAppend()函数又会调用maybeSendAppend()函数来向特定的节点发送日志同步命令。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647func (r *raft) maybeSendAppend(to uint64, sendIfEmpty bool) bool &#123; pr := r.getProgress(to) // ... m := pb.Message&#123;&#125; m.To = to term, errt := r.raftLog.term(pr.Next - 1) ents, erre := r.raftLog.entries(pr.Next, r.maxMsgSize) if len(ents) == 0 &amp;&amp; !sendIfEmpty &#123; return false &#125; // sendIfEmpty 可以用作控制空消息是否可以被发送（消息过多时，肯定不建议发送） // 如果获取 term 或者 ents 失败，则发送 snap 消息 if errt != nil || erre != nil &#123; // 此处主要是构建 snap 消息的相关操作 // ... m.Type = pb.MsgSnap snapshot, err := r.raftLog.snapshot() // ... m.Snapshot = snapshot sindex, sterm := snapshot.Metadata.Index, snapshot.Metadata.Term // ... pr.becomeSnapshot(sindex) r.logger.Debugf("%x paused sending replication messages to %x [%s]", r.id, to, pr) &#125; else &#123; // 先设置消息的相关属性 m.Type = pb.MsgApp m.Index = pr.Next - 1 m.LogTerm = term m.Entries = ents m.Commit = r.raftLog.committed // 此处针对节点不同的状态（定义在 progress.go 文件中），来控制一次性给节点发送的消息数量，是批量发送，还是一次只发一条，还是要先暂停探测一下 if n := len(m.Entries); n != 0 &#123; switch pr.State &#123; // optimistically increase the next when in ProgressStateReplicate case ProgressStateReplicate: last := m.Entries[n-1].Index pr.optimisticUpdate(last) pr.ins.add(last) case ProgressStateProbe: pr.pause() default: r.logger.Panicf("%x is sending append in unhandled state %s", r.id, pr.State) &#125; &#125; &#125; // send 函数会将消息保存到 raft.msgs 字段，最后用于构建 Ready 实例结构，以发送给应用程序， // 事实上，此步骤才是真正执行消息发送的步骤（raft 协议库向应用程序发送消息，然后应用程序来控制并执行具体的日志消息网络传输的操作） r.send(m) return true&#125; // /etcd/raft/raft.go 简单而言，上述函数的逻辑为：首先根据该节点上一次已同步的日志位置pr.Next-1，从raftLog中获取该位置之后的日志项，并且日志同步的数量会受到maxMsgSize控制。并且若果无法从raftLog获取到想要的日志项，此时需要只能发送snap（即MsgSnap消息），因为对应日志项可能由于已经被commit而丢弃了。另外，真正的发送消息的操作其实是向r.msgs字段中追加实际需要发送的消息，后面会由node将其打包入Ready结构中，转而发送给应用程序，由应用程序执行真正消息的网络传输操作。 至此，leader节点广播日志项给follower相关流程已经分析完毕。 follower 节点追加日志在分析具体的follower节点追加leader节点给它发送的消息中的日志之前，我们把这个过程阐述得更完整一些。当应用程序调用transport网络传输组件将MsgApp类型的消息由传至follower节点时，更准确而言，transport组件的接收器在接收到消息后，会调用其Raft组件的Process()方法（此部分逻辑不再展示相关代码，在上上篇文章【etcd-raft 网络传输源码简析】中包含了此部分逻辑）。而应用程序会实现此Process()接口，在raftexample示例程序中，其实现逻辑也较为简单： 123func (rc *raftNode) Process(ctx context.Context, m raftpb.Message) error &#123; return rc.node.Step(ctx, m) // 直接调用底层协议核心结构 node 的 Step 函数来处理消息&#125; // /etcd/contrib/raftexample/raft.go 调用Step()函数后，类似于leader节点，会进入到node实例的Step()函数中，它会调用node的一系列函数，包括step()、stepWithWaitOption()函数，然后将消息传入recvc通道，然后在node节点的主循环函数run()中，会一直监视着各通道，因此会从recvc通道中取出消息，最后调用raft.Step()，接下来经过一系列的检查，会调用step()函数即，同样，这里是follower节点，因此最后会调用stepFollower()函数（后面这一个阶段的函数调用栈同leader节点接收到应用程序的请求的流程是一样的）。下面简要贴出在recv通道放入消息之前流程的相关代码： 123456789101112131415161718192021222324252627282930func (n *node) Step(ctx context.Context, m pb.Message) error &#123; // ignore unexpected local messages receiving over network if IsLocalMsg(m.Type) &#123; // TODO: return an error? return nil &#125; return n.step(ctx, m)&#125; // node.gofunc (n *node) step(ctx context.Context, m pb.Message) error &#123; return n.stepWithWaitOption(ctx, m, false)&#125; // node.gofunc (n *node) stepWait(ctx context.Context, m pb.Message) error &#123; return n.stepWithWaitOption(ctx, m, true)&#125; // node.go// Step advances the state machine using msgs. The ctx.Err() will be returned,// if any.func (n *node) stepWithWaitOption(ctx context.Context, m pb.Message, wait bool) error &#123; if m.Type != pb.MsgProp &#123; select &#123; case n.recvc &lt;- m: return nil case &lt;-ctx.Done(): return ctx.Err() case &lt;-n.done: return ErrStopped &#125; &#125; // ... return nil&#125; // node.go 下面来重点看一下stepFolower()函数的逻辑，具体是接收到MsgApp类型的消息的处理逻辑。 1234567891011121314151617181920212223242526272829303132333435func stepFollower(r *raft, m pb.Message) error &#123; switch m.Type &#123; case pb.MsgProp: // 如果应用程序将请求直接发到了 follower 节点，则可能会将消息转发给 leader if r.lead == None &#123; r.logger.Infof("%x no leader at term %d; dropping proposal", r.id, r.Term) return ErrProposalDropped &#125; else if r.disableProposalForwarding &#123; r.logger.Infof("%x not forwarding to leader %x at term %d; dropping proposal", r.id, r.lead, r.Term) return ErrProposalDropped &#125; m.To = r.lead r.send(m) // 转发给 leader case pb.MsgApp: // 接收到 leader 发送的日志同步消息 r.electionElapsed = 0 r.lead = m.From r.handleAppendEntries(m) // 追加日志操作 case pb.MsgHeartbeat: r.electionElapsed = 0 r.lead = m.From r.handleHeartbeat(m) case pb.MsgSnap:// 接收到 leader 发送的 snap 同步消息 r.electionElapsed = 0 r.lead = m.From r.handleSnapshot(m) // 处理快照同步的操作 case pb.MsgTransferLeader: // ... case pb.MsgTimeoutNow: // ... case pb.MsgReadIndex: // ... case pb.MsgReadIndexResp: // .. &#125; return nil&#125; // /etcd/raft/raft.go 上面的逻辑很清晰。我们紧接着查看handleAppendEntries()函数： 123456789101112131415161718192021func (r *raft) handleAppendEntries(m pb.Message) &#123; // 消息中的索引不能小于节点已经提交的消息的索引，否则不追加消息，以已提交的索引作为参数直接回复 if m.Index &lt; r.raftLog.committed &#123; // 此处的 send 函数同 前面 leader 节点在广播日志最终调用的 send 函数为同一个函数 // 即将此消息放到 raft.msgs 结构中，此结构最后会作为 node 打包 Ready 结构的参数 // 最后发送给应用程序，然后由应用程序通过网络转发给对应的节点（此处为 leader） r.send(pb.Message&#123;To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.committed&#125;) return &#125; // 调用 maybeAppend 函数进行日志追加，若追加成功，则以追加后的日志项集合作为参数回复 if mlastIndex, ok := r.raftLog.maybeAppend(m.Index, m.LogTerm, m.Commit, m.Entries...); ok &#123; r.send(pb.Message&#123;To: m.From, Type: pb.MsgAppResp, Index: mlastIndex&#125;) &#125; else &#123; // 否则表示日志追加失败，则是日志索引不匹配造成 //（详情可查看 maybeAppedn函数，简而言之，最后会通过调用 append、truncateAndAppend函数以将消息追加到 raftLog 的 unstable 结构中。 // 此函数在之前的 raftLog 接口实现分析中有涉及，因此不再阐述）， // 则设置冲突的提示，以及本节点的最后的日志项索引作为参数进行回复 r.logger.Debugf("%x [logterm: %d, index: %d] rejected msgApp [logterm: %d, index: %d] from %x", r.id, r.raftLog.zeroTermOnErrCompacted(r.raftLog.term(m.Index)), m.Index, m.LogTerm, m.Index, m.From) r.send(pb.Message&#123;To: m.From, Type: pb.MsgAppResp, Index: m.Index, Reject: true, RejectHint: r.raftLog.lastIndex()&#125;) &#125;&#125; // /etcd/raft/raft.go 作个简单小结，从上面分析的逻辑可以发现，同leader类似，follower节点的数据最终也是被写入了日志模块raftLog的unstable结构中，同样，follower节点的回复消息也是加入到raft.msgs结构中，最后会成为Ready的成员，以传递给应用程序，由应用程序进行实际的网络转发操作。 leader 节点处理 follower 节点日志追加响应最后，同样，当follower将回复消息发送之后，再由网络传输组件transport调用node.Process()函数以处理此消息（此逻辑已在上面的【folower节点追加日志】小节中最开始阐述）。因此，最后同样会进入leader的stepLeader()函数，而且会进入消息类型为MsgAppResp分支处理逻辑中，关键代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263func stepLeader(r *raft, m pb.Message) error &#123; // ... switch m.Type &#123; case pb.MsgAppResp: pr.RecentActive = true if m.Reject &#123; // 若 follower回复拒绝消息 r.logger.Debugf("%x received msgApp rejection(lastindex: %d) from %x for index %d", r.id, m.RejectHint, m.From, m.Index) // 则需要减小消息的索引，即往前挑选消息（raft 论文中关于日志冲突已经详细介绍）， // 即 if pr.maybeDecrTo(m.Index, m.RejectHint) &#123; r.logger.Debugf("%x decreased progress of %x to [%s]", r.id, m.From, pr) if pr.State == ProgressStateReplicate &#123; pr.becomeProbe() &#125; // 再次将消息发送给 follower r.sendAppend(m.From) &#125; &#125; else &#123; // 否则 follower 回复成功追加日志 oldPaused := pr.IsPaused() // 此处为更新 leader 维护的对各 follower 节点的进度详情（具体在 progress.go中描述） // 比较简单，因此为了节约篇幅，此处不展开叙述。 // 事实上，这也是 etcd-raft 针对 原始的 raft 论文作的一些优化。 if pr.maybeUpdate(m.Index) &#123; switch &#123; case pr.State == ProgressStateProbe: pr.becomeReplicate() case pr.State == ProgressStateSnapshot &amp;&amp; pr.needSnapshotAbort(): r.logger.Debugf("%x snapshot aborted, resumed sending replication messages to %x [%s]", r.id, m.From, pr) pr.becomeProbe() pr.becomeReplicate() case pr.State == ProgressStateReplicate: pr.ins.freeTo(m.Index) &#125; // 检查是否需要提交，若的确可以提交，则同样将此消息进行广播 if r.maybeCommit() &#123; r.bcastAppend() &#125; else if oldPaused &#123; r.sendAppend(m.From) &#125; // We've updated flow control information above, which may // allow us to send multiple (size-limited) in-flight messages // at once (such as when transitioning from probe to // replicate, or when freeTo() covers multiple messages). If // we have more entries to send, send as many messages as we // can (without sending empty messages for the commit index) for r.maybeSendAppend(m.From, false) &#123; &#125; // Transfer leadership is in progress. if m.From == r.leadTransferee &amp;&amp; pr.Match == r.raftLog.lastIndex() &#123; r.logger.Infof("%x sent MsgTimeoutNow to %x after received MsgAppResp", r.id, m.From) r.sendTimeoutNow(m.From) &#125; &#125; &#125; case pb.MsgHeartbeatResp: // ... case pb.MsgSnapStatus: // ... case pb.MsgTransferLeader: // ... &#125; return nil&#125; // /etcd/raft/raft.go 至此，leader节点处理follower节点对日志追加消息的回复也已经分析完毕。 因此，整个完整的流程也已经结束。我们也对unstabel以及raftLog的流程，即raft协议库与raftLog的交互作一个简单小结：可以发现，unstable或者说raftLog只是协议存储管理日志的组件，没有其它作用，即它没有用作诸如节点宕机后重启、新节点加入过程的日志来源。unstable是未落盘的日志项集合，即可能会丢失，因此unstable日志最终会持久化到storage中，即持久化到snap以及WAL日志。 最后，需要提醒读者的是，文章比较长，若读者没有时间，也可以挑选部分小节进行参考（各小节是独立分析阐述的）。最重要的是，读者自己能够进入到源码文件进行查看，那比本文所贴出的代码逻辑会更容易理解，读者也会获取得更多。 参考文献 [1]. https://github.com/etcd-io/etcd/tree/master/raft]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式协调服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd-raft WAL日志管理源码简析]]></title>
    <url>%2F2019%2F01%2F11%2Fetcd-raft-WAL%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章简单分析了etcd-raft 网络传输组件相关的源码。本文会简要分析etcd-raft WAL日志管理部分的源码。WAL(Write-Ahead Logging)即为预写式日志，即在真正执行写操作之前先写日志，这在数据库系统和分布式系统领域很常见。它是为了保证数据的可靠写。日志对于利用一致性协议构建高可用的分布式系统而言至关重要，在etcd raft中，日志也会在各节点之间同步。并且etcd提供了一个WAL的日志库，它暴露日志管理相关的接口以方便应用程序具体操作日志的逻辑。本文从应用调用 WAL库执行日志追加逻辑切入，重点分析etcd提供的WAL日志库的相关接口实现逻辑细则，包括WAL日志的设计、日志创建追加等。 数据结构同之前的文章类似，希望读者能够主动查看源码（主要涉及目录/etcd/wal），文章作为参考。按惯例，先来观察与WAL相关的重要数据结构的设计。从最核心的数据结构切入WAL： 12345678910111213141516171819202122232425262728293031// WAL 是持久化存在的逻辑表示。并且要么处于读模式要么处于追加模式。// 新创建的 WAL 处于追加模式，可用于记录追加。// 刚打开的 WAL 处于读模式，可用于记录读取。// 当读完之前所有的 WAL 记录后，WAL 才可用于记录追加。type WAL struct &#123; lg *zap.Logger // 日志存储目录 dir string // the living directory of the underlay files // dirFile is a fd for the wal directory for syncing on Rename // 文件描述符，用于 WAL 目录同步重命名操作 dirFile *os.File // 元数据，在创建日志文件时，在写在文件头位置 metadata []byte // metadata recorded at the head of each WAL state raftpb.HardState // hardstate recorded at the head of WAL start walpb.Snapshot // snapshot to start reading decoder *decoder // decoder to decode records readClose func() error // closer for decode reader mu sync.Mutex // WAL 中保存的最后一条日志的索引 enti uint64 // index of the last entry saved to the wal encoder *encoder // encoder to encode records // LockedFile 封装了 os.File 的结构，具备文件锁定功能 locks []*fileutil.LockedFile // the locked files the WAL holds (the name is increasing) fp *filePipeline&#125; // wal.go 简单的字段在代码中作了注释。下面了解下几个重点的结构： state: HardState{Term,Vote,Commit}类型，它表示节点在回复消息时，必须先进行持久化保持的状态。 start: walpb.Snapshot{Index, Term}类型，即表示WAL日志中的快照，当读WAL日志时需从此索引后一个开始，如应用在重放日志逻辑中，需要打开WAL日志，则其只需要对Snapshot索引后的日志作重放。 decoder: decoder封装了Reader，并且使用crc来校验读取的记录，一个文件对应一个decoder。 encoder: encoder封装了PageWriter，同样使用crc来检验写入记录，encoder实例同样对应一个文件。 fp: filePipeline类型，它管理文件创建时的磁盘空间分配操作逻辑。若文件以写模式打开，它会开启一个单独的go routine为文件创建预分配空间，以提高文件创建的效率。此逻辑封装在file_pipeline.go。 我们不妨简单看看encoder的结构（比decoder结构稍复杂），它包含了一个执行具体写操作的PageWriter，以及一个crc字段，另外，还包含两个预分配的缓冲区，其中buf(1MB)用于写入实际数据，而uint64buf(8B)用于写入长度相关字段。其代码如下： 123456789101112131415161718type encoder struct &#123; mu sync.Mutex bw *ioutil.PageWriter crc hash.Hash32 buf []byte // 用于写入实际记录数据 uint64buf []byte // 用于写入长度相关字段&#125; // encoder.gofunc newEncoder(w io.Writer, prevCrc uint32, pageOffset int) *encoder &#123; return &amp;encoder&#123; bw: ioutil.NewPageWriter(w, walPageBytes, pageOffset), crc: crc.New(prevCrc, crcTable), // 1MB buffer buf: make([]byte, 1024*1024), uint64buf: make([]byte, 8), &#125;&#125; // encoder.go 另外，存储在WAL日志的记录包括两种，一种以Record形式保存，它是一种普通的记录格式，另一种以Snapshot形式保存，它专门用于快照记录的存储，但快照类型的记录最终还是作为Record类型记录存储： 123456789101112type Record struct &#123; Type int64 `protobuf:"varint,1,opt,name=type" json:"type"` Crc uint32 `protobuf:"varint,2,opt,name=crc" json:"crc"` Data []byte `protobuf:"bytes,3,opt,name=data" json:"data,omitempty"` XXX_unrecognized []byte `json:"-"`&#125; // record.pb.gotype Snapshot struct &#123; Index uint64 `protobuf:"varint,1,opt,name=index" json:"index"` Term uint64 `protobuf:"varint,2,opt,name=term" json:"term"` XXX_unrecognized []byte `json:"-"`&#125; // record.pb.go 对于普通记录Record类型结构（即WAL日志类型），它的Type字段表示日志类型，包括如下几种日志类型： 12345678910111213141516const ( // 元数据类型日志项，被写在每个日志文件的头部，具体内容可以任意，包括空值 metadataType int64 = iota + 1 // 实际的数据，即日志存储中的关键数据 entryType // 表示保存的为 HardState 类型的数据 stateType // 前一个 WAL 日志记录数据的 crc 值 crcType // 表示快照类型的日志记录，它表示当前 Snapshot 位于哪个日志记录，保存的是索引(Term,Index)数据 snapshotType // warnSyncDuration is the amount of time allotted to an fsync before // logging a warning warnSyncDuration = time.Second) // wal.go 而它的crc字段表示校验和数据，需要注意的是它并非直接保存的是当前日志记录的校验数据，而保存的是当前文件该日志项之前的所有日志项的校验和，这似乎是采用类似一种rolling crc，以保证WAL日志的连续性，因为写日志的时候可能会涉及到cut操作，它会将日志内容存储到不止一个文件。data字段会根据不同的类型来具体确定，若为stateType，则存储HardState类型的数据，若为entryType，则存储Entry类型的数据，若为snapshotType，则存储Snapshot类型的数据（只是索引数据），若为metadataType，则似乎可以由应用决定（目前来看在raftNode结构中，使用了此类型的日志，但传过来的数据为空），若为crcType，则存储Crc类型(unit32)的数据。 最后的padding字段，则是为了保持日志项数据 8 字节对其的策略，而进行填充的内容。这个我们可以从任一一处编码Record记录的代码中观察得知，如从wal.go中w.encoder.encode(...)代码往下追溯具体的encode()的逻辑，在encode()函数中会调用encodeFrameSize(len(data))，其函数具体的代码如下： 1234567891011func encodeFrameSize(dataBytes int) (lenField uint64, padBytes int) &#123; lenField = uint64(dataBytes) // force 8 byte alignment so length never gets a torn write padBytes = (8 - (dataBytes % 8)) % 8 // 先得出 padding 的 bytes 的大小，一定小于 8 if padBytes != 0 &#123; lenField |= uint64(0x80|padBytes) &lt;&lt; 56 // 将 0x80 与 padBytes 进行或操作，得到 4 个二进制位的内容，然后再左移 56 位。最后得到的记录的存储二进制结构为： // &#123;|-(1位标记位)| |---(3位表示 Padding bytes Size)|&#125;&#123;...(56位于表示实际的 Record bytes Size)&#125; &#125; return lenField, padBytes&#125; // encoder.go 至此相关的重要的数据结构项已经查看完毕，主要是围绕WAL结构展开。文章为了节约篇幅并没有将所有的数据项结构的代码帖出，读者可以自己深入源码查看，较为简单。 关键流程在此部分分析中，简要分析阐述WAL库提供的各个接口实现的逻辑，主要包括WAL创建、WAL初始化（打开）、WAL日志项读取及WAL追加日志项等流程。另外，关于raft协议核心库如何操作日志的逻辑暂不涉及。 WAL 创建在raftexample示例代码中，应用在启动时，对WAL日志执行重放操作（raft.go，在startRaft()中的rc.replayWAL()），而在重放日志函数的逻辑中，它先加载snapshot数据，然后，将其作为参数传递给rc.openWAL(snapshot)函数，以对打开文件，如果文件不存在，则会先创建日志文件。关键代码如下所示： 123456789101112131415161718192021222324252627282930313233// replayWAL replays WAL entries into the raft instance.func (rc *raftNode) replayWAL() *wal.WAL &#123; log.Printf("replaying WAL of member %d", rc.id) snapshot := rc.loadSnapshot() // 加载 snapshot 数据 w := rc.openWAL(snapshot) // 打开 WAL 日志文件，以读取 snaptshot 位置后的日志 _, st, ents, err := w.ReadAll() // 读取 WAL 日志文件，相关逻辑后面详述 // ... return w&#125; // raft.go// openWAL returns a WAL ready for reading.func (rc *raftNode) openWAL(snapshot *raftpb.Snapshot) *wal.WAL &#123; if !wal.Exist(rc.waldir) &#123; if err := os.Mkdir(rc.waldir, 0750); err != nil &#123; log.Fatalf("raftexample: cannot create dir for wal (%v)", err) &#125; // 1. 创建日志文件，注意在这里 metaData 参数为 nil w, err := wal.Create(zap.NewExample(), rc.waldir, nil) w.Close() &#125; // 2. 创建 snapshotType 类型的日志项，以用于记录当前快照的索引情况(Term, Index) walsnap := walpb.Snapshot&#123;&#125; if snapshot != nil &#123; walsnap.Index, walsnap.Term = snapshot.Metadata.Index, snapshot.Metadata.Term &#125; log.Printf("loading WAL at term %d and index %d", walsnap.Term, walsnap.Index) // 3. 打开从 snapshot 位置打开日志，其相关的逻辑在后面详述 w, err := wal.Open(zap.NewExample(), rc.waldir, walsnap) if err != nil &#123; log.Fatalf("raftexample: error loading wal (%v)", err) &#125; return w&#125; // raft.go 阐明了应用WAL日志库的入口后，我们先来查看WAL创建函数相关的逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// Create creates a WAL ready for appending records. The given metadata is// recorded at the head of each WAL file, and can be retrieved with ReadAll.// 创建一个 WAL 文件用于日志记录追加。元数据存放在文件头部，可以通过 ReadAll 检索到func Create(lg *zap.Logger, dirpath string, metadata []byte) (*WAL, error) &#123; if Exist(dirpath) &#123; return nil, os.ErrExist &#125; // keep temporary wal directory so WAL initialization appears atomic // 1. 先创建一个临时文件，然后对此文件进行重命名，以使得文件被原子创建 tmpdirpath := filepath.Clean(dirpath) + ".tmp" if fileutil.Exist(tmpdirpath) &#123; if err := os.RemoveAll(tmpdirpath); err != nil &#123; return nil, err &#125; &#125; if err := fileutil.CreateDirAll(tmpdirpath); err != nil &#123; return nil, err &#125; // 2. 构造文件名，即构建 dir/filename, 其中 filename 伤脑筋 walName函数来获取，文件名构建规则为：seq-index.wal p := filepath.Join(tmpdirpath, walName(0, 0)) // 3. WAL 对文件的操作都是通过 LockFile 来执行的 f, err := fileutil.LockFile(p, os.O_WRONLY|os.O_CREATE, fileutil.PrivateFileMode) if err != nil &#123; return nil, err &#125; // 4. 定位到文件末尾 if _, err = f.Seek(0, io.SeekEnd); err != nil &#123; return nil, err &#125; // 5. 预分配文件，默认 SegmentSizeBytes 大小为 64MB if err = fileutil.Preallocate(f.File, SegmentSizeBytes, true); err != nil &#123; return nil, err &#125; // 6. 初始化 WAL 数据结构 w := &amp;WAL&#123; lg: lg, dir: dirpath, metadata: metadata, &#125; // 7. 针对此文件构建 WAL 结构的 encoder 字段，并且将 preCrc 字段赋值为0 w.encoder, err = newFileEncoder(f.File, 0) // 8. 将此（具备锁定性质的）文件添加到 WAL 结构的 locks 数组字段 w.locks = append(w.locks, f) // 9. 保存类型为 crcType 的 crc 记录项，具体的 crc 数据为 preCrc=0 if err = w.saveCrc(0); err != nil &#123; return nil, err &#125; // 10. 利用 encoder 编码类型为 metadataType 的 metaData 记录项 if err = w.encoder.encode(&amp;walpb.Record&#123;Type: metadataType, Data: metadata&#125;); err != nil &#123; return nil, err &#125; // 11. 保存类型为 snapshotType 的空的 Snapshot 记录 if err = w.SaveSnapshot(walpb.Snapshot&#123;&#125;); err != nil &#123; return nil, err &#125; // 12. 重命名操作，之前以.tmp结尾的文件，初始化完成之后进行重命名，类似原子操作 if w, err = w.renameWAL(tmpdirpath); err != nil &#123; return nil, err &#125; // directory was renamed; sync parent dir to persist rename pdir, perr := fileutil.OpenDir(filepath.Dir(w.dir)) // 13. 将上述涉及到对文件的操作进行同步处理 if perr = fileutil.Fsync(pdir); perr != nil &#123; return nil, perr &#125; if perr = pdir.Close(); err != nil &#123; return nil, perr &#125; return w, nil&#125; // wal.go 上述代码片段中的注释对整个创建过程进行了详细阐述，这是总结一下，它主要涉及到几个操作： 创建WAL目录，用于存储WAL日志文件及索引，同时使用临时文件及重命名的方式来原子操作。 对日志文件的创建，会预分配空间，以提高创建的效率。 在日志文件创建时，会初始化 WAL结构实例，同时写入crcType、metadataType记录项，并且保存一个空的snapshotType记录项。对于各种类型记录项，上文中数据结构小节已经详细阐述。 我们来看下它是如何保存snapshotType类型的Snapshot数据的，相关逻辑在函数SaveSnapShot(Snapshot): 123456789101112131415161718// 持久化 walpb.Snapshot 数据func (w *WAL) SaveSnapshot(e walpb.Snapshot) error &#123; b := pbutil.MustMarshal(&amp;e) // 1. 先执行序列化操作 w.mu.Lock() defer w.mu.Unlock() // 2. 构建 snaptshotType 类型的记录结构，并以序列化的数据作为参数 rec := &amp;walpb.Record&#123;Type: snapshotType, Data: b&#125; // 3. 利用 encoder 编码写入 if err := w.encoder.encode(rec); err != nil &#123; return err &#125; // update enti only when snapshot is ahead of last index // 4. w.enti 表示的是 WAL 中最后一条日志的索引，因此只有当其小于快照的索引时，才进行替换 if w.enti &lt; e.Index &#123; w.enti = e.Index &#125; return w.sync()&#125; // wal.go 我们不妨深入encoder.encode()函数中查看一下编码的细节： 12345678910111213141516171819202122232425262728293031323334353637383940// 编码一条数据记录项func (e *encoder) encode(rec *walpb.Record) error &#123; e.mu.Lock() defer e.mu.Unlock() // 1. 生成校验和数据 e.crc.Write(rec.Data) rec.Crc = e.crc.Sum32() var ( data []byte err error n int ) // 2. 如果记录的大小超过预分配的 1MB 的 buffer（与文件的预分配可能类似），则重新分配空间 if rec.Size() &gt; len(e.buf) &#123; data, err = rec.Marshal() if err != nil &#123; return err &#125; &#125; else &#123; // 否则直接使用预分配的空间 n, err = rec.MarshalTo(e.buf) if err != nil &#123; return err &#125; data = e.buf[:n] &#125; // 3. 调用 encodeFrameSize 函数来构建 lenField 以及判断对齐的位数 lenField, padBytes := encodeFrameSize(len(data)) // 4. 先写记录编码后的长度字段 if err = writeUint64(e.bw, lenField, e.uint64buf); err != nil &#123; return err &#125; // 5. 然后，若需要对齐，则追加对齐填充数据 if padBytes != 0 &#123; data = append(data, make([]byte, padBytes)...) &#125; // 6. 最后正式写入记录的所包含的所有数据内容 _, err = e.bw.Write(data) return err&#125; // encoder.go 关于WAL文件创建相关逻辑已经阐述完毕，下一小节阐述与创建类似的操作即初始化操作。 WAL 初始化WAL初始化，我将表示它表示为打开文件逻辑，即在应用程序里面中的代码wal.Open()函数中的流程。具体而言，打开WAL文件的目的是为了从里面读取日志文件（读取的目的一般是重放日志）。因此，更准确而言，是从指定索引处打开，此索引即表示之前的已经执行的快照的索引，从那之后开始进行读操作，而且只有当把快照索引之后的日志全部读取完毕才能进行追加操作。另外，打开操作必须保证快照之前已经被存储，否则读取操作ReadlAll会执行失败。打开操作的相关的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182func Open(lg *zap.Logger, dirpath string, snap walpb.Snapshot) (*WAL, error) &#123; // 只打开最后一个 seq 小于 snap 中的 index 之后的所有 wal 文件，并且以写的方式打开 w, err := openAtIndex(lg, dirpath, snap, true) if w.dirFile, err = fileutil.OpenDir(w.dir); err != nil &#123; return nil, err &#125; return w, nil&#125; // wal.go// 打开指定索引后的日志文件func openAtIndex(lg *zap.Logger, dirpath string, snap walpb.Snapshot, write bool) (*WAL, error) &#123; // 1. 读取所有 WAL 日志文件名称 names, err := readWALNames(lg, dirpath) if err != nil &#123; return nil, err &#125; // 2. 返回名称集合中最后一个小于或者等于 snap.Index 的名称索引（在文件名称集合中的索引） nameIndex, ok := searchIndex(lg, names, snap.Index) // 3. 检查 nameIndex 之后的文件名的 seq 是否有序递增的 if !ok || !isValidSeq(lg, names[nameIndex:]) &#123; return nil, ErrFileNotFound &#125; // open the wal files rcs := make([]io.ReadCloser, 0) rs := make([]io.Reader, 0) ls := make([]*fileutil.LockedFile, 0) // 3. 对返回的索引之后的文件进行遍历，同时构造 rcs、rs、ls 数组 for _, name := range names[nameIndex:] &#123; // 4. 构建文件路径 p := filepath.Join(dirpath, name) // 5. 如果是写模式打开，则进行如下操作 if write &#123; l, err := fileutil.TryLockFile(p, os.O_RDWR, fileutil.PrivateFileMode) if err != nil &#123; closeAll(rcs...) return nil, err &#125; ls = append(ls, l) // 写模式似乎有锁定文件属性 rcs = append(rcs, l) // 追加文件读取与关闭接口 &#125; else &#123; // 6. 如果是读模式打开，则进行如下操作 rf, err := os.OpenFile(p, os.O_RDONLY, fileutil.PrivateFileMode) if err != nil &#123; closeAll(rcs...) return nil, err &#125; ls = append(ls, nil) // 读模式并没有锁定文件属性 rcs = append(rcs, rf) // 同样追加文件读取与关闭接口 &#125; rs = append(rs, rcs[len(rcs)-1]) &#125; // 7. 构建用于文件读取与关闭的句柄集合 closer := func() error &#123; return closeAll(rcs...) &#125; // create a WAL ready for reading // 8. 利用以上信息构造 WAL 实例 w := &amp;WAL&#123; lg: lg, dir: dirpath, start: snap, // 初始化快照数据，实际上表示可以从哪一个索引位置处开始读 decoder: newDecoder(rs...), // decoder 又以上述打开文件的句柄集合为参数 readClose: closer, // 文件关闭句柄集合 locks: ls, // 具备锁定属性的文件集合 &#125; // 9. 若为写打开，则会重用读的文件描述符，因此不需要关闭 WAL 文件（需要释放锁）以直接执行追加操作 if write &#123; // write reuses the file descriptors from read; don't close so // WAL can append without dropping the file lock w.readClose = nil if _, _, err := parseWALName(filepath.Base(w.tail().Name())); err != nil &#123; closer() return nil, err &#125; // 10. 创建 FilePipeline 进行创建文件操作的空间预分配操作，具体是在 go routine 中循环执行空间分配操作， // 并将分配好的文件放到通道中，等待后面正式创建的时候使用 w.fp = newFilePipeline(w.lg, w.dir, SegmentSizeBytes) &#125; return w, nil&#125; // wal.go WAL文件打开以进行后续的读取与追加操作的相关逻辑已经阐述完毕。下面阐述日志项的读取相关逻辑。 WAL 日志项读取同样，在应用raftexample中启动初始化应用时(startRaft())中可能会涉及到日志的读取操作(w.ReadAll())。因此，我们来详细了解读取逻辑。大概地，它会读取WAL所有日志记录，当读取完毕后，就可以执行操作： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126// ReadAll reads out records of the current WAL.// If opened in write mode, it must read out all records until EOF. Or an error// will be returned.// If opened in read mode, it will try to read all records if possible.// If it cannot read out the expected snap, it will return ErrSnapshotNotFound.// If loaded snap doesn't match with the expected one, it will return// all the records and error ErrSnapshotMismatch.// TODO: detect not-last-snap error.// TODO: maybe loose the checking of match.// After ReadAll, the WAL will be ready for appending new records.func (w *WAL) ReadAll() (metadata []byte, state raftpb.HardState, ents []raftpb.Entry, err error) &#123; w.mu.Lock() defer w.mu.Unlock() // 1. 初始化一个空的记录项 rec := &amp;walpb.Record&#123;&#125; decoder := w.decoder // 2. 根据记录不同的类型（在数据结构部分已经详述），来执行不同操作 var match bool for err = decoder.decode(rec); err == nil; err = decoder.decode(rec) &#123; switch rec.Type &#123; // 2.1. 如果为 entryType 类型 case entryType: e := mustUnmarshalEntry(rec.Data) // 若读取到的日志的日志项的索引大于快照的索引，则将其追加到日志面集合， // 并且更新 WAL 的最后一条日志的日志索引 if e.Index &gt; w.start.Index &#123; ents = append(ents[:e.Index-w.start.Index-1], e) &#125; w.enti = e.Index // 2.2. 如果为 stateType 类型 case stateType: state = mustUnmarshalState(rec.Data) // 2.3 如果为 metadataType 类型，从此处来看 metadata 还可以用作检验 case metadataType: if metadata != nil &amp;&amp; !bytes.Equal(metadata, rec.Data) &#123; state.Reset() return nil, state, nil, ErrMetadataConflict &#125; metadata = rec.Data // 2.4 如果为 crcType 类型，则需要校验此 decoder 保存的 crc 检验和是否与记录的一致 case crcType: crc := decoder.crc.Sum32() // current crc of decoder must match the crc of the record. // do no need to match 0 crc, since the decoder is a new one at this case. if crc != 0 &amp;&amp; rec.Validate(crc) != nil &#123; state.Reset() return nil, state, nil, ErrCRCMismatch &#125; decoder.updateCRC(rec.Crc) // 2.5 如果为 snapshotType 类型 case snapshotType: var snap walpb.Snapshot pbutil.MustUnmarshal(&amp;snap, rec.Data) // 在反序列化之后，如果记录中的快照与 WAL 日志中快照不匹配，则报错 if snap.Index == w.start.Index &#123; if snap.Term != w.start.Term &#123; state.Reset() return nil, state, nil, ErrSnapshotMismatch &#125; match = true &#125; default: state.Reset() return nil, state, nil, fmt.Errorf("unexpected block type %d", rec.Type) &#125; &#125; // 3. 通过 WAL 日志文件中最后一条记录来做不同的处理 switch w.tail() &#123; case nil: // 如果是读模式，则并不需要读取所有的记录，因为最后一条记录可能是部分写的 // We do not have to read out all entries in read mode. // The last record maybe a partial written one, so // ErrunexpectedEOF might be returned. if err != io.EOF &amp;&amp; err != io.ErrUnexpectedEOF &#123; state.Reset() return nil, state, nil, err &#125; default: // 如果是写模式，则需要读取所有记录，直至返回 EOF // We must read all of the entries if WAL is opened in write mode. if err != io.EOF &#123; state.Reset() return nil, state, nil, err &#125; // decodeRecord() will return io.EOF if it detects a zero record, // but this zero record may be followed by non-zero records from // a torn write. Overwriting some of these non-zero records, but // not all, will cause CRC errors on WAL open. Since the records // were never fully synced to disk in the first place, it's safe // to zero them out to avoid any CRC errors from new writes. if _, err = w.tail().Seek(w.decoder.lastOffset(), io.SeekStart); err != nil &#123; return nil, state, nil, err &#125; if err = fileutil.ZeroToEnd(w.tail().File); err != nil &#123; return nil, state, nil, err &#125; &#125; err = nil if !match &#123; err = ErrSnapshotNotFound &#125; // 4. 读取完毕后，则关闭读操作 // close decoder, disable reading if w.readClose != nil &#123; w.readClose() w.readClose = nil &#125; w.start = walpb.Snapshot&#123;&#125; w.metadata = metadata // 5. 如果最后一条记录不为空，则创建 encoder，准备追加操作 if w.tail() != nil &#123; // create encoder (chain crc with the decoder), enable appending w.encoder, err = newFileEncoder(w.tail().File, w.decoder.lastCRC()) if err != nil &#123; return &#125; &#125; w.decoder = nil return metadata, state, ents, err&#125; // wal.go 另外，关于记录的decode操作，下面帖出简要的注释过程，基本上是encode操作的逆操作，但是加了一个校验的过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// decode 日志记录项func (d *decoder) decode(rec *walpb.Record) error &#123; rec.Reset() d.mu.Lock() defer d.mu.Unlock() return d.decodeRecord(rec)&#125;func (d *decoder) decodeRecord(rec *walpb.Record) error &#123; // 1. 需要读取器 if len(d.brs) == 0 &#123; return io.EOF &#125; // 2. 首先读与长度相关字段 l, err := readInt64(d.brs[0]) // 3. 解析出记录数据的字节大小以及对齐字节的大小 recBytes, padBytes := decodeFrameSize(l) // 4. 构建缓冲区用于存储具体读出的数据 data := make([]byte, recBytes+padBytes) // 5. 执行读实际数据的操作 if _, err = io.ReadFull(d.brs[0], data); err != nil &#123; return err &#125; // 6. 对数据执行反序列化操作 if err := rec.Unmarshal(data[:recBytes]); err != nil &#123; return err &#125; // 7. 对非 crcType 类型的记录，需要校验 crc，即检测记录的 crc 数值与 decoder 的检验和是否一致 // skip crc checking if the record type is crcType if rec.Type != crcType &#123; d.crc.Write(rec.Data) if err := rec.Validate(d.crc.Sum32()); err != nil &#123; if d.isTornEntry(data) &#123; return io.ErrUnexpectedEOF &#125; return err &#125; &#125; // 8. 更新目前已经检验的字节索引，下一次从此处开始检验 // record decoded as valid; point last valid offset to end of record d.lastValidOff += frameSizeBytes + recBytes + padBytes return nil&#125;// 为 encoder.encodeFrameSize() 函数的逆过程func decodeFrameSize(lenField int64) (recBytes int64, padBytes int64) &#123; // the record size is stored in the lower 56 bits of the 64-bit length recBytes = int64(uint64(lenField) &amp; ^(uint64(0xff) &lt;&lt; 56)) // non-zero padding is indicated by set MSb / a negative length if lenField &lt; 0 &#123; // padding is stored in lower 3 bits of length MSB padBytes = int64((uint64(lenField) &gt;&gt; 56) &amp; 0x7) &#125; return recBytes, padBytes&#125; // decoder.go 最后一个部分来简要阐述日志项的追加逻辑。 WAL 日志项追加同样，在【etcd raftexample 源码简析】中，当应用收到底层raft协议的指令消息时，会先进行写日志(rc.wal.Save(rd.HardState, rd.Entries))，也即此处的日志项追加操作。 123456789101112131415161718192021222324252627282930313233343536373839404142// 日志项追加操作func (w *WAL) Save(st raftpb.HardState, ents []raftpb.Entry) error &#123; w.mu.Lock() defer w.mu.Unlock() // short cut, do not call sync // 1. 若无 需要持久化的字段 且无日志项数据，则返回 if raft.IsEmptyHardState(st) &amp;&amp; len(ents) == 0 &#123; return nil &#125; // 2. MustSync 会检查当前的 Save 操作是否需要同步存盘 // 事实上，其逻辑大致为检查 log entries 是否为0，或者 candidate id 是否变化或者是 term 有变化， // 一旦这些条件中之一满足，则需要先执行存盘操作。 // 这些字段为 raft 实例需要持久化的字段，以便重启的时候可以继续协议 mustSync := raft.MustSync(st, w.state, len(ents)) // TODO(xiangli): no more reference operator // 3. 遍历日志项，并保存，在 saveEntry 中，会构建 Entry 记录，并更新 WAL 的 enti 索引字段 for i := range ents &#123; if err := w.saveEntry(&amp;ents[i]); err != nil &#123; return err &#125; &#125; // 4. 保存 HardState 字段，并保存，在 saveState 中，会构建 State 记录，但不会更新 enti 索引字段 if err := w.saveState(&amp;st); err != nil &#123; return err &#125; // 5. 获取最后一个 LockedFile 的大小（已经使用的） curOff, err := w.tail().Seek(0, io.SeekCurrent) if err != nil &#123; return err &#125; // 6. 若小于预分配空间大小 64MB，则直接返回即可 if curOff &lt; SegmentSizeBytes &#123; if mustSync &#123; // 若需同步刷盘操作，则要将已经 encode 的记录存盘 return w.sync() &#125; return nil &#125; // 6. 若大于预分配空间，则需要另外创建一个文件 return w.cut()&#125; // wal.go 其中涉及到的几个保存不同类型的记录的函数如下，比较简单： 12345678910111213141516171819202122232425262728293031func MustSync(st, prevst pb.HardState, entsnum int) bool &#123; // Persistent state on all servers: // (Updated on stable storage before responding to RPCs) // currentTerm // votedFor // log entries[] return entsnum != 0 || st.Vote != prevst.Vote || st.Term != prevst.Term&#125; // node.go 由 raft 协议库核心提供func (w *WAL) saveEntry(e *raftpb.Entry) error &#123; // TODO: add MustMarshalTo to reduce one allocation. b := pbutil.MustMarshal(e) // 构建 entryType 类型的 Record，并对记录进行编码 rec := &amp;walpb.Record&#123;Type: entryType, Data: b&#125; if err := w.encoder.encode(rec); err != nil &#123; return err &#125; // 更新 WAL 日志项中最后一条日志的索引号 w.enti = e.Index return nil&#125; // wal.gofunc (w *WAL) saveState(s *raftpb.HardState) error &#123; if raft.IsEmptyHardState(*s) &#123; return nil &#125; w.state = *s b := pbutil.MustMarshal(s) rec := &amp;walpb.Record&#123;Type: stateType, Data: b&#125; return w.encoder.encode(rec)&#125; // wal.go 最后若当前的文件的预分配的空间不够，则需另外创建新的文件来进行保存日志项。cut()函数流程如下，它的流程同Create()函数非常类似： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// cut closes current file written and creates a new one ready to append.// cut first creates a temp wal file and writes necessary headers into it.// Then cut atomically rename temp wal file to a wal file.// cut 函数实现了WAL文件切换的功能，即关闭当前WAL日志，创建新的WAL日志，继续用于日志追加。// 每个 WAL 文件的预分配空间为 64MB，一旦超过该大小，便需要创建新的 WAL 文件// 同样，cut 操作也会原子性的创建，能够创建临时文件来实现。func (w *WAL) cut() error &#123; // close old wal file; truncate to avoid wasting space if an early cut // 1. 关闭当前 WAL 文件，得到文件大小 off, serr := w.tail().Seek(0, io.SeekCurrent) if serr != nil &#123; return serr &#125; // 2. 截断文件 if err := w.tail().Truncate(off); err != nil &#123; return err &#125; if err := w.sync(); err != nil &#123; return err &#125; // 3. 构建新文件的路径（文件名），顺序递增 seq 及 enti fpath := filepath.Join(w.dir, walName(w.seq()+1, w.enti+1)) // create a temp wal file with name sequence + 1, or truncate the existing one // 4. 创建临时文件，其会使用先前 pipelinefile 预先分配的空间来执行此创建操作 newTail, err := w.fp.Open() if err != nil &#123; return err &#125; // update writer and save the previous crc // 5. 同 Create 函数类似，将文件加入到 WAL 的 locks 数组集合 w.locks = append(w.locks, newTail) // 6. 计算 crc 检验和，它是本文件之前的所有记录的检验和 prevCrc := w.encoder.crc.Sum32() // 7. 构建 WAL 实例的 encoder w.encoder, err = newFileEncoder(w.tail().File, prevCrc) if err != nil &#123; return err &#125; // 8. 先保存 检验和 if err = w.saveCrc(prevCrc); err != nil &#123; return err &#125; // 9. 再保存 metadata if err = w.encoder.encode(&amp;walpb.Record&#123;Type: metadataType, Data: w.metadata&#125;); err != nil &#123; return err &#125; // 10. 接着保存 HardState if err = w.saveState(&amp;w.state); err != nil &#123; return err &#125; // atomically move temp wal file to wal file if err = w.sync(); err != nil &#123; return err &#125; off, err = w.tail().Seek(0, io.SeekCurrent) if err != nil &#123; return err &#125; // 11. 重命名 if err = os.Rename(newTail.Name(), fpath); err != nil &#123; return err &#125; if err = fileutil.Fsync(w.dirFile); err != nil &#123; return err &#125; // reopen newTail with its new path so calls to Name() match the wal filename format newTail.Close() // 12. 重新打开并上锁新的文件（重命名之后的） if newTail, err = fileutil.LockFile(fpath, os.O_WRONLY, fileutil.PrivateFileMode); err != nil &#123; return err &#125; if _, err = newTail.Seek(off, io.SeekStart); err != nil &#123; return err &#125; // 13. 将新的文件加入数组 w.locks[len(w.locks)-1] = newTail // 14. 重新计算检验和 以及 encoder prevCrc = w.encoder.crc.Sum32() w.encoder, err = newFileEncoder(w.tail().File, prevCrc) if err != nil &#123; return err &#125; return nil&#125; // wal.go 至此关于WAL库的日志管理相关的接口已经分析完毕。简单总结一下，本文是从【etcd raftexample 源码简析】中对WAL库接口的调用切入（日志重放的操作），然后简要分析了WAL日志文件创建、WAL初始化（打开）、WAL日志项读取及WAL追加日志项等流程。最后，关于WAL库与如何与raft核心协议交互的内容，后面再了解。 参考文献 [1]. etcd-raft日志管理[2]. https://github.com/etcd-io/etcd/tree/master/wal]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>WAL 日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd-raft 网络传输源码简析]]></title>
    <url>%2F2019%2F01%2F10%2Fetcd-raft-%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上一篇文章简单分析了etcd raftexample的源码。我们知道，etcd raft只实现了raft协议核心部分，而将诸如日志、快照及消息的网络传输交给应用来管理。本文会简单分析raft集群用来实现消息在节点之间传输部分的相关逻辑。因为etcd raft会在节点之间传递各种消息指令，包括日志复制、快照拷贝等，这都需要通过应用来将对应的消息转发到集群中其它节点。简单而言，raft实现的节点之间的网络传输将消息的读写进行分离，即每两个节点之间存在两条消息通道，分别用作消息的接收与发送，另外针对不同类型的消息的收发，其也提供了不同的组件。本文会对大致的消息传输的流程进行介绍。 数据结构同上一篇博文类似，希望读者能够主动查看源码（主要涉及目录/etcd/etcdserver/api/rafthttp），文章只作参考。我们先来观察一下与网络传输相关的重要的数据结构，一般而言，只要理解了核心的数据结构的功能，基本就能推断相关的功能与大致的流程。网络传输最核心的结构为Transporter： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354type Transporter interface &#123; // Start starts the given Transporter. // Start MUST be called before calling other functions in the interface. // 在处理具体的消息收发之前，需要启动网络传输组件。它在应用初始化（如初始化 raftNode）时启动 // 网络传输组件 Start() error // Handler returns the HTTP handler of the transporter. // A transporter HTTP handler handles the HTTP requests // from remote peers. // The handler MUST be used to handle RaftPrefix(/raft) // endpoint. // 消息传输组件的消息处理器，它对不同消息配置不同的消息处理器（如pipelineHandler、streamHandler）。 Handler() http.Handler // Send sends out the given messages to the remote peers. // Each message has a To field, which is an id that maps // to an existing peer in the transport. // If the id cannot be found in the transport, the message // will be ignored. // 消息发送接口，即将消息发送到指定 id 的节点 Send(m []raftpb.Message) // SendSnapshot sends out the given snapshot message to a remote peer. // The behavior of SendSnapshot is similar to Send. // 快照数据发送接口 SendSnapshot(m snap.Message) // 后面都是关于节点的管理的方法，不作重点阐述 // AddRemote adds a remote with given peer urls into the transport. // A remote helps newly joined member to catch up the progress of cluster, // and will not be used after that. // It is the caller's responsibility to ensure the urls are all valid, // or it panics. AddRemote(id types.ID, urls []string) // AddPeer adds a peer with given peer urls into the transport. // It is the caller's responsibility to ensure the urls are all valid, // or it panics. // Peer urls are used to connect to the remote peer. AddPeer(id types.ID, urls []string) // RemovePeer removes the peer with given id. RemovePeer(id types.ID) // RemoveAllPeers removes all the existing peers in the transport. RemoveAllPeers() // UpdatePeer updates the peer urls of the peer with the given id. // It is the caller's responsibility to ensure the urls are all valid, // or it panics. UpdatePeer(id types.ID, urls []string) // ActiveSince returns the time that the connection with the peer // of the given id becomes active. // If the connection is active since peer was added, it returns the adding time. // If the connection is currently inactive, it returns zero time. ActiveSince(id types.ID) time.Time // ActivePeers returns the number of active peers. ActivePeers() int // Stop closes the connections and stops the transporter. Stop()&#125; // transport.go 需要补充一点的是，在上面的函数声明中，我们可以推测，节点采用peer的实例来进行消息的收发，由transport只是对外提供统一的接口，并提供逻辑框架。那remote又作何用？查看源码注释可以知道，remote是帮助新加入到集群的节点”追赶”当前集群正常节点的组件，除那之后，它没有其它作用。而相比之下，peer则代表raft节点与其它节点通信的实体。后面会详细阐述peer组件。下面来了解下Trasnporter的具体实现Transport结构： 1234567891011121314151617181920212223242526272829303132// Transport 实现了 Transporter 接口，用户使用其提供的接口实现完成消息收发type Transport struct &#123; Logger *zap.Logger DialTimeout time.Duration // maximum duration before timing out dial of the request // DialRetryFrequency defines the frequency of streamReader dial retrial attempts; // a distinct rate limiter is created per every peer (default value: 10 events/sec) DialRetryFrequency rate.Limit TLSInfo transport.TLSInfo // TLS information used when creating connection ID types.ID // local member ID URLs types.URLs // local peer URLs ClusterID types.ID // raft cluster ID for request validation Raft Raft // raft state machine, to which the Transport forwards received messages and reports status Snapshotter *snap.Snapshotter ServerStats *stats.ServerStats // used to record general transportation statistics // used to record transportation statistics with followers when // performing as leader in raft protocol LeaderStats *stats.LeaderStats // leader 节点用于记录传输消息到 follower 的相关数据统计 ErrorC chan error streamRt http.RoundTripper // roundTripper used by streams pipelineRt http.RoundTripper // roundTripper used by pipelines mu sync.RWMutex // protect the remote and peer map remotes map[types.ID]*remote // remotes map that helps newly joined member to catch up peers map[types.ID]Peer // peers map pipelineProber probing.Prober streamProber probing.Prober&#125; // transport.go 可以发现，Transport里面包含了一个对Raft状态机接口，容易想到，因为，当网络传输组件接收到涎宾，需要对消息进行处理，具体即需要交给Raft来处理，因此它提供这样一个接口。应用可以实现此接口以实现对接收到的消息进行处理。 1234567type Raft interface &#123; // 消息处理接口，raftNode 实现了此函数，并调用底层的 raft 协议库 node 的 Step 函数来处理消息 Process(ctx context.Context, m raftpb.Message) error IsIDRemoved(id uint64) bool ReportUnreachable(id uint64) ReportSnapshot(id uint64, status raft.SnapshotStatus)&#125; // transport.go 下面重点来查看一下peer数据结构（暂且忽略remote）。Peer接口定义如下： 12345678910111213141516171819202122232425type Peer interface &#123; // send sends the message to the remote peer. The function is non-blocking // and has no promise that the message will be received by the remote. // When it fails to send message out, it will report the status to underlying // raft. // 发送消息的接口，注意此接口是 non-blocking 的，但它不承诺可靠消息传输，但会报告出错信息 send(m raftpb.Message) // sendSnap sends the merged snapshot message to the remote peer. Its behavior // is similar to send. // 传输快照数据 sendSnap(m snap.Message) // update updates the urls of remote peer. update(urls types.URLs) // attachOutgoingConn attaches the outgoing connection to the peer for // stream usage. After the call, the ownership of the outgoing // connection hands over to the peer. The peer will close the connection // when it is no longer used. // 一旦接收到对端的连接，会把连接 attach 到节点 encoder 的 writer 中，以协同 encoder 和对端decoder的工作了 attachOutgoingConn(conn *outgoingConn) activeSince() time.Time stop()&#125; // peer.go 紧接着，我们了解下Peer接口的实现peer： 1234567891011121314151617181920212223242526272829type peer struct &#123; lg *zap.Logger localID types.ID // id of the remote raft peer node id types.ID r Raft status *peerStatus picker *urlPicker msgAppV2Writer *streamWriter writer *streamWriter pipeline *pipeline snapSender *snapshotSender // snapshot sender to send v3 snapshot messages msgAppV2Reader *streamReader msgAppReader *streamReader recvc chan raftpb.Message propc chan raftpb.Message mu sync.Mutex paused bool cancel context.CancelFunc // cancel pending works in go routine created by peer. stopc chan struct&#123;&#125;&#125; // peer.go 首先，需要说明的是，peer包含两种机制来发送消息：stream及pipeline。其中stream被初始化为一个长轮询的连接，在消息传输过程中保持打开的状态。另外，peer也提供一种优化后的stream以用来发送msgApp类型的消息，这种消息由leader向follower节点发送，其占据一大部分的消息内容。而对比之下，pipeline则是为http请求提供的http客户端。它只在stream还没有被建立的时候使用。另外，从peer结构中发现还有一个专门用于发送snap的发送器。换言之，针对不同的类型的消息采用不同的传输方式应该可以提高效率。 关键流程下面从组件启动开始监听、消息发送及消息接收三个方面来阐述相关的逻辑，这三个方面可能会相互穿插，但如果读者跟着代码来解读，相信也较容易理解。 启动监听下面会从raftNode(raft.go)中初始化代码开始索引(startRaft())，它使用rc.transport.Start()启动网络传输组件，并通过t.peers[id] = startPeer(t, urls, id, fs)启动各节点上的网络传输实体。在startPeer函数中，分别创建启动了pipeline以及stream，并提供了两个管道，一个作为消息的缓冲区，但因为消息会被阻塞处理（调用了Process()），可能花费较长时间，因此额外提供了一个pending的管理用于接收消息。 另外，我们接紧着先来查看一下，stream监听消息的逻辑（pipeline监听的逻辑更为简单，但流程类似，初始化，然后设置监听）。注意到在startPeer函数中有两行代码（针对不同的版本同时启动了相关的逻辑处理），启用了stream监听(p.msgAppV2Reader.start())，在start()方法中，开启了一个 go routine，它这个协程中(run()方法)，它会先与对端建立连接，通过dial()来实现，然后调用decodeLoope()函数来循环读取远程的节点发来的消息，并调用decode()函数进行消息解码处理。 消息发送下面梳理一下消息的发送的流程，即在raft.serveChannels()函数中，当raft应用层收到底层raft的消息指令时，需要把消息指令转发给其它peer（rc.transport.Send(rd.Messages)）。在Send()方法中，其大致逻辑为取出对端地址，然后对消息进行发送。在peer.send()函数中，它将消息发送到指定的writerc中，writerc是pipeline的一个结构p.pipeline.msgc，它在pipeline.start()中被初始化，并且在handler()方法中持续监听此通道的消息，一旦管道中有消息，则取出消息，并使用post()函数发送。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263func (p *peer) send(m raftpb.Message) &#123; p.mu.Lock() paused := p.paused p.mu.Unlock() if paused &#123; return &#125; // 1. 根据消息的类型选择具体的传输方式 writec, name := p.pick(m) select &#123; // 2. 将消息放到管道中 case writec &lt;- m: default: p.r.ReportUnreachable(m.To) if isMsgSnap(m) &#123; p.r.ReportSnapshot(m.To, raft.SnapshotFailure) &#125; // ... &#125;&#125; // peer.go// pick picks a chan for sending the given message. The picked chan and the picked chan// string name are returned.func (p *peer) pick(m raftpb.Message) (writec chan&lt;- raftpb.Message, picked string) &#123; var ok bool // Considering MsgSnap may have a big size, e.g., 1G, and will block // stream for a long time, only use one of the N pipelines to send MsgSnap. if isMsgSnap(m) &#123; return p.pipeline.msgc, pipelineMsg &#125; else if writec, ok = p.msgAppV2Writer.writec(); ok &amp;&amp; isMsgApp(m) &#123; return writec, streamAppV2 &#125; else if writec, ok = p.writer.writec(); ok &#123; return writec, streamMsg &#125; return p.pipeline.msgc, pipelineMsg&#125; // peer.gofunc (p *pipeline) start() &#123; p.stopc = make(chan struct&#123;&#125;) p.msgc = make(chan raftpb.Message, pipelineBufSize) p.wg.Add(connPerPipeline) for i := 0; i &lt; connPerPipeline; i++ &#123; go p.handle() &#125; // ...&#125; // pipeline.gofunc (p *pipeline) handle() &#123; defer p.wg.Done() for &#123; select &#123; case m := &lt;-p.msgc: start := time.Now() err := p.post(pbutil.MustMarshal(&amp;m)) // 发送消息 end := time.Now() if err != nil &#123; // ... &#125; // ... &#125;&#125; // pipeline.go 因此，整个消息发送的流程还是比较简单且清晰的。 消息接收还记得在raftNode初始化的过程中，有一行这样的代码go rc.serveRaft()，没错，它是用于启动节点网络传输监听。它将监听的处理程序设置为transport.Handler()，相关代码如下： 123456789101112131415161718192021222324func (rc *raftNode) serveRaft() &#123; url, err := url.Parse(rc.peers[rc.id-1]) ln, err := newStoppableListener(url.Host, rc.httpstopc) err = (&amp;http.Server&#123;Handler: rc.transport.Handler()&#125;).Serve(ln) // 开启监听，设置处理器 select &#123; case &lt;-rc.httpstopc: default: log.Fatalf("raftexample: Failed to serve rafthttp (%v)", err) &#125; close(rc.httpdonec)&#125; // raft.go// 为不同的消息类型设置了不同类型的处理器程序func (t *Transport) Handler() http.Handler &#123; pipelineHandler := newPipelineHandler(t, t.Raft, t.ClusterID) streamHandler := newStreamHandler(t, t, t.Raft, t.ID, t.ClusterID) snapHandler := newSnapshotHandler(t, t.Raft, t.Snapshotter, t.ClusterID) mux := http.NewServeMux() mux.Handle(RaftPrefix, pipelineHandler) mux.Handle(RaftStreamPrefix+"/", streamHandler) mux.Handle(RaftSnapshotPrefix, snapHandler) mux.Handle(ProbingPrefix, probing.NewHandler()) return mux&#125; // transport.go 我们具体到其中一个处理器进行查看，比如pipelineHandler，其相关代码如下： 123456789101112131415161718192021222324252627func (h *pipelineHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; // 1. 请求数据检查 if r.Method != "POST" &#123; w.Header().Set("Allow", "POST") http.Error(w, "Method Not Allowed", http.StatusMethodNotAllowed) return &#125; w.Header().Set("X-Etcd-Cluster-ID", h.cid.String()) limitedr := pioutil.NewLimitedBufferReader(r.Body, connReadLimitByte) b, err := ioutil.ReadAll(limitedr) // ... &#125; // 2. 消息解码 var m raftpb.Message if err := m.Unmarshal(b); err != nil &#123; // ... &#125; receivedBytes.WithLabelValues(types.ID(m.From).String()).Add(float64(len(b))) // 3. 调用 Raft 的 Process 函数进行消息处理 if err := h.r.Process(context.TODO(), m); err != nil &#123; switch v := err.(type) &#123; case writerToResponse: v.WriteTo(w) default: // ... &#125;&#125; // http.go 同样，整个消息的接收的流程也较为简单，针对不同类型的消息采用不同的接收及发送处理器，并将接收到的消息直接交给由应用定义的消息处理接口。至此，整个关于etcd-raft的网络传输相关逻辑的大致流程已经梳理完毕，介绍得比较浅显，只大概梳理了整个流程，如果读者想要深入了解，可以具体到每一个环节的代码深入分析。 参考文献 [1]. etcd-raft网络传输组件实现分析[2]. https://github.com/etcd-io/etcd/blob/master/etcdserver/api/rafthttp]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>网络传输</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd raftexample 源码简析]]></title>
    <url>%2F2019%2F01%2F09%2Fetcd-raftexample-%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[最近集中了解了ZAB、Raft及Paxos协议的基本理论，因此想进一步深入到源代码仔细体验一致性协议如何在分布式系统中发挥作用。虽然在 MIT 6.824 课程中有简单实现Raft协议，并基于Raft构建了一个粗糙的 kv 存储系统。但还是想了解下工业生产级别的Raft协议的实现内幕，故选择etcd进行解读。etcd是 CoreOS 基于Raft协议使用 go 开发的分布式 kv 存储系统，可用于服务发现、共享配置及其它利用一致性保障的功能（如leader选举及分布式锁、队列等）。这些功能ZooKeeper不也有提供？没错。它们都可以作为其它分布式应用的独立协调服务，这通过通用的一致性元信息存储来实现。但在易用性上，etcd可谓略胜一筹。因此，后续的一系列博客会简单对etcd各重要组成部分的源码进行简要分析（重点在Raft实现）。本文主要是分析etcd的raftexample的代码。它是etcd官方提供的如何使用etcd内部的Raft协议组件来构建分布式应用的一个简单示例最近集中了解了ZAB、Raft及Paxos协议的基本理论，因此想进一步深入到源代码仔细体验一致性协议如何在分布式系统中发挥作用。虽然在 MIT 6.824 课程中有简单实现Raft协议，并基于Raft构建了一个粗糙的 kv 存储系统。但还是想了解下工业生产级别的Raft协议的实现内幕，故选择etcd进行解读。etcd是 CoreOS 基于Raft协议使用 go 开发的分布式 kv 存储系统，可用于服务发现、共享配置及其它利用一致性保障的功能（如leader选举及分布式锁、队列等）。这些功能ZooKeeper不也有提供？没错。它们都可以作为其它分布式应用的独立协调服务，这通过通用的一致性元信息存储来实现。但在易用性上，etcd可谓略胜一筹。因此，后续的一系列博客会简单对etcd各重要组成部分的源码进行简要分析（重点在Raft实现）。本文主要是分析etcd的raftexample的代码。它是etcd官方提供的如何使用etcd内部的Raft协议组件来构建分布式应用的一个简单示。 （阐述etcd-raft的系列文章对应的etcd-raft的版本为 3.3.11，但遗憾实际上看的master unstable版本）etcd内部使用Raft协议对集群各节点的状态（数据、日志及快照等）进行同步。类似于ZooKeeper利用ZAB协议作为底层的可靠的事务广播协议。但etcd对Raft的实现有点特殊，它底层的Raft组件库只实现了Raft协议最核心的部分，这主要包括选主逻辑、一致性具体实现以及成员关系变化。而将诸如WAL、snapshot以及网络传输等模块让用户来实现，这明显增加了使用的难度，但对于应用本质上也更灵活。 本文会简单分析etcd提供的如何其核心的Raft协议组件来构建一个简单的高可用内存 kv 存储（其本质是一个状态机），用户可以通过 http 协议来访问应用（kv 存储系统），以对数据进行读写操作，在对日志进行读写过程中，Raft组件库能够保证各节点数据的一致性。其对应的源码目录为/etcd-io/etcd/tree/master/contrib/raftexample。另外，需要强调的是，本文的主题是利用Raft协议库来构建一个简单的 kv 存储，关于Raft协议库实现的细节不会过多阐述。若读者想继续了解此文，个人建议clone源代码，在阅读源代码的过程中，参考本文效果可能会更好，如果有理解错误的地方，欢迎指正！ 数据结构在按raftexample/main的示例完整解读整个流程之前，先熟悉几个重要的数据结构会有好处。此示例构建的应用为 kv 存储系统，因此，先来了解 kvstore定义的相关字段： 1234567// a key-value store backed by rafttype kvstore struct &#123; proposeC chan&lt;- string // channel for proposing updates mu sync.RWMutex kvStore map[string]string // current committed key-value pairs snapshotter *snap.Snapshotter&#125; // kvstore.go 关键结构成员解释如下： proposeC: 应用与底层Raft核心库之间的通信channel，当用户向应用通过 http 发送更新请求时，应用会将此请求通过channel传递给底层的Raft库。 kvStore: kv 结构的内存存储，即对应应用的状态机。 snapshotter: 由应用管理的快照snapshot接口。 接下来分析一下应用封装底层Raft核心库的结构raftNode，应用通过与raftNode结构进行交互来使用底层的Raft核心协议，它封装完整的Raft协议相关的逻辑（如WAL及snapshot等）。我们先列举它的相关处理逻辑，然后展示其结构内容。具体地逻辑如下： 将应用的更新请求传递给Raft核心来执行。 同时，将Raft协议已提交的日志传回给应用，以指示应用来将日志请求应用到状态机。 另外，它也处理由Raft协议相关的指令，包括选举、成员变化等。 处理WAL日志相关逻辑。 处理快照相关的逻辑。 将底层Raft协议的指令消息传输到集群其它节点。 123456789101112131415161718192021222324252627282930313233// A key-value stream backed by rafttype raftNode struct &#123; proposeC &lt;-chan string // proposed messages (k,v) confChangeC &lt;-chan raftpb.ConfChange // proposed cluster config changes commitC chan&lt;- *string // entries committed to log (k,v) errorC chan&lt;- error // errors from raft session id int // client ID for raft session peers []string // raft peer URLs join bool // node is joining an existing cluster waldir string // path to WAL directory snapdir string // path to snapshot directory getSnapshot func() ([]byte, error) lastIndex uint64 // index of log at start confState raftpb.ConfState snapshotIndex uint64 appliedIndex uint64 // raft backing for the commit/error channel node raft.Node raftStorage *raft.MemoryStorage wal *wal.WAL snapshotter *snap.Snapshotter snapshotterReady chan *snap.Snapshotter // signals when snapshotter is ready snapCount uint64 transport *rafthttp.Transport stopc chan struct&#123;&#125; // signals proposal channel closed httpstopc chan struct&#123;&#125; // signals http server to shutdown httpdonec chan struct&#123;&#125; // signals http server shutdown complete&#125; // raft.go 关键结构成员解释如下： proposeC: 同kvStore.proposeC通道类似，事实上，kvStore会将用户的更新请求传递给raftNode以使得其最终能传递给底层的Raft协议库。 confChangeC: Raft协议通过此channel来传递集群配置变更的请求给应用。 commitC: 底层Raft协议通过此channel可以向应用传递准备提交或应用的channel，最终kvStore会反复从此通道中读取可以提交的日志entry，然后正式应用到状态机。 node: 即底层Raft协议组件，raftNode可以通过node提供的接口来与Raft组件进行交互。 raftStorage: Raft协议的状态存储组件，应用在更新kvStore状态机时，也会更新此组件，并且通过raft.Config传给Raft协议。 wal: 管理WAL日志，前文提过etcd将日志的相关逻辑交由应用来管理。 snapshotter: 管理 snapshot文件，快照文件也是由应用来管理。 transport: 应用通过此接口与集群中其它的节点(peer)通信，比如传输日志同步消息、快照同步消息等。网络传输也是由应用来处理。 其它的相关的数据结构不再展开，具体可以查看源代码，辅助注释理解。 关键流程我们从main.go中开始通过梳理一个典型的由客户端发起的状态更新请求的完整流程来理解如何利用Raft协议库来构建应用状态机。main.go的主要逻辑如下： 123456789101112131415161718func main() &#123; // 解析客户端请求参数信息 ... proposeC := make(chan string) defer close(proposeC) confChangeC := make(chan raftpb.ConfChange) defer close(confChangeC) // raft provides a commit stream for the proposals from the http api var kvs *kvstore getSnapshot := func() ([]byte, error) &#123; return kvs.getSnapshot() &#125; commitC, errorC, snapshotterReady := newRaftNode(*id, strings.Split(*cluster, ","), *join, getSnapshot, proposeC, confChangeC) kvs = newKVStore(&lt;-snapshotterReady, proposeC, commitC, errorC) // the key-value http handler will propose updates to raft serveHttpKVAPI(kvs, *kvport, confChangeC, errorC)&#125; // main.go 显然，此示例的步骤较为清晰。主要包括三方面逻辑：其一，初始化raftNode，并通过 go routine 来启动相关的逻辑，实际上，这也是初始化并启动Raft协议组件，后面会详细相关流程。其二，初始化应用状态机，它会反复从commitC通道中读取raftNode/Raft传递给它的准备提交应用的日志。最后，启动 http 服务以接收客户端读写请求，并设置监听。下面会围绕这三个功能相关的逻辑进行阐述。 Raft 初始化首先我们来理顺Raft初始化的逻辑，这部分相对简单。 12345678910111213141516171819202122232425262728func newRaftNode(id int, peers []string, join bool, getSnapshot func() ([]byte, error), proposeC &lt;-chan string, confChangeC &lt;-chan raftpb.ConfChange) (&lt;-chan *string, &lt;-chan error, &lt;-chan *snap.Snapshotter) &#123; commitC := make(chan *string) errorC := make(chan error) rc := &amp;raftNode&#123; proposeC: proposeC, confChangeC: confChangeC, commitC: commitC, errorC: errorC, id: id, peers: peers, join: join, waldir: fmt.Sprintf("raftexample-%d", id), snapdir: fmt.Sprintf("raftexample-%d-snap", id), getSnapshot: getSnapshot, snapCount: defaultSnapshotCount, // 只有当日志数量达到此阈值时才执行快照 stopc: make(chan struct&#123;&#125;), httpstopc: make(chan struct&#123;&#125;), httpdonec: make(chan struct&#123;&#125;), snapshotterReady: make(chan *snap.Snapshotter, 1), // rest of structure populated after WAL replay &#125; go rc.startRaft() // 通过 go routine 来启动 raftNode 的相关处理逻辑 return commitC, errorC, rc.snapshotterReady&#125; // raft.go newRaftNode初始化一个Raft实例，并且将commitC、errorC及snapshotterReady三个通道返回给raftNode。raftNode初始化所需要的信息包括集群中其它peer的地址、WAL管理日志以及snapshot管理快照的目录等。接下来，分析稍为复杂的startRaft的逻辑： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657func (rc *raftNode) startRaft() &#123; if !fileutil.Exist(rc.snapdir) &#123; // 若快照目录不存在，则创建 if err := os.Mkdir(rc.snapdir, 0750); err != nil &#123; log.Fatalf("raftexample: cannot create dir for snapshot (%v)", err) &#125; &#125; rc.snapshotter = snap.New(zap.NewExample(), rc.snapdir) rc.snapshotterReady &lt;- rc.snapshotter oldwal := wal.Exist(rc.waldir) //判断是否已存在 WAL 日志（在节点宕机重启时会执行） rc.wal = rc.replayWAL() // 重放 WAL 日志以应用到 raft 实例中 rpeers := make([]raft.Peer, len(rc.peers)) for i := range rpeers &#123; // 创建集群节点标识 rpeers[i] = raft.Peer&#123;ID: uint64(i + 1)&#125; &#125; c := &amp;raft.Config&#123; // 初始化底层 raft 协议实例的配置结构 ID: uint64(rc.id), ElectionTick: 10, HeartbeatTick: 1, Storage: rc.raftStorage, MaxSizePerMsg: 1024 * 1024, MaxInflightMsgs: 256, MaxUncommittedEntriesSize: 1 &lt;&lt; 30, &#125; if oldwal &#123; // 若已存在 WAL 日志，则重启节点（并非第一次启动） rc.node = raft.RestartNode(c) &#125; else &#123; startPeers := rpeers if rc.join &#123; // 节点可以通过两种不同的方式来加入集群，应用以 join 字段来区分 startPeers = nil &#125; // 启动底层 raft 的协议实体 node rc.node = raft.StartNode(c, startPeers) &#125; // 初始化集群网格传输组件 rc.transport = &amp;rafthttp.Transport&#123; Logger: zap.NewExample(), ID: types.ID(rc.id), ClusterID: 0x1000, Raft: rc, ServerStats: stats.NewServerStats("", ""), LeaderStats: stats.NewLeaderStats(strconv.Itoa(rc.id)), ErrorC: make(chan error), &#125; // 启动（初始化）transport 的相关内容 rc.transport.Start() for i := range rc.peers &#123; // 为每一个节点添加集群中其它的 peer，并且会启动数据传输通道 if i+1 != rc.id &#123; rc.transport.AddPeer(types.ID(i+1), []string&#123;rc.peers[i]&#125;) &#125; &#125; // 启动 go routine 来处理本节点与其它节点通信的 http 服务监听 go rc.serveRaft() // 启动 go routine 来处理 raftNode 与 底层 raft 通过通道来进行通信 go rc.serveChannels()&#125; 应用初始化应用初始化相关代码较为简单，它只需要初始化内存状态机，并且监听从raftNode传来的准备提交的日志的channel即可，以将commitC读到的日志应用到内存状态机。应用初始化相关代码如下： 12345678func newKVStore(snapshotter *snap.Snapshotter, proposeC chan&lt;- string, commitC &lt;-chan *string, errorC &lt;-chan error) *kvstore &#123; s := &amp;kvstore&#123;proposeC: proposeC, kvStore: make(map[string]string), snapshotter: snapshotter&#125; // replay log into key-value map s.readCommits(commitC, errorC) // read commits from raft into kvStore map until error go s.readCommits(commitC, errorC) return s&#125; // kvstore.go 其中readComits即循环监听通道，并从其中取出日志的函数。并且如果本地存在snapshot，则先将日志重放到内存状态机中。 12345678910111213141516171819202122232425262728293031323334func (s *kvstore) readCommits(commitC &lt;-chan *string, errorC &lt;-chan error) &#123; for data := range commitC &#123; if data == nil &#123; // done replaying log; new data incoming // OR signaled to load snapshot snapshot, err := s.snapshotter.Load() if err == snap.ErrNoSnapshot &#123; return &#125; if err != nil &#123; log.Panic(err) &#125; log.Printf("loading snapshot at term %d and index %d", snapshot.Metadata.Term, snapshot.Metadata.Index) // 将之前某时刻快照重新设置为状态机目前的状态 if err := s.recoverFromSnapshot(snapshot.Data); err != nil &#123; log.Panic(err) &#125; continue &#125; // 先对数据解码 var dataKv kv dec := gob.NewDecoder(bytes.NewBufferString(*data)) if err := dec.Decode(&amp;dataKv); err != nil &#123; log.Fatalf("raftexample: could not decode message (%v)", err) &#125; s.mu.Lock() s.kvStore[dataKv.Key] = dataKv.Val s.mu.Unlock() &#125; if err, ok := &lt;-errorC; ok &#123; log.Fatal(err) &#125;&#125; // kvstore.go 开启 http 服务监听此应用对用户（客户端）提供 http 接口服务。用户可以通过此 http 接口来提交对应用的数据更新请求，应用启动对外服务及设置监听相关逻辑如下： 1234567891011121314151617181920// serveHttpKVAPI starts a key-value server with a GET/PUT API and listens.func serveHttpKVAPI(kv *kvstore, port int, confChangeC chan&lt;- raftpb.ConfChange, errorC &lt;-chan error) &#123; srv := http.Server&#123; Addr: ":" + strconv.Itoa(port), Handler: &amp;httpKVAPI&#123; store: kv, confChangeC: confChangeC, &#125;, &#125; go func() &#123; if err := srv.ListenAndServe(); err != nil &#123; log.Fatal(err) &#125; &#125;() // exit when raft goes down if err, ok := &lt;-errorC; ok &#123; log.Fatal(err) &#125;&#125; // httpapi.go 而接收并解析用户的请求相关逻辑如下所示，它将从用户接收到的对应用的读写请求，传递给raftNode，由raftNode传递至底层的raft协议核心组件来处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func (h *httpKVAPI) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; key := r.RequestURI switch &#123; case r.Method == "PUT": v, err := ioutil.ReadAll(r.Body) if err != nil &#123; log.Printf("Failed to read on PUT (%v)\n", err) http.Error(w, "Failed on PUT", http.StatusBadRequest) return &#125; // 将请求传递至 raftNode 组件，最终会传递到底层的 raft 核心协议模块 h.store.Propose(key, string(v)) // Optimistic-- no waiting for ack from raft. Value is not yet // committed so a subsequent GET on the key may return old value w.WriteHeader(http.StatusNoContent) case r.Method == "GET": if v, ok := h.store.Lookup(key); ok &#123; w.Write([]byte(v)) &#125; else &#123; http.Error(w, "Failed to GET", http.StatusNotFound) &#125; case r.Method == "POST": url, err := ioutil.ReadAll(r.Body) if err != nil &#123; log.Printf("Failed to read on POST (%v)\n", err) http.Error(w, "Failed on POST", http.StatusBadRequest) return &#125; nodeId, err := strconv.ParseUint(key[1:], 0, 64) if err != nil &#123; log.Printf("Failed to convert ID for conf change (%v)\n", err) http.Error(w, "Failed on POST", http.StatusBadRequest) return &#125; cc := raftpb.ConfChange&#123; Type: raftpb.ConfChangeAddNode, NodeID: nodeId, Context: url, &#125; h.confChangeC &lt;- cc // As above, optimistic that raft will apply the conf change w.WriteHeader(http.StatusNoContent) case r.Method == "DELETE": nodeId, err := strconv.ParseUint(key[1:], 0, 64) if err != nil &#123; log.Printf("Failed to convert ID for conf change (%v)\n", err) http.Error(w, "Failed on DELETE", http.StatusBadRequest) return &#125; cc := raftpb.ConfChange&#123; Type: raftpb.ConfChangeRemoveNode, NodeID: nodeId, &#125; h.confChangeC &lt;- cc // .. &#125;&#125; // httpapi.go 状态机更新请求在 httpapi.go 的逻辑中，我们选择 PUT 请求分支来进行分析。当它接收到用户发送的更新请求时。它会调用 kvstore的Propose函数，并将更新请求相关参数传递过去： 12345678func (s *kvstore) Propose(k string, v string) &#123; var buf bytes.Buffer // 编码后，传递至 raftNode if err := gob.NewEncoder(&amp;buf).Encode(kv&#123;k, v&#125;); err != nil &#123; log.Fatal(err) &#125; s.proposeC &lt;- buf.String()&#125; // kvstore.go 在kvstore将请求 buf 压到管道后，raftNode可以在管道的另一端取出，即在serverChannel函数取出请求，并交由底层 raft协议核心库来保证此次集群状态的更新。相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889func (rc *raftNode) serveChannels() &#123; snap, err := rc.raftStorage.Snapshot() if err != nil &#123; panic(err) &#125; // 利用 raft 实例的内存状态机初始化 snapshot 相关属性 rc.confState = snap.Metadata.ConfState rc.snapshotIndex = snap.Metadata.Index rc.appliedIndex = snap.Metadata.Index defer rc.wal.Close() // 初始化一个定时器，每次触发 tick 都会调用底层 node.Tick()函数，以表示一次心跳事件， // 不同角色的事件处理函数不同。 ticker := time.NewTicker(100 * time.Millisecond) defer ticker.Stop() // send proposals over raft // 开启 go routine 以接收应用层(kvstore)的请求（包括正常的日志请求及集群配置变更请求） go func() &#123; confChangeCount := uint64(0) // 循环监听来自 kvstore 的请求消息 for rc.proposeC != nil &amp;&amp; rc.confChangeC != nil &#123; select &#123; // 1. 正常的日志请求 case prop, ok := &lt;-rc.proposeC: if !ok &#123; rc.proposeC = nil &#125; else &#123; // blocks until accepted by raft state machine // 调用底层的 raft 核心库的 node 的 Propose 接口来处理请求 rc.node.Propose(context.TODO(), []byte(prop)) &#125; // 2. 配置变更请求类似处理 case cc, ok := &lt;-rc.confChangeC: if !ok &#123; rc.confChangeC = nil &#125; else &#123; confChangeCount++ cc.ID = confChangeCount rc.node.ProposeConfChange(context.TODO(), cc) &#125; &#125; &#125; // client closed channel; shutdown raft if not already close(rc.stopc) &#125;() // event loop on raft state machine updates // 开启 go routine 以循环处理底层 raft 核心库通过 Ready 通道发送给 raftNode 的指令 for &#123; select &#123; // 触发定时器事件 case &lt;-ticker.C: rc.node.Tick() // store raft entries to wal, then publish over commit channel // 1.通过 Ready 获取 raft 核心库传递的指令 case rd := &lt;-rc.node.Ready(): // 2. 先写 WAL 日志 rc.wal.Save(rd.HardState, rd.Entries) if !raft.IsEmptySnap(rd.Snapshot) &#123; rc.saveSnap(rd.Snapshot) rc.raftStorage.ApplySnapshot(rd.Snapshot) rc.publishSnapshot(rd.Snapshot) &#125; // 3. 更新 raft 实例的内存状态 rc.raftStorage.Append(rd.Entries) // 4. 将接收到消息传递通过 transport 组件传递给集群其它 peer rc.transport.Send(rd.Messages) // 5. 将已经提交的请求日志应用到状态机 if ok := rc.publishEntries(rc.entriesToApply(rd.CommittedEntries)); !ok &#123; rc.stop() return &#125; // 6. 如果有必要，则会触发一次快照 rc.maybeTriggerSnapshot() // 7. 通知底层 raft 核心库，当前的指令已经提交应用完成，这使得 raft 核心库可以发送下一个 Ready 指令了。 rc.node.Advance() case err := &lt;-rc.transport.ErrorC: rc.writeError(err) return case &lt;-rc.stopc: rc.stop() return &#125; &#125;&#125; // raft.go 上述关于 raftNode与底层Raft核心库交互的相关逻辑大致已经清楚。大概地，raftNode会将从kvstore接收到的用户对状态机的更新请求传递给底层raft核心库来处理。此后，raftNode会阻塞直至收到由raft组件传回的Ready指令。根据指令的内容，先写WAL日志，更新内存状态存储，并分发至其它节点。最后如果指令已经可以提交，即底层raft组件判定请求在集群多数节点已经完成状态复制后，则应用到状态机，具体由kvstore来执行。并且若触发了快照的条件，则执行快照操作，最后才通知raft核心库可以准备下一个Ready指令。关于 Ready结构具体内容，我们可以大致看一下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// Ready encapsulates the entries and messages that are ready to read,// be saved to stable storage, committed or sent to other peers.// All fields in Ready are read-only.// Ready 结构包装了事务日志，以及需要发送给其它 peer 的消息指令，这些字段都是只读的，且有些必须进行持久化，或者已经可以提交应用。type Ready struct &#123; // The current volatile state of a Node. // SoftState will be nil if there is no update. // It is not required to consume or store SoftState. // 包含了内存中的状态，即瞬时状态数据 *SoftState // The current state of a Node to be saved to stable storage BEFORE // Messages are sent. // HardState will be equal to empty state if there is no update. // 包含了持久化的状态，即在消息发送给其它节点前需要保存到磁盘 pb.HardState // ReadStates can be used for node to serve linearizable read requests locally // when its applied index is greater than the index in ReadState. // Note that the readState will be returned when raft receives msgReadIndex. // The returned is only valid for the request that requested to read. // 用于节点提供本地的线性化读请求，但其条件是节点的 appliedIndex 必须要大于 ReadState 中的 index，这容易理解，否则会造成客户端的读的数据的不一致 ReadStates []ReadState // Entries specifies entries to be saved to stable storage BEFORE // Messages are sent. // 表示在发送其它节点之前需要被持久化的状态数据 Entries []pb.Entry // Snapshot specifies the snapshot to be saved to stable storage. // 与快照相关，指定了可以持久化的 snapshot 数据 Snapshot pb.Snapshot // CommittedEntries specifies entries to be committed to a // store/state-machine. These have previously been committed to stable // store. // 可以被提交应用到状态机的状态数据 CommittedEntries []pb.Entry // Messages specifies outbound messages to be sent AFTER Entries are // committed to stable storage. // If it contains a MsgSnap message, the application MUST report back to raft // when the snapshot has been received or has failed by calling ReportSnapshot. // 当 Entries 被持久化后，需要转发到其它节点的消息 Messages []pb.Message // MustSync indicates whether the HardState and Entries must be synchronously // written to disk or if an asynchronous write is permissible. MustSync bool&#125; // /etcd/raft/node.go 日志管理raftexample中使用了etcd提供的通用日志库来管理WAL日志，我们下面来分析下应用管理日志的相关逻辑。在上面的状态机更新请求中，注意到当raftNode接收到raft核心传递的Ready指令，第一步就进行写WAL日志操作，这种操作较为常见，以避免更新丢失。值得一提的的，WAL日志也会在各节点进行同步。另外在startRaft函数中，即启动raftNode相关逻辑时，便进行了WAL日志重放rc.wal = rc.replayWAL()，我们详细看一下日志重放的流程： 123456789101112131415161718192021222324252627282930313233// replayWAL replays WAL entries into the raft instance.// 重放节点 WAL 日志，以将重新初始化 raft 实例的内存状态func (rc *raftNode) replayWAL() *wal.WAL &#123; log.Printf("replaying WAL of member %d", rc.id) // 1. 加载快照数据 snapshot := rc.loadSnapshot() // 2. 借助快照数据（的相关属性）来打开 WAL 日志。应用只会重放快照时间点（索引）之后的日志，因为快照数据直接记录着状态机的状态数据（这等同于将快照数据所对应的 WAL 日志重放），因此可以直接应用到内存状态结构。换言之，不需要重放 WAL 包含的所有的日志项，这明显可以加快日志重放的速度。结合 openWAL 函数可以得出结论。 w := rc.openWAL(snapshot) // 3. 从 WAL 日志中读取事务日志 _, st, ents, err := w.ReadAll() if err != nil &#123; log.Fatalf("raftexample: failed to read WAL (%v)", err) &#125; // 4. 构建 raft 实例的内存状态结构 rc.raftStorage = raft.NewMemoryStorage() if snapshot != nil &#123; // 5. 将快照数据直接加载应用到内存结构 rc.raftStorage.ApplySnapshot(*snapshot) &#125; rc.raftStorage.SetHardState(st) // append to storage so raft starts at the right place in log // 6. 将 WAL 记录的日志项更新到内存状态结构 rc.raftStorage.Append(ents) // send nil once lastIndex is published so client knows commit channel is current if len(ents) &gt; 0 &#123; // 更新最后一条日志索引的记录 rc.lastIndex = ents[len(ents)-1].Index &#125; else &#123; rc.commitC &lt;- nil &#125; return w&#125; // raft.go 通过查看上述的流程，关于 WAL日志重放的流程也很清晰。 快照管理快照(snapshot)本质是对日志进行压缩，它是对状态机某一时刻（或者日志的某一索引）的状态的保存。快照操作可以缓解日志文件无限制增长的问题，一旦达日志项达到某一临界值，可以将内存的状态数据进行压缩成为snapshot文件并存储在快照目录，这使得快照之前的日志项都可以被舍弃，节约了磁盘空间。我们在上文的状态机更新请求相关逻辑中，发现程序有可能会对日志项进行快照操作即这一行代码逻辑rc.maybeTriggerSnapshot()，那我们来具体了解快照是如何创建的： 1234567891011121314151617181920212223242526272829303132333435func (rc *raftNode) maybeTriggerSnapshot() &#123; // 1. 只有当前已经提交应用的日志的数据达到 rc.snapCount 才会触发快照操作 if rc.appliedIndex-rc.snapshotIndex &lt;= rc.snapCount &#123; return &#125; log.Printf("start snapshot [applied index: %d | last snapshot index: %d]", rc.appliedIndex, rc.snapshotIndex) // 2. 生成此时应用的状态机的状态数据，此函数由应用提供，可以在 kvstore.go 找到它的定义 data, err := rc.getSnapshot() if err != nil &#123; log.Panic(err) &#125; // 2. 结合已经提交的日志以及配置状态数据正式生成快照 snap, err := rc.raftStorage.CreateSnapshot(rc.appliedIndex, &amp;rc.confState, data) if err != nil &#123; panic(err) &#125; // 4. 快照存盘 if err := rc.saveSnap(snap); err != nil &#123; panic(err) &#125; compactIndex := uint64(1) // 5. 判断是否达到阶段性整理内存日志的条件，若达到，则将内存中的数据进行阶段性整理标记 if rc.appliedIndex &gt; snapshotCatchUpEntriesN &#123; compactIndex = rc.appliedIndex - snapshotCatchUpEntriesN &#125; if err := rc.raftStorage.Compact(compactIndex); err != nil &#123; panic(err) &#125; log.Printf("compacted log at index %d", compactIndex) // 6. 最后更新当前已快照的日志索引 rc.snapshotIndex = rc.appliedIndex&#125; // raft.go 需要注意的是，每次生成的快照实体包含两个方面的数据：一个显然是实际的内存状态机中的数据，一般将它存储到当前的快照目录中。另外一个为快照的索引数据，即当前快照的索引信息，换言之，即记录下当前已经被执行快照的日志的索引编号，因为在此索引之前的日志不需要执行重放操作，因此也不需要被WAL日志管理。快照的索引数据一般存储在日志目录下。 另外关于快照的操作还有利用快照进行恢复操作。这段逻辑较为简单，因为快照就代表内存状态机的瞬时的状态数据，因此，将此数据执行反序列化，并加载到内存状态机即可： 12345678910func (s *kvstore) recoverFromSnapshot(snapshot []byte) error &#123; var store map[string]string if err := json.Unmarshal(snapshot, &amp;store); err != nil &#123; return err &#125; s.mu.Lock() s.kvStore = store s.mu.Unlock() return nil&#125; // kvstore.go 至此，raftexmaple主要流程已经简单分析完毕。这是一个简单的应用etcd提供的raft核心库来构建一个 kv 存储的示例，虽然示例的逻辑较为简单，但它却符合前面提到的一点：raft核心库只实现了raft协议的核心部分（包括集群选举、成员变更等），而将日志管理、快照管理、应用状态机实现以及消息转发传输相关逻辑交给应用来处理。这使得底层的raft核心库的逻辑简单化，只要实现协议的核心功能（一致性主义的保证），然后提供与上层应用的接口，并通过channel与上层应用组件交互，如此来构建基于Raft协议的分布式高可靠应用。 参考文献 [1]. etcd-raftexample [2]. etcd-raft示例分析]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式协调服务</tag>
        <tag>分布式存储</tag>
        <tag>分布式缓存</tag>
        <tag>一致性协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单对比 Raft 及 ZAB 协议]]></title>
    <url>%2F2019%2F01%2F08%2F%E7%AE%80%E5%8D%95%E5%AF%B9%E6%AF%94-Raft-%E5%8F%8A-ZAB-%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[如果你了解过Raft协议、ZAB(ZooKeeper&#39;s Atomic Broadcast)协议及Paxos算法，你会发现它们本质上都是为了解决共识问题，即属于一种一致性算法（原子广播协议通常意义上可以等同于一致性协议）。但你可能会觉得相比于Paxos，ZAB与Raft可能更相似。从直观感受上，Paxos协议（Basic Paxos）更像是一种广义上的一致性算法的理论版本，它泛化了很多问题，并且没有基于特定场景的（工程）设计，因此相对而言也更难理解。而ZAB及Raft则像是具化的一致性化算法，并且简化了一些问题的前提设定，这也使得它们更易理解，也更易实现。本文对Raft协议及ZAB协议进行简单理解对比，主要讨论它们的不同之处。考虑到Raft论文给出了关于实现的详细细节，但官方提供的ZAB论文并没有涉及太多实现细节（Andr´e Medeiros 于 2012 年发表了一篇理论结合实践的论文），因此关于ZAB的细节是针对ZooKeeper的实现而言的。 首先，考虑一个问题，为什么需要选举出一个leader？我们知道，在Basic Paxos中并没有强调一定需要一个leader。但在Raft中包含了leader的强领导原则，而ZAB协议，正常的Broadcast阶段也需要一个leader。很自然地，若能够选举出一个leader节点，由其来统筹所有的客户端请求，可以方便并发控制，而且，因为leader是具备最新日志的节点，这使得日志同步过程也变得更简单，单向地由leader流向follower。另外，其实在日志恢复过程中，需要挑选出包含最新日志的节点，如果将它作为leader，那将使得失败恢复过程加快。最后，根本上而言，Raft及ZAB的对日志的应用都差不多归纳为一个二阶段过程，先收集follower反馈，然后，根据特定规则决定是否提交。那么收集反馈的工作若交由leader来处理，明显简化了协议流程。 接下来，我们简述Raft协议与ZAB协议中选举流程的对比情况。明显地，二者都是先选投票给自己，然后广播投票信息，另外它们都包含了选举轮次的概念（在Raft中为任期term，在ZAB中为round，两者的选举过程可能会涉及多轮），这确实比较类似，但需要注意的是，选举完成后，对于Raft而言，term即为leader所在的任期，而ZAB协议却额外使用了一个任期概念(epoch)。在具体的选举过程中，Raft协议规定一旦节点认为它能够为候选者投票，则在此轮投票过程中，都不会改变。而在ZAB协议中，集群中各节点反复交换选票信息（里面包含各自已提交的历史事务日志），以更新选票信息。二者都有quorum选票成功的概念。 与选举流程相关的另一个问题就是如何定义节点包含更新的事务日志。在Raft中，是通过依次比较term及index来确定。而ZAB协议是依次比较epoch及counter来决定（即通过比较zxid），值得注意的是选举轮次round也会作为比较因素。另外，在Raft中有一个很重要的一点为，被选举出来的leader只能提交本term的事务日志（不能显式提交之前term的未提交的事务日志，论文中详细阐述了原因），即在提交当前term的事务日志时，隐式（顺便）提交了之前term的未提交的（但已被复制到quorum节点）事务日志。在ZAB协议中，当leader选举未完成后，不会存在这样的情况，因为在Broadcast阶段之前，Synchronization阶段（Raft协议并未提供此阶段）会保证各节点的日志处于完全一致的状态。 另外，ZAB与Raft协议在选举阶段都使用了超时机制，以保证节点在超时时间内未收到投票信息，会自动转入下一轮的选举。具体而言，Raft的选举流程还可能会出现瓜分选票的情况(split vote)，因此，Raft通过随机化超时(randomized timeout)时间来缓解这个问题（不是解决）。而ZAB协议不会存在瓜分选票的情况，唯一依据是节点的选票的新旧程度。因此，理论上Raft可能存在活性的问题，即不会选举过程不会终止。而ZAB的选举时间应该会比Raft的选举时间更长（更频繁的交换选票信息）。 其次，在ZAB论文中有提到过，follower及leader由Broadcast阶段进入选举阶段，有各自判定依据，或者，这可以表述为，各节点如何触发leader选举过程。明显，在集群刚启动时，节点会先进行选举。另外，Raft协议通过周期性地由leader向follower发送心跳，心巩固leader的领导地位，一旦超时时间内，follower未收到心跳信息，则转为candidate状态、递增term，并触发选举流程（当leader发现消息回复中包含更高term时，便转为follower状态）。而在ZAB协议中，也是通过leader周期性向follower发送心跳，一旦leader未检测到quorum个回复，则会转为election状态，并进入选举流程（它会断开与follower的连接）。而此时follower一旦检测到leader已经卸任，同样会进入election状态，进入选举流程。 如果不幸leader发生了宕机，集群因此重新进行了选举，并生成了新的leader，上一个term并不会影响到当前的leader的工作。这在Raft及ZAB协议中分别可以通过term及epoch来判定决定。那上一任期遗留的事务日志如何处理？典型地，这包含是否已被quorum节点复制的日志。而对于之前term的事务日志，Raft的策略在前文已经叙述，不会主动提交，若已经被过半复制，则会隐式提交。而那些未过半复制的，可能会被删除。而ZAB协议则采取更激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机。 最后，是关于如何让一个新的节点加入协议流程的问题。在Raft中，leader会周期性地向follower发送心跳信息，里面包含了leader信息，因此，此节点可以重构其需要的信息。在ZAB中会有所不同，刚启动后，它会向转入election状态，并向所有节点发送投票信息，因此，正常情况下它会收到集群中其它的follower节点发送的关于leader的投票信息，当然也会收到leader的消息，然后从这些回复中判断当前的leader节点的信息，然后转入following状态，会周期性收到leader的心跳消息。需要注意的一点是，对于Raft而言，一个节点加入协议（不是新机器）不会阻塞整个协议的运行，因为leader保存有节点目前已同步的信息，或者说下一个需要同步的日志的索引，因此它只需要将后续的日志通过心跳发送给follower即可。而ZAB协议中是会阻塞leader收到客户端的写请求。因此，leader向follower同步日志的过程，需要获取leader数据的读锁，然后，确定需要同步给follower的事务日志，确定之后才能释放锁。值得注意的是，Raft的日志被设计成是连续的。而ZAB的日志被设计成允许存在空洞。具体而言，leader为每个follower保存了一个队列，用于存放所有变更。当follower在与leader进行同步时，需要阻塞leader的写请求，只有等到将follower和leader之间的差异数据先放入队列完成之后，才能解除阻塞。这是为了保证所有请求的顺序性，因为在同步期间的数据需要被添加在了上述队列末尾，从而保证了队列中的数据是有序的，从而进一步保证leader发给follower的数据与其接受到客户端的请求的顺序相同，而follower也是一个个进行确认请求（这不同于Raft，后者可以批量同步事务日志），所以对于leader的请求回复也是严格有序的。 最后，从论文来看，二者的快照也略有不同。Raft的快照机制对应了某一个时刻状态机数据（即采取的是准确式快照）。而ZooKeeper为了保证快照的高性能，采用一种fuzzy snapshot机制（这在ZooKeeper博文中有介绍），大概地，它会记录从快照开始的事务标识，并且此时不会阻塞写请求（不锁定内存），因此，它会对部分新的事务日志应用多次（事务日志的幂等特性保证了这种做法的正确性）。 顺便提一下，ZooKeepr为保证读性能的线性扩展，让任何节点都能处理读请求。但这带来的代价是过期数据。（虽然可通过sync read来强制读取最新数据）。而Raft不会出现过期数据的情况（具体如何保证取决于实现，如将读请求转发到leader）。 本文是从协议流程的各个阶段来对比Raft及ZAB协议。这里也提供更系统、更理论、更深入的对比（加入了Viewstamped Replication和Paxos一致性协议），它简要概括了论文。 关于ZAB协议与Paxos的区别，这里便不多阐述了。在ZAB文章中有简略介绍。另外，也可以在这里进行了解。这篇博文主要参考了文献[1]。 参考文献 [1]. Raft对比ZAB协议[2]. Vive La Différence: Paxos vs Viewstamped Replication vs Zab[3]. Van Renesse R, Schiper N, Schneider F B. Vive la différence: Paxos vs. viewstamped replication vs. zab[J]. IEEE Transactions on Dependable and Secure Computing, 2015, 12(4): 472-484.]]></content>
      <categories>
        <category>分布式系统</category>
        <category>一致性算法</category>
      </categories>
      <tags>
        <tag>一致性算法</tag>
        <tag>原子广播协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 ZAB 协议]]></title>
    <url>%2F2019%2F01%2F05%2F%E7%90%86%E8%A7%A3-ZAB-%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[ZAB 协议是应用于 ZooKeeper 分布式协调框架中的可靠原子广播协议(atomic broadcast protocol)（或者称之为全局有序的广播协议totaly ordered broadcast protocol，二者基本等价），这使得ZooKeeper实现了一个主从(primary-backup)模式的架构以通过主服务器接受客户端的数据变更请求，并使用ZAB协议将数据变更请求增量的传播(progpagate)到集群副本节点。在一定程度上，原子广播协议等价于一致性算法(consensus algorithm)，但它们的侧重点有所不同。本质上而言，ZooKeeper依赖于ZAB协议为其它分布式应用提供诸如配置管理、分布式互斥锁以及leader选举等协调原语服务。另一方面，ZooKeeper之所以能提供高可用(highly-available)（比如支持支持崩溃恢复efﬁcient crash-recovery）及高性能(highly-performance)（包括低延迟low latency、高吞吐量good throughput）的协调服务，部分原因是ZAB协议的核心设计（区别于paxos）及工程实现上的优化。大致地，ZAB协议可以分为四个阶段：leader 选举(leader election)、发现(Discovery)、同步(Synchronization)以及广播(Broadcast)，论文中将阶段一与二合并了，ZAB的实际工程实现耦合了阶段二与三（与论文论述并发完全一致），因此也可以称之为三个阶段。 本文主要阐述自己对ZAB协议的理解，这源自于ZAB相关的三篇论文的总结，但并非对原论文的完整翻译，因此更准确、更完整且更正式的内容可以参考原论文。值得注意的是，本论文并非如原论文那般详细、正式且全面地阐述ZAB协议，因此读者最好先阅读原论文，可以参考本文的协议解读。另外，本文不会过多阐述ZooKeeper的关键原理及系统架构，读者有兴趣可以参考文章，以大致了解ZooKeeper协调服务，并从应用层面整体把握ZAB协议。本文先介绍ZAB协议与二阶段提交的关系及与paxos作简单地对比论述。然后按照ZAB协议的四个阶段展开论述。因为本人暂未详细阅读过 Apache ZooKeeper/ZAB的实现源码，因此本文基本不会涉及与实现相关的细节，最后，考虑到本人知识的局限性，如有论述不当之处，谢谢指正！ 在阅读ZAB相关之前，本人已初步了解过raft和paxos这两个一致性算法，如果你有了解过raft或者paxos，那么ZAB也较容易理解。直观上理解，paxos和ZAB都可以视作改进的二阶段提交的协议，因为原始的二阶段（包括三阶段）提交协议因为至少受到网络分区影响而不能称被直接应用于分布式系统构建。实际上，ZAB协议本质上是一个简化的二阶段协议，从协议构成的阶段形式上看，leader首先提出一个请求(称之为request或者proposal)，等待follower对请求的投票结果返回，最后综合投票结果以提交请求。但相比原始的二阶段提交，ZAB中follower（或者称backup，协议不同阶段的不同称呼）不会abort来自leader的请求，具体地，它只要么接受(acknowledge)leader的proposal，要么放弃此leader，重新进入新的一轮选举。另外，避免abort操作也意味着在ZAB协议中，leader提交请求并不需要经集群中所有的follower的同意，即只要quorum个follower给leader返回了ACK，则leader即请求已经在集群中达成一致。简化的二阶段提交也使得ZAB不得不面临leader失败的情况，因此，ZAB整个协议流程中必须考虑如何从leader失败中恢复的问题。在二阶段提交中，如果协调者失败，可以选择abort事务（准确而言是三阶段，在这里我们并不作严格区分）。 那么对比于paxos算法，ZAB协议有什么优势（即利用ZAB可以方便、正确且高效实现或满足，但paxos则不能达到此要求）？这包括两个方面：其一，ZAB协议允许客户端并发地发送请求消息，换言之，ZAB（ZAB的primary）能够同时处理若干个消息请求，并能保证请求消息以客户端提出的顺序（请求消息的FIFO顺序）被广播到backup节点。事实上，ZAB的能够提供这样的保证的原因是，ZAB中所有的请求消息 （准确而言，所有的写请求消息，因为只有写请求消息才需要被广播，以保持数据的一致性）都由ZAB中的（唯一一个）primary进行广播。因此，ZAB需要保证协议的始终只存在一个primary节点。然而，paxos协议却不能简单直接地保证此属性。简单而言，在paxos协议中，若各primary并发地提出请求（请求之间遵循一定的依赖关系，即只能按照其提出的顺序应用到集群），那么learner并不能保证按照primary提出事务请求的顺序来学习（应用）消息请求。虽然可以一次性将多个proposal进行打包形成一个单独的proposal，即对这些请求进行批处理，但这会影响到整个算法的性能，而且单个打包的proposal数量也不能简单求得。 其二，ZAB协议被设计成能够迅速从失败（可能是由于leader或follower崩溃或者网络故障而断连）中恢复，即efficient recovery。ZAB使用事务标识机制(trasaction identification scheme)来全局排序事务日志，并保证准leader(prospective leader)能够容易获知需要同步或截断的日志项。详细而言，ZAB采用&lt;value, (epoch|counter)&gt;来唯一标识一条事务日志，其中value为事务日志的内容。epoch（也被称为是instance）为leader的任期，每一个任期内保证只存在一个leader，每当重新进入leader选举时，需要递增此任期，事实上，任期可用于保证当上一任的leader失败重启后不会干扰到当前任期的leader的广播操作（这同raft类似，都采用了epoch以在一段逻辑时间内唯一标识leader）。counter为事务消息计数器，每次重新选举时，需要清空counter，此值随着客户端发送的请求消息而递增。epoch与counter各占 32 位以构成事务的zxid，即作为事务日志的标识。这提供了一种简单且方便的方式来比较事务日志的新旧：先比较epoch，epoch越大，日志越新，当epoch相等时，比较counter，counter越大，日志越新。在此种事务日志标识机制下，只有具备了最新的事务日志的节点才允许将其日志项拷贝到准leader。换言之，准leader只需从各节点返回的所有的日志中选择包含最新的日志的节点，以从此节点拷贝其缺失的事务日志（若需要的话）（需要注意的是，事实上这属于Discover阶段中的协议内容，若把此阶段的协议归并到leader选举中，则选举算法阶段会直接选择包含最新的事务日志的节点作为准leader，因此避免了准leader去包含最新的日志项的节点去拷贝操作）。而paxos协议并未要求失败恢复的高效执行。详细地，在其恢复阶段，只凭借拥有最大的日志编号（在paxos中proposer提出的每一条日志都有一个全局唯一的编号）并不能要求其对应的值被新的leader接受(accpet)（更多可以参考paxos论文或者这里 ），因此，新的leader必须为其缺少的日志编号所对应的日志项重新执行paxos协议阶段一的协议内容。 另外值得注意的是，ZAB采用了TCP（可靠的）作为节点之间的通信协议，因此避免了部分网络故障问题（如消息乱序、重复及丢失），TCP协议能够保证消息能够按照其发出的顺序(FIFO)达到目标节点。但paxos和raft协议并不依赖此条件。 在介绍ZAB协议的各阶段前，先简要声明一些术语。在ZAB协议中，每个节点可能处于三种状态中的一种：following、leading及election。所有的leader和follower都会依次循环执行前述的三个阶段：Discover（发现集群中全局最新的事务）、Synchronization（由leader向follower同步其缺失的事务日志）及Broadcast（由leader向follower广播复制客户端的事务日志），且在阶段一之前，节点处于election状态，当它通过执行leader选举流程后，它会判断自己是否有资格成为leader（收到quorum张选票），否则成为follower，我们暂且将leader选举作为协议的第零个阶段。显然，正常情况下，协议只循环在Broadcast阶段中执行，一旦发生follower与leader断连，则节点自动切换到选举阶段。在节点进入Broadcast前，必须保证集群的数据处于一致的状态。另外，在本文中节点、机器或者server同义；请求日志、事务日志、提案及日志命令等也作同义处理（不严谨，但读者需明白它们的细微区别）。下面各阶段涉及的术语： − history: 已被节点所接收的提案日志信息− acceptedEpoch: 接收到的最后一个NEWEPOCH消息的epoch（由准leader生成的epoch）− currentEpoch: 接收到的最后一个NEWLEADER消息的epoch（旧的leader的epoch）− lastZxid: history中最后一个（最新的）事务提案的Zxid编号 Leader Election在leader选举阶段，所有节点的初始状态为election，当选举结束后，节点将选举的结果持久化。在此阶段，若节点p给节点q投票，则节点q称节点p的准leader(prospective leader)，直至进入阶段三Broadcast，准leader才能被称为正式的leader(estabilshed leader)，同时它也会担任primary的角色（这样设计有许多优点）。ZAB协议中，leader与primary的称呼基本表示同一个节点，只不过它们是作为同一节点不同阶段（承担不同功能）的称呼。在leader选举过程中，所有的节点最开始都会为自己投票，若经过若干轮的投票广播后，发现自己不够”资格”成为leader时，就会转入following的状态，否则转为leadering状态。leader选举阶段需要为后面的阶段(Broadcast)提供一个后置条件(postcondition)，以保证在进入Broadcast阶段前，各节点的数据处于一致的状态，所谓的postcondition可以表述为leader必须包含所有已提交(commit)的事务日志。 前文提到，部分leader选举实现会直接选择包含最新的日志的节点作为准leader，FLP(Fast Leader Election)正是这样一种选举算法的实现。它通过选择包含有最大的lastZxid（历史日志中最后一条日志记录的zxid）值的节点作为准leader（因为具有最大lastZxid日志的节点必定具有最全的历史日志提交记录），这可以为后阶段的事务广播提供postcondition保证，FLE由若干轮(round)选举组成，在每一轮选举中，状态为election节点之间互相交换投票信息，并根据自己获得的选票信息(发现更好的候选者)不断地更新自己手中的选票。注意，在FLE执行过程中，节点并不会持久化相关状态属性（因此round的值不会被存盘）。 − recvSet: 用于收集状态为election、following及leading的节点的投票信息− outOfElection: 用于收集状态为following及leading的节点的投票信息（说明选举过程已完成） 具体的选举的流程大致如下（更详细的流程可以参考论文)： 一旦开始选举，节点的初始状态为election，初始化选举超时时间，初始化recvSet及outOfElection。每个节点先为自己投票，递增round值，并把投票(vote包含节点的lastZxid及id)的消息（notification包含vote, id, state及round）广播给其它节点，即将投票信息发送到各节点的消息队列，并等待节点的回复，此后节点循环从其消息队列中取出其它节点发送给它的消息： 若接收到的消息中的round小于其当前的round，则忽略此消息。 若接收到的消息中的round大于节点当前的round，则更新自己的 round，并清空上一轮自己获得的选票的信息集合recvSet。此时，如果消息中的选票的lastZxid比自己的要新，则在本地记录自己为此节点投票，即更新recvSet，否则在本地记录为自己投票。最后将投票信息广播到其它节点的消息队列中。 如果收到的消息的round与节点本地的round相等，即表示两个节点在进行同一轮选举。并且若此消息的state为election并且选票的lastZxid比自己的要新，则在本地记录自己为此节点投票，并广播记录的投票结果。若消息的提案号比自己旧或者跟自己一样，则记录这张选票。 整个选举过程中（节点的状态保持为election，即节点消息队列中的消息包含的状态），若节点检测到自己或其它某个节点得到超过集群半数的选票，自己切换为leading/following状态，随即进入阶段二(Recovery)（FLE选举后，leader具备最新的历史日志，因此，跳过了Discovery阶段，直接进入Synchronization阶段。否则进入Discovery阶段）。 另外，如果在选举过程中，从消息队列中检索出的消息的状态为following或者leading，说明此时选举过程已经完成，因此，消息中的vote即为leader的相关的信息。 具体而言，如果此时消息中的round与节点相同，先在本地记录选票信息，然后若同时检测到消息中的状态为leading，则节点转为following状态，进入下一阶段，否则若非leading状态，则需检查recvSet来判断消息中的节点是否有资格成为leader。 否则，如果round不同，此时很有可能是选举已经完成。此时节点需要判断消息被投票的节点（有可能为leader）是否在recvSet或outOfElection字典中具备quorum张选票，同时，还要检查此节点是否给自己发送给投票信息，而正式确认此节点的leading状态。这个额外的检查的目的是为了避免这种情况：当协议非正常运行时，如leader检测到与follower失去了心跳连接，则其会自动转入election状态，但此时follower可能并没有意识到leader已经失效（这需要一定的时间，因为不同于raft，在ZAB协议中，leader及follower是通过各自的方式来检测到需要重新进行选举过程）。如果在follower还未检测到的期间内，恰好有新的节点加入到集群，则新加入的节点可能会收到集群中quorum个当前处于following状态的节点对先前的leader的投票（此时它已转入election状态），因此，此时仍需要此新加入的节点进行额外的判断，即检查它是否会收到leader发给它的投票消息（如果确实存在）。 最后，补充一点，ZAB的选举过程同样加入了超时机制（且很可能并非线性超时），以应对当节点超时时间内未收到任何消息时，重新进入下一轮选举。 DiscoveryDiscovery阶段的目的是发现全局（quorum个也符合条件）最新的事务日志，并从此事务日志中获取epoch以构建新的epoch，这可以使历史epoch的leader失效，即不再能提交事务日志。另外，一旦一个处于非leadering状态节点收到其它节点的FOLLOWERINFO消息时，它将拒绝此消息，并重新发起选举。简而言之，此阶段中每一个节点会与它的准leader进行通信，以保证准leader能够获取当前集群中所包含的被提交的最新的事务日志。更详细的流程阐述如下： 首先，由follower向其准leader发送FOLLOWERINFO（包含节点的accpetedEpoch）消息。当leader收到quorum个FOLLOWERINFO消息后，从这些消息中选择出最大的epoch值，并向此quorum个follower回复NEWEPOCH（包含最大的epoch）消息。接下来，当follower收到leader的回复后，将NEWEPOCH中的epoch与其本地的epoch进行对比，若回复消息中的epoch更大，则将自己本地的accpetedEpoch设置为NEWEPOCH消息中的epoch值，并向leader回复ACKEPOCH（包含节点的currentEpoch，history及lastZxid）消息。反之，重新进入选举阶段，即进入阶段零。当leader从quorum个节点收到follower的ACKEPOCH消息后，从这些ACKEPOCH消息中(history)查找出最新的（先比较currentEpoch，再比较lastZxid）历史日志信息，并用它覆盖leader本地的history事务日志。随即进入阶段二。 SynchronizationSynchronization阶段包含了失败恢复的过程，在这个阶段中，leaer向follower同步其最新的历史事务日志。简而言之，leader向follower发送其在阶段一中更新的历史事务日志，而follower将其与自己本地的历史事务日志进行对比，如果follower发现本地的日志集更旧，则会将这些日志应用追加到其本地历史日志集合中，并应答leader。而当leader收到quorum个回复消息后，立即发送commit消息，此时准leader(prospective leader)变成了正式leader(established leader)。更详细的流程阐述如下： 首先由准leader向quorum发送NEWLEADER（包含阶段一中的最大epoch及history），当follower收到NEWLEADER消息后，其对比消息中的epoch与其本地的acceptedEpoch，若二者相等，则更新自己的currentEpoch并且接收那些比自己新的事务日志，最后，将本地的history设置为消息中的history集合。之后向leader回复ACKNEWLEADER消息。若leader消息中的epoch与本地的不相等，则转为election状态，并进入选举阶段。当leader收到quorum个ACKNEWLEADER消息后，接着向它们发送COMMIT消息，并进入阶段三。而follower收到COMMIT消息后，将上一阶段接收的事务日志进行正式提交，同样进入阶段三。 事实上，在有些实现中，会对同步阶段进行优化，以提高效率。具体而言，leader实际上拥有两个与日志相关的属性（在前述中，我们只用了history来描述已提交的事务日志），其一为outstandingProposals：每当leader提出一个事务日志，都会将该日志存放至outstandingProposals字典中，一旦议案被过半认同了，就要提交该议案，则从outstandingProposals中删除该议案；其二为toBeApplied：每当准备提交一个议案，就会将该议案存放至toBeApplied中，一旦议案应用到ZooKeeper的内存树中了，就可以将该议案从toBeApplied集合中删除。因此，这将日志同步大致分为两个方面： 一方面，对于那些已应用的日志（已经从toBeApplied集合中移除）可以通过不同的方式来进行同步：若follower消息中的lastZxid要小于leader设定的某一个事务日志索引(minCommittedLog)，则此时采用快照会更高效。也存在这样一种情况，follower中包含多余的事务日志，此时其lastZxid会大于leader的最新的已提交的事务日志索引(maxCommittedLog)，因此，会把多余的部分删除。最后一种情况是，消息中的lastZxid位于二个索引之间，因此，leader需要把follower缺失的事务日志发送给follower。当然，也会存在二者存在日志冲突的情况，即leader并没有找到lastZxid对应的事务日志，此时需要删除掉follower与leader冲突的部分，然后再进行同步。 另一方面，对于那些未应用的日志的同步方式为：对于toBeApplied集合中的日志（已提交，但未应用到内存），则直接将大于follower的lastZxid的索引日志发送给follower，同时发送提交命令。对于outstandingProposals的事务日志，则同样依据同样的规则发送给follower，但不会发送提交命令。 需要注意的的，在进行日志同步时，需要先获取leader的内存数据的读锁（因此在释放读锁之前不能对leader的内存数据进行写操作）。但此同步过程仅涉及到确认需要同步的议案，即将需要被同步的议案放置到对应follower的队列中即可，后续会通过异步方式进行发送。但快照同步则是同步写入阻塞。 当同步完成后，leader会几follower发送UPTODATE命令，以表示同步完成。此时，leader开始进入心跳检测过程，周期性地向follower发送心跳，并检查是否有quorum节点回复心跳，一旦出现心跳断连，则转为election状态，进入leader选举阶段。 BroadcastBroadcast为ZAB正常工作所处的阶段。当进入此阶段，leader会调用ready(epoch)，以使得ZooKeeper应用层能够开始广播事务日志到ZAB协议。同时，此阶段允许动态的加入新节点(follower)，因此，leader必须在新节点加入的时候，与这些节点建立通信连接，并将最新日志同步到这些节点。更详细的流程阐述如下： 当leader(primary)收到客户端发送的消息（写）请求value，它将消息请求转化为事务日志(epoch, &lt;value,zxid&gt;), zxid=(epoch|counter)，广播出去。当follower从leader收到事务请求时，将此事务日志追加到本地的历史日志history，并向leader回复ACK。而一旦leader收到quorum个ACK后，随即向quorum节点发送COMMIT日志，当follower收到此命令后，会将未提交的日志正式进行提交。需要注意的是，当有新的节点加入时，即在Broadcast阶段，若leader收到FOLLOWINFO消息，则它会依次发送NEWEPOCH和NEWLEADER消息，并带上epoch及history。收到此消息的节点会将设置节点本地的epoch并更新本地历史日志。 根据在Synchronization提到的两个数据结构outstandingProposals及toBeApplied。因此，事实上，leader会将其提出的事务日志放至outstandingProposals，如果获得了quorum节点的回复，则会将其从outstandingProposals中移除，并将事务日志放入toBeApplied集合，然后开始提交议案，即将事务日志应用到内存中，同时更新lastZxid，并将事务日志保存作缓存，同时更新maxCommittedLog和minCommittedLog。 最后，讨论ZAB协议中两个额外的细节： 若leader宕机，outstandingProposals字典及toBeApplied集合便失效（并没有持久化），因此它们对于leader的恢复并不起作用，而只是在Synchronization阶段（该阶段实际上是leader向follower同步日志，即也可以看成是follower挂了，重启后的日志同步过程），且同步过程包含快照同步及日志恢复。 另外，在日志恢复阶段，协议会将所有最新的事务日志作为已经提交的事务来处理的，换言之，这里面可能会有部分事务日志还未真正提交，而这里全部当做已提交来处理。（这与raft不同，个人认为，这并不会产生太大影响，因为在日志恢复过程中，并不会恢复那些未被quorum节点通过的事务日志，只是在ZAB在提交历史任期的日志的时机与raft不同，rfat不会主动提交历史任期未提交的日志，只在新的leader提交当前任期内的日志时顺便提交历史的未提交但已经复制到quorum节点的日志项）。 需要注意的是，本文使用的一些术语与Yahoo!官方发表的论文[2]可能不一样（个人参照另外一篇论文[4]阐述），但它们的问题意义相同。而且，对于每个阶段，本文先是大概阐述其流程，然后从实际实现的角度进行拓展，希望不要造成读者的困扰。另外，实际工程实现可能并不完全符合这些阶段，而且ZooKeeper各版本的实现也可能会包含不同的工程优化细节。具体参考论文，当然，查看ZooKeeper源码实现可能更清晰。 参考文献 [1] Gray J N. Notes on data base operating systems[M]//Operating Systems. Springer, Berlin, Heidelberg, 1978: 393-481.[2] Junqueira F P, Reed B C, Serafini M. Zab: High-performance broadcast for primary-backup systems[C]//Dependable Systems &amp; Networks (DSN), 2011 IEEE/IFIP 41st International Conference on. IEEE, 2011: 245-256.[3] Reed B, Junqueira F P. A simple totally ordered broadcast protocol[C]//proceedings of the 2nd Workshop on Large-Scale Distributed Systems and Middleware. ACM, 2008: 2.[4] Medeiros A. ZooKeeper’s atomic broadcast protocol: Theory and practice[J]. Aalto University School of Science, 2012, 20.[5] 倪超. 从 Paxos 到 Zookeeper: 分布式一致性原理与实践[J]. 2015.[6]. ZooKeeper的一致性算法赏析]]></content>
      <categories>
        <category>分布式系统</category>
        <category>一致性算法</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>一致性协议</tag>
        <tag>原子广播协议</tag>
        <tag>选举算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 Paxos Made Simple]]></title>
    <url>%2F2018%2F12%2F20%2F%E7%90%86%E8%A7%A3-Paxos-Made-Simple%2F</url>
    <content type="text"><![CDATA[Paxos 算法在分布式系统领域早已是如雷贯耳般的存在，基本成为了分布式一致性协议的代名词，想必对于任何一个从事分布式领域的人来说都充满敬畏——即感叹算法的精巧，也畏惧算法的晦涩。Leslie Lamport 早在 1980s 就写作了描述 Paxos最原始的论文 《The Part-Time Parliament》，但因其难以理解（与论述方式相关?）而没有得到过多的关注（相反，Lamport 本人却坚持认为自己采用了一种更加形象恰当且容易理解的方式阐述，摈弃了传统学术论文的”死板“风格）。在 2001年，Lamport 对 Paxos 论文进行整理简化并发表了《Paxos Made Simple》，引起了广泛关注。论文的第一句话 The Paxos algorithm, when presented in plain English, is very simple 可以体会到 Leslie Lamport 似乎仍旧对众人对 Paxos 冠以难理解性的言行的”不屑“。 最近重新阅读了《Paxo Made Simple》论文，想从论文本身出发，阐述自己对论文的一些（浅显，且可能有误）的理解，因为还未了解Paoxs系列其它论文（如 Fast Paxos），因此个人的理解可能存在一定的局限性。同时，个人坚持认为，反复读原始论文是理解算法的最根本途径，最好结合开源实现进行理解（开源实现一般都会对算法进行工程上的优化与”妥协”）。当然读完原论文可能会有困惑，因此，也可以尝试参考别人的理解（从不同的角度思考问题，或许会有收获），但最终还是要回归论文。如果你对本文有兴趣，你需要先阅读论文。另外，你需要先了解其应用场景。本文先简述其应用场景，然后按照原论文推理的逻辑和步骤来逐步阐述自己对这些步骤的理解。 Paxos 应用场景Paxos用于解决分布式场景的一致性问题。换言之，Paxos是一个一致性（共识）算法。这个说法可能比较笼统宽泛，因为你可能在很多领域了解过一致性问题（虽然这些解释背后的含义可能也存在共性）。比如对于分布式存储，典型的Nosql数据库领域，所谓的一致性可能是要求客户端能够读取其最新写入的数据。换言之，最近写入的数据需要对所后续的客户端的读都可见，强调的是可见性。这可以用线性一致性(Linearizability)来描述；再者，在数据库领域，顺序一致性(serializability)是事务正确性的保证，即强调正确性；而复制状态机(replicated state machine)是很多一致性算法的典型应用场景（包括Paxos），其强调的是让一组互为备份的节点执行一系列相同的命令日志来保证存储在此节点集合中的数据的一致，以达到容错目的。另外，从一致性算法的强弱角度来考虑，一致性算法包括强一致性，弱一致性以及最终一致性。而Paxos则属于强一致性算法。另外，我们再简单了共识算法的正确性的保证： Agreement - all N (or a majority) nodes decide on the same value Validity - the value that is decided upon must have been proposed by some node in N Termination - all nodes eventually decide 这些都容易理解，比如，对于Agreement而言，若某个算法都不难最后表决出来的值是同一个，那就不能称之为共识算法，而Validity可能觉得是很显然的事情，可以从这样一个角度思考，如果所有节点始终回复相同的值，而不管实际提出的值是什么，那么Agreement能够得到保证，但却违反了Validity条件。最后的Termination保证了算法最终能够停止，即我们不仅希望你们能够做表决，也希望能够最终表决出一个结果，否则此表决过程没有意义。而Paxos论文提到的safty requirement 如下： Only a value that has been proposed may be chosen, Only a single value is chosen, and A process never learns that a value has been chosen unless it actually has been. 明确提出了，只保证了前面两点(Agreement及Validity，只是换了一种说法，并颠倒1与2的顺序)，换言之，理论上而言，Paxos是存在活锁的问题，后面会详细阐述。当然Paxos算法只考虑节点存在non-Byzantine及asynchronous网络的条件下。 那么Paxos如何应用于复制状态机呢？简单而言，Paxos试图通过对所有的（客户端发送的）命令日志（如SET X=1）进行全局编号，如果能够全局编号成功，那么互为备份的节点按照此全局编号顺序来执行对应的命令日志，即能够保证数据的一致性。在一个分布式系统中，若执行命令日志序列前，系统处于一致的状态，且节点都执行了相同的命令日志序列，那么最终整个系统也处于一个一致的状态。因此为了保证每个节点都能够以相同的顺序执行命令日志，所有节点必须对于每一条命令日志达成共识（比如，有两个节点尝试提交命令日志，节点a尝试让v=i，而节点b尝试让v=j，明显这会产生冲突，因此需要协调以达成共识，即最终v的值要么是i，那么所有节点都会认为v=a），即每个节点看到的指令顺序是一致的。显然，问题在于不同的节点可能接收到的日志的编号的顺序是不同的，因此不能按照单个节点的意愿进行命令日志的执行（否则会出现数据一致的情况），换言之，所有节点需要相互通信协调，每个节点都对全局编号的排序进行表决。每一次表决，只能对一条命令日志（数据）进行编号，这样才能保证确定的日志执行，这也正是Paxos所做的，即Paxos的核心在于确保每次表决只产生一条命令日志（一个value，这里的命令日志可以表示一个操作，也可以表示一个值）。当然某一次表决成功（达成一致）并不意味着此时所有节点的本地的value都相同，因为可能有节点宕机，即通常而言，只要保证大多数(quorum)个节点存储相同的value即可。 论文理解这里省略了协议的一些基本术语及概念。但还是再强调一下，协议对某个数据达成一致的真正含义提什么，其表示proposer、acceptor及learner都要认为同一个值被选定。详细而言，对于acceptor而言，只要其接受了某个proposal，则其就认定该proposal的value被选定了。而对于proposer而言，只要其issue的proposal被quorum个acceptor接受了，则其就认定该proposal对应的value就被选定了。最后对于learner而言，需要acceptor将最终决定的value发送给它，则其就认定该value被选定了。另外，acceptor是可能有多个的，因为单个acceptor很明显存在单点故障的问题。 我们直接一步步来观察 Lamport 论文中的推导，以达到最终只有一个值被选中的目的（确定一个值），即Only a single value is chosen。这句话很重要，它暗示了不能存在这样的情形，某个时刻v被决定为了i，而在另一时刻v又被决定成了j。 P1. An acceptor must accept the ﬁrst proposal that it receives. 乍一看此条件，让人有点不知所措。论文前一句提到，在没有故障的情况，我们希望当只有一个proposer的时候，并且其只提出一个value时，能够有一个value被选中，然后就引出了P1。这是理所当然的，因为此acceptor之前没有收到任何的value，或许后面也不会收到了，那它选择此value就无可厚非。换言之，此时acceptor并没有一个合适的拒绝策略，只能先选择这个值。但很明显，这个条件远不能达到我们的目的（比如，多个acceptor可能会接受到不同的proposer提出的不同的value，直接导致不同的value被选定，因此不可能只决定一个值）。而且仔细想想，作者提出的这个条件确实比较奇怪，因为你不知道此条件与最终协议的充要条件有什么联系，而且，你可能会想，既然已经选择了第一个值，若后面又有第二个proposal来了应该如何处理（才能保证最终只选择一个值）。直观上我们可能会推断出，每个acceptor只接受一个proposal是行不通的，即它可能会接受多个proposal，那既然会接受多个proposal，这些proposal肯定是不同的（至少是不同时间点收到的），因此需要进行区分衡量，这也正是提案编号proposal id的作用。另外还暗示了一点，正常情况下，对于proposer而言，一个proposal不能由只被一个acceptor接受了就认定其value被选定，必须要由大多数的（即法定集合quorum）选定才能说这个值被选定了。 直观上理解，虽然我们允许了一个acceptor可以accept多个proposal，但为了保证最终只能决定一个value，因此很容易想到的办法是保证acceptor接受的多个proposal的value相同。这便引出了P2： P2. If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v. 为了保证每次只选定一个值，P2规定了如果在一个value已经被选定的情况下，若还有的proposer提交value，那么之后（拥有更高编号higher-numbered）被accept的value应该与之前已经被accept的保持一致。这是一个比较强的约束条件。显然，如果能够保证P2，那么也能够够保证Paxos算法的正确性。 但从另一方面考虑，对比P1与P2，感觉它们有很大的不同，它们阐述的不是同一个问题。P1讨论的是如何选择proposal的问题，而P2则直接跳到了选出来后的问题：一旦value被选定了，后面的被选出来的value应该保持不变。从论文中后面的推断不断增强可以分析出，P2其实包含了P1，两个条件并不是相互独立的，因为P2其实也是一个如何选的过程，只不过它表示了一般情况下应该如何选的问题，而P1是针对第一个proposal应该如何选的问题。换言之，P1是任何后续的推论都需要保证的，后续作出的任何推断都不能与P1矛盾。 注意到，后续若有其它的proposal被选定，前提肯定是有acceptor接受了这个proposal。自然而然，可以转换P2的论述方式，于是就有了P2a： P2a . If a proposal with value v is chosen, then every higher-numbered proposal accepted by any acceptor has value v. P2a其实是在对acceptor做限制。事实上，P2与P2a是一致的，只要满足了P2a就能满足P2。但前面提到过P1是后续推断所必须满足的，而仔细考量P2a，它似乎违反了P1这个约束底线。可以考虑这样一个场景：若有 2 个proposer和 5 个acceptor。首先由proposer-1提出了[id1, v1]的提案，恰好acceptor1~3都顺利接受了此提案，即quorum个节点选定了该值v1，于是对于proposer-1及acceptor1~3而言，它们都选定了v1。而acceptor4在proposer-1提出提案的时候，刚好宕机了（事实上，只要其先接受proposer-2的提案即可，且proposer-2的编号大于proposer-1的编号）而后有proposer-2提出了提案[id2, v2]且id2&gt;id1 &amp; v1!=v2。那么由P1知，acceptor-4在宕机恢复后，必须接受提案[id2, v2]，即选定v2。很明显这不符合P2a的条件。因此，我们只有对P2a进行加强，才能让它继续满足P1所设定的底线。 我们自己可以先直观思考，为了保证acceptor后续通过的proposal的值与之前已经认定的值是相同的。如果直接依据之前的简单流程：proposer直接将其提案发送给acceptor，这可能会产生冲突。所以，我们可以尝试限制后续的proposer发送的提案的value，以保证proposer发送的提案的`value与之前已经通过的提案的value相同，于是引出了P2b`： P2b. If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v. P2b的叙述同P2a类似，但它强调（约束）的是proposer的issue提案的过程。因为，issue是发生在accept之前，那么accept的proposal一定已经被issue过的。因此，P2a可以由P2b来保证，而且，P2b的限制似乎更强。另外，P1也同时得到满足。 对于P2b这个条件，其实是难以实现。因为直观上，你不能限定各个proposer该issue什么样的proposal，不能issue什么样的proposal。那么又该如何保证P2b呢？我们同样可以先自己主观思考，为了让proposer之后issue的proposal的value与之前已经被通过的proposal的value的值保持一致，我们是不是可以尝试让proposer提前与acceptor进行沟通，以获取之前已经通过的proposal的value呢？具体如何沟通，无非是相互通信，接收消息或者主动询问，接收消息未免显得过于消极，而主动询问显然是更好的策略。如果的确存在这样的value，那为了保证一致，我就不再指定新的value了，与先前的value保持一致即可。而原论文给出了P2c: P2c. For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that either (a) no acceptor in S has accepted any proposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S. 作者认为，P2c里面包含了P2b。P2c中的(a)容易理解，因为如果从来没有accept过编号小于n的提案，那由P1自然而然就可以接受。而对于(b)可以用法定集合的性质简单证明，即两个法定集合(quorum)必定存在一个公共元素。我们可以采用反证法结合归纳法来简单证明。假定编号为m且值为v的提案已经被选定，那么，存在一个法定集合C，C中每一个acceptor都选定了v。然后有编号为n的proposal被提出 ：那么， ① 当n=m+1 时，假设编号为n的提案的value不为v而为w。则根据P2c，存在一个法定集合S，要么S中的acceptor从来没有批准过小于n的提案；要么在批准的所有编号小于n的提案中，编号最大的提案的值为w。但因为S和C至少存在一个公共acceptor，明显两个条件都不满足。所以假设不成立。因此n的值为v。② 当编号m属于m ... (n-1)，同样假设编号为n的提案的value不为v，而为w’ 。则存在一个法定集合S’，要么在S’中没有一个acceptor批准过编号小于n的提案；要么在S’中批准过的所有的编号小于n的提案中，编号最大的提案的值为w’。根据假设条件，编号属于m...(n-1)的提案的值都为v，并且S’和C至少有一个公共acceptor，所以由S’中的acceptor批准的小于n的提案中编号最大的那个提案也属于m...(n-1)。从而必然有w’=v。 若要满足P2c，其实也从侧面反映出若要使得proposer提交一个正确的value，必须同时对proposer和acceptor作出限制。我们现在回顾一下先前的推断的递推关系：P2c=&gt;P2b=&gt;P2a=&gt;P2。因此，P2c最终确保了P2，即当一个value被选定之后，后续的编号更大的被选定的proposal都具有先前已经被选定的value。整个过程，先是对整个结果提出要求形成P2，然后转为对acceptor提出要求P2a，进行转为对proposer提出要求P2b，最后，同时对acceptor及proposer作出要求P2c。 Paxos 算法步骤最后，我们简单阐述一下Paxos算法的步骤。其大致可以分为两个阶段。 阶段一，prepare阶段。 proposer选择一个新的编号n发送给quorum个acceptor，并等待回应。 如果acceptor收到一个针对编号为n的prepare请求，则若此prepare请求的编号n大于它之前已经回复过的proposal的所有编号的值，那么它会 (1) 承诺不再接受编号小于n的proposal。(b) 向proposer回复之前已经接受过的proposal中编号最大的proposal（如果有的话）。否则，不予回应。或者，回复一个error给proposer以让proposer终止此轮决议，并重新生成编号。 阶段二，accept阶段。 如果proposer收到了quorum个acceptor对其编号为n的prepare请求的回复，那么它就发送一个针对[n, v]的proposal给quorum个acceptor（此quorum与prepare阶段的quorum不必相同）。其中，v是收到的prepare请求的响应的proposal集合中具有最大编号的proposal的value。如果收到的响应集合中不包含任何proposal，则由此proposer自己决定v的值。 如果acceptor收到一个针对编号为n的accept请求，则若其没有对编号大于n的prepare请求做出过响应，就接受该proposal。 Paxos 算法活性前面提到，理论上Paxos可能永远不会终止（即永远无法达成一致），即使是在没有故障发生的情况。考虑这样一个场景，proposer-1发起了prepare阶段并获得了大多数acceptor的支持，然后proposer-2立刻带着更高的编号来了，发起了prepare阶段，同样获得了大多数的acceptor的支持（因为proposer-2的编号更高，acceptor只能对prepare请求回复成功）。紧接着proposer-a进入了accept阶段，从acceptor的回复中得知大家又都接受了一个更高的编程，因此不得不选择更大的编号并重新发起一轮prepare阶段。同样，proposer-2也会面临proposer-1同样的问题。于是，它们轮流更新编号，始终无法通过。这也就是所谓的活锁问题。FLP定理早就证明过即使允许一个进程失败，在异步环境下任何一致性算法都存在永不终止的可能性。论文后面提出为了避免活锁的问题，可以引入了一个proposer leader，由此leader来提出proposal。但事实上，leader的选举本身也是一个共识问题。而在工程实现上，存在一些手段可以用来减少两个提案冲突的概率（在raft中采用了随机定时器超时的方式来减小选票瓜分的可能性）。 最后，为了更好地理解Paxos算法时，补充（明确）以下几点。 Paxos算法的目的是确定一个值，一轮完整的paxos交互过程值用于确定一个值。且为了确定一个值，各节点需要协同互助，不能”各自为政”。且一旦接受提案，提案的value就被选定。 Paxos算法的强调的是值value，而不是提案proposal，更加不是编号。提案和编号都是为了确定一个值所采用的辅助手段。显然，当一个值被确定时，acceptor接受的提案可能是多个，编号当然也就不同，但是这些提案所对应的值一定是一样的。 Paxos流程保证最终对选定的值达到一致，这需要一个投票决议过程，需要一定时间。 上面描述的大多流程都是正常情况，但毫无疑问，acceptor收到的消息有可能错位，比如 (1) acceptor还没收到prepare请求就直接收到了accept请求，此时要直接写入日志。(2) acceptor还未返回对prepare请求的确认，就收到了accept请求，此时直接写入日志，并拒绝后续的prepare请求。 因为节点任何时候都可能宕机，因此必须保证节点具备可靠的存储。具体而言，(1) 对于proposer需要持久化已提交的最大proposal编号、决议编号(instance id)（表示一轮Paxos的选举过程）。(2) 对于acceptor需要持久化已经promise的最大编号、已accept的最大编号和value以及决议编号。 参考资料 [1]. Lamport L. Paxos made simple[J]. ACM Sigact News, 2001, 32(4): 18-25.[2]. https://blog.csdn.net/chen77716/article/details/6166675[3]. 如何浅显易懂地解说 Paxos 的算法]]></content>
      <categories>
        <category>分布式系统</category>
        <category>一致性算法</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>一致性算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析分布式事务]]></title>
    <url>%2F2018%2F12%2F11%2F%E6%B5%85%E6%9E%90%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[分布式系统中，将数据块冗余到不同节点使得系统具备容错能力，但其代价是必须要保证各数据副本间的一致性。同样，我们可以将计算（执行）分发到不同节点，以更有效地利用节点并行处理能力，但其代价是必须要对各节点的执行进行协调，以产生应用程序期望的结果。换言之，我们需要推断出节点内部执行的正确性，以保证应用程序可见的语义。而数据库通常能提供涉及事务(transaction)和可序列化(serializability)的相对较强的语义。因此，对于分布式系统而言，此种语义的正确性可以通过分布式事务(distributed transaction)来保证，其通常涉及两个或多个在物理上分离且通过网络连接的主机的数据库事务。 正式而言，分布式事务包含两个方面：并发控制(concurrency control)及原子提交(atomic commit)。并发控制描述事务并发执行的正确性，而原子提交表示事务包含的一组操作要么全部执行，要么全部不执行，这通常与失败(failure)相关。本文会依次阐述分布式事务的并发控制、原子提交相关内容。 并发控制我们以一个例子来展开对并发控制的讨论。考虑一个银行转账的场景：有两个银行账户x和y，且x与y仅次于不同的服务器上，x与y账户初始数目都是 10 。客户端c1将从x账户转账1到y账户，同时，c2是一个审计者以检查银行各账户的钱是否有丢失。因此，抽象化c1及c2的操作为： 1234c1: c2:add(x, 1) tmp1 = get(x)add(y, -1) tmp2 = get(y) print tmp1, tmp2 我们（应用程序）期待最终的结果为：x=11, y=9，同时c2打印 10, 10或者11, 9。但并发执行的操作可能并不会按照应用期待的结果输出。比如，若c2的两个操作完全运行在c1的两个操作之间，导致最终的结果为：x=11, y=9，同时打印11, 10。显然，对于此应用场景而言，我们并不希望出现后者。因此我们需要对并发执行的操作进行协调，以保证其操作结果的语义能够符合应用程序。 先引出一个概念before-or-after atomicity，其定义如下。 Concurrent actions have the before-or-after property if their effect from the point of view of their invokers is the same as if the actions occurred either completely before or completely after one another. 显然，若并发操作能before-or-after atomicity属性，则此转账应用产生的结果是正确的。基于此，我们尝试给出一个能够保证应用程序的正确性的论断。 Coordination among concurrent actions can be considered to be correct if every result is guaranteed to be one that could have been obtained by some purely serial application of those same actions. 可以通过如下几个步骤来认证此观点的正确性：考虑一个系统被应用（可能是并发执行的）操作之后从一个状态转换到另一个状态，如果系统的初始状态是正确的（由具体应用程序确定），并且操作正确地被执行应用到系统，则系统新的状态也是正确的。并且此论述独立于应用程序。同样，如果如果是多个操作并发执行，则上述的论断变更为，若系统最终所处的状态是应用到系统的并发操作集的某个顺序执行后系统的状态，那么此时系统的新的状态也是正确的。换言之，结合before-or-after atomicity属性，可以得出这样的结论：若协调并发操作的规则遵循before-or-after atomicity，则这些并发操作是可序列化的，即存在某些并发事务构成的串行执行顺序，若遵循这些顺序，将导致系统处于相同的终止状态，此时并发操作的结果是正确的。这同样是传统数据事务正确性定义——serializability所要求的。 理论上而言，并发操作的中间过程是不重要的，因为只要保证并发操作所产生的系统新的状态与按照某一个顺序顺序执行所有的“单个”的原子操作所产生的系统新的状态相同，我们并不关心具体与哪一个操作顺序相同，甚至，我们都不要求所谓的顺序操作的中间状态是否真实存在（如图所示，即若操作执行的中间状态的路径是按照虚线进行的），只要此中间状态不会被外部应用程序所观察到，那么我们同样认为这样的操作具备before-or-after atomicity属性，即符合serializability的要求。值得注意的是，对系统应用并发操作的目的是提高性能，但具备before-or-after atomicity属性，或满足serializability的并发操作并不保证系统具备最佳的执行性能。另外，满足serializability特性的并发操作，对编程人员是友好的，因为，我们不必关心并发操作细节。 基于锁的 before-or-after atomicity 属性的实现基于锁来实现事务的并发控制可以分为两个类别：悲观锁(pessimistic)及乐观锁(optimistic)。前者会在操作共享对象之前获取锁，如果在获取锁时，锁已经被其它事务占用了，则必须等待。而后者并不要求在操作共享对象之前获取锁，它会先将对象进行拷贝，然后操作对象，在提交事务的时候检查原始对象是否有被更改过，若没有，则提交事务，否则中止事务，换言之，在获取锁失败时，乐观并发控制(optimistic cocurrency controll)采用的是abort+retry的模式来操作共享对象，因为它没有直接给对象加锁，因此若对象访问没有冲突时，它比悲观锁要快，反之，若在一个充满锁竞争的事务环境下，乐观锁的效果一种会比悲观锁要差。而本节下面提到的基于锁的before-or-after atomicity属性（或serializability）的实现都属于悲观锁的实现。 system-wide lock，即系统级锁。这是基于锁实现的before-or-after atomicity属性最简单的版本。顾名思义，它在系统开始运行时便在内存中创建一个（唯一一个）锁对象，并且必须在事务执行的开始与结束位置插入获取锁与释放锁的代码。显然，system-wide lock一次只允许运行单个事务，它会将所有的事务按照其获取锁的顺序依次执行，不支持事务的并发执行。因为system-wide lock锁住孙事务涉及的所有对象，因此在某些场合其是不必根据，换言之，其锁的粒度（范围）过大。 simple locking，即简单锁。它满足两个规则：其一，每个事务在对某一对象执行实际的读写操作时，必须提前获取此对象的锁。其二，当事务所有操作完成后被提交或者事务被中断时才释放锁。其中，lock point被定义为事务获取其范围内操作所有对象的锁的时刻。而lock set被定义为截止lock point时间点，其所获取的锁的集合。因此，为了保证能正确地协调事务的并发执行，应用程序在执行其每个事务前必须获取事务所对应的lock set，同样，在事务执行完成时释放lock set中的锁。下面简单证明simple locking的策略能够保证before-or-after atomicity。 假定有一个外部观察者维护一个事务标识符的列表，并且一旦某个事务到达其lock point，其标识符就会被添加到此列表，并在事务执行完毕即将释放锁时将其从列表中移除。simple locking能够保证：每个事务都不会在其被添加到列表之前读或写任何对象，并且列表中此事务前面的所有事务都已经通过其对应的lock point。由于任意两个事务lock set不会出现相同的数据对象，因此任何事务的lock set中的数据对象都不会出现在列表中它前面的事务的lock set中，所以也不会出现在列表中更早的事务的lock set中。因此，此事务的输入所涉及的对象内容与列表中的其前一个事务commit（事务顺利完成）或abort（事务中止）后的输出的对象的内容相同。因此，simple locking规则保证此事务before-or-after atomicity属性。 显然，simple locking 所提供的并发粒度也过大，因为，它必须对事务可能涉及到的每一个共享对象加锁，因此它有可能锁住那些原本并不需要的对象。 two-phase locking(2PL)，即两阶段锁。相比于simple locking，它并不要求事务在操作共享对象之前获取其所涉及到的所有对象的锁（准确而言，对于simple locking，一旦事务操作任一共享对象，都需要获取所有对象的锁，而two-phase locking只有等到事务真正操作某一对象时，才去尝试获取此对象对应的锁，因此其锁的粒度要比simple locking要小）。典型地，2PL包括两个过程：1. 扩展锁阶段(expanding phase)，根据操作共享对象的顺序依次获取锁，在此过程中不会有锁被释放。2. 收缩锁阶段(shrinking phase)，锁逐渐被释放，并且在此过程不会尝试获取锁（如果阶段一没有明确的完成标志，那么为了保证事务安全，会等到事务提交或者事务中止时，才会一次性释放所有锁）。但同simple locking类似的是，2PL也允许应用程序并发执行事务，其也会保证所有事务的执行所产生的结果同它们以某一个顺序（到达lock point的顺序）执行所产生的结果相同（因此，2PL有可能导致死锁）。虽然，同simple locking相比，2PL提供更强的事务并发执行能力，但其同样会导致原本允许并发执行的事务的串行顺序执行。参考文献[1]还讨论了当事务执行失败时，锁与日志的交互如何保证事务的顺序执行。 原子提交若构成事务的操作分布在不同机器上，为了确保事务被正确执行，则必须保证事务原子性提交，即分布在不同机器上的事务要么全部执行，要么都不执行。 Two-phase commit(2PC)，两阶段提交。它被用于解决分布式事务原子提交问题（但并没有完全解决）。先简要阐述经典的2PC协议，整个事务由一个事务协调者(transaction coordinator及若干事务参与者(participant)构成，协议的执行大致可以分为如下两个阶段： prepare阶段：客户端向TC发送事务提交请求，TC开始执行两阶段提交。它首先通过RPC向所有的participant发送prepare消息，若participant当前能够执行事务，则向TC回复prepare成功(YES)，并且锁定事务执行所需要的锁与资源，否则回复NO。 commit阶段：若TC收到所有participant回复的YES消息，则开始正式commit事务。它会给所有的participant发送commit消息。participant收到commit消息后，释放事务过程中持有的锁和其他资源，并将事务在本地提交，然后向TC回复commit成功，即YES，否则回复NO。TC收到所有participant回复的commit成功的消息后，向客户端返回成功。反之，一旦TC收到某个participant对preapre消息回复了NO消息，则向所有的participant回复abort消息。 显然，若整个过程无任何故障发生，2PC能够保证分布式事务提交的原子性，因为所有事务参与者对事务的提交都是经由事务协调者来协调决定，因此它们要么全部提交事务要么都不会提交事务。 上述为正常条件下协议执行流程，即没有节点宕机，也没有网络故障。下面讨论若发生失败，会有怎样的情况： 事务参与者宕机，然后重启。若此participant在宕机前对TC的prepare消息回复了YES，那么它必须在宕机前对日志记录。因为其它的participant也有可能同意了prepare消息。具体而言，如果participant重启后，其日志文件记录了prepare的YES消息，但其并没有commit事务，此时它必须主动发消息给TC，或者等待TC重新向它发送commit消息。且在整个过程中，participant必须一直保持对资源及锁的占用。 事务协调者宕机，然后重启。因为TC可能在宕机前对所有协调者发送了commit消息，因此它也必须对此作日志记录。因为或许某个participant已经执行了事务的commit。如果其在重启后，收到了participant的询问消息，必须重新发送commit消息（或者等待一段超时时间后，重新发送commit消息）。 事务协调者一直未收到事务参与者的对prepare消息的回复。可能此时participant已经宕机并且没有重启，也有可能网络发生了故障。因此TC必须设置超时机制，一旦超时未收到回复，则中止事务的提交（因为此时并没有发送commit消息，所有participant都不会提交，保证了事务提交的原子性）。 事务参与者在收到prepare消息前宕机，或超时（一直未收到）。此时，因为participant并没有回复prepare消息（即未对TC作出事务执行的任何承诺），因此其允许单方面中止事务，释放锁及其它资源，此时可能是协议还未开始执行（自然而然，participant的宕机对协议是没有任何影响，直到下一次协议开始执行了，若此participant仍旧处于宕机状态，则将导致abort事务）。 事务协调者在未发送prepare消息前宕机。此时，同上一种情况类似，协议很可能还未开始执行，因此TC的宕机并不影响事务正确性。 事务参与者对prepare消息回复了YES，但是一直未收到commit/abort消息。此时，participant不能单方面中止事务，因为其已经向TC的prepare消息回复了YES，且其它participant也有可能向TC回复了YES，因此TC可能已经向除此participant之外的所有participant发送了commit消息，然后TC发生了宕机。但收到commit消息的participant可能已经commit本地事务。因此，此participant不能单方面abort事务（否则造成事务不一致）。同时，此participant也不能单方面的commit本地事务，因为同样，其它的participant也有可能对TC的prepare消息回复了NO，因此TC在收到所有的prepare消息的回复后，中止了事务的提交。总而言之，若participant对TC的prepare消息回复了YES，则它不能单方面作出任何决定，只能一直阻塞等待TC对事务的决定。 因此，通过上述分述，经典的2PC协议存在明显的局限性： 事务协调者宕机：2PC为一个阻塞式协议，一旦事务协调者宕机，则若有参与者处于执行commit/abort之前的任何阶段，事务进程都将会被阻塞，必须等待事务协调者重启后，事务才能继续执行。 交互延迟：事务协调者必须将事务的commit/abort写日志后才能发送commit/abort‘消息。因此整个过程至少包含2次RPC(prepare+commit)，以及3次日志记录的延迟（prepare写日志+事务协调者状态持久化+commit写日志）。 参考文献 [1] https://en.wikipedia.org/wiki/Two-phase_locking[2] Saltzer J H, Kaashoek M F. Principles of computer system design: an introduction[M]. Morgan Kaufmann, 2009.[3]. 两阶段提交的工程实践]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式事务</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
        <tag>并发控制</tag>
        <tag>原子提交</tag>
        <tag>二阶段提交</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解分布式协调服务 zookeeper]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%90%86%E8%A7%A3%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E6%9C%8D%E5%8A%A1-ZooKeeper%2F</url>
    <content type="text"><![CDATA[ZooKeeper是 Yahoo! 于 2010 年在 USENIX 会议上发表的一篇论文中提出的，被用作分布式应用程序的协调服务(coordination service)。虽然ZooKeeper被认为是 Google Chubby的开源实现，但其设计理念却存在较大差异：ZooKeeper致力于提供一个简单且高性能(high performance)的内核(kernel)以为客户端（应用程序）构建更复杂、更高层(high level)的协调原语(coordination primitives)。换言之，ZooKeeper并不针对特定应用或者具体某一协调服务而设计实现，它只提供构建应用协调原语的内核，而将具体协调原语的构建逻辑放权给客户端，并且，它确保了客户端在不需要更改内核服务的前提下，能够灵活构建出新的、更高级的且更强大的协调原语，比如分布式互斥锁、分布式队列等。ZooKeeper为每个客户端操作提供FIFO顺序保证，并且为所有写操作提供linearlizablity保证。ZooKeeper的实现原理为构建在其之上的服务提供高性能保证。 Zookeeper 为分布式应用提供诸如配置管理(configuration managation)、leader 选举等协调服务，这通过为应用程序提供构建协调原语的 API来实现。并且，与那些提供阻塞原语的服务不同，ZooKeeper实现的 wait-free 数据对象确保其容错和高性能特性，因为若利用阻塞原语来构建协调服务，可能会导致那些慢的(slow)或者有错误的(faulty)的客户端影响正常的客户端的服务性能。此博客阐述个人对ZooKeeper的理解，并从一个ZooKeeper的应用实例开始讨论，分别阐述ZooKeeper两个ordering guarantees、。因为本文并非对原论文的完整翻译，因此你需要提前阅读原论文，确保熟知ZooKeeper数据模型以及客户端API等内容，而且，博客也会省略论文所阐述的利用ZooKeeper来实现部分协调服务部分，具体内容可以参考原论文。 一个应用实例阐述我们知道MapReduce需要知道集群master的ip:port以使得其它节点能够与master建立连接通信，为此，MapReduce可以利用ZooKeeper作为动态配置服务，让master candidate在ZooKeeper上并发注册（创建）各自ephemeral类型的ip:port节点，并让slave监听对应节点的watch event，因此一旦有master candidate注册成功（且只能有一个创建成功），则其它节点将能获取到master的ip:port。 若使用基于raft构建的复制状态机实现，比如在raft集群上构建一个key/value存储系统来存放GFS master的元信息。则整个过程大致如下：首先，master candidate向raft发送Put(&quot;gfs-master&quot;, &quot;ip:port&quot;)命令日志，当raft集群apply此命令日志后，其它节点可通过向raft发送Get(&quot;gfs-master&quot;)命令来获取master的ip:port。但此过程存在几个问题：其一，若多个master candidate同时向raft发送节点地址的注册命令日志，此时将产生race condition，其会导致后发送的命令被应用到状态机，因此master candidate需要进一步判断自己是否成为真正的master（不能仅通过发送了节点地址命令日志来确定）；其二，若master失效，其地址项日志必须要从存储中移除，那么谁来执行此操作？因此，必须对master的元数据信息设置timeout timestamp，并且让master通过定期向raft发送Put(ip:port, timestamp)日志来更新timeout的timestamp，而集群其它节点通过向raft轮询(poll)此timestamp来确保master正常工作，毫无疑问，这将产生大量不必要的poll cost。对比使用ZooKeeper来提供此协调服务（上一段），问题是如何被ZooKeeper高效便捷地解决呢？首先它会确保在多个master candidate同时注册地址信息时，只会有一个操作成功；其次，ZooKeeper的session机制简化了timestamp timeout设置，一旦master宕机，其在ZooKeeper上注册的元信息节点将会自动清除。而且，对应的节点移除消息也会通知到其它节点，避免了slave的大量的轮询消耗。由此可见，使用ZooKeeper来进行集群配置信息的管理，有利于简化服务实现的逻辑。 ZooKeeper 两个 ordering guarantees在讨论ZooKeeper两个基本的ordering guarantees之前，先了解什么是 wait-free，你可以从维基或者 Herlihy的论文 上找到其明确定义： A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. Wait-freedom is the strongest non-blocking guarantee of progress, combining guaranteed system-wide throughput with starvation-freedom. An algorithm is wait-free if every operation has a bound on the number of steps the algorithm will take before the operation completes 而对于ZooKeeper而言，其提供的API被称为是wait-free的，因为ZooKeeper直接响应客户端请求，即此请求的返回并不会受到其它客户端操作的影响（通常是slow或者faulty）。换言之，若此客户端请求为写节点数据操作，只要ZooKeeper收到状态变更，则会立即响应此客户端。如果在这之前某一客户端监听了此节点的数据变更事件，则一旦此节点的数据发生变化，则ZooKeeper会推送变更事件给监听的客户端，然后立即返回给写数据的客户端，并不会等待此监听客户端确认此事件。相比于同步阻塞的调用，wait-free明显提供更好的性能，因为客户端不用同步等待每次调用的返回，且其可以进行异步的批量调用batch call操作，以均摊(amortize)网络传输和IO开销。wait-free的API是ZooKeeper具备高性能的基础，因此也是ZooKeeper的设计核心。 ZooKeeper提供了两个基本的ordering guarantees：Linearizable writes及FIFO client order。Linearizable write表示对ZooKeeper的节点状态更新的请求都是线性化的(serializable)，而FIFO client order则表示对于同一个客户端而言，ZooKeeper会保证其操作的执行顺序与客户端发送此操作的顺序一致。毫无疑问，这是两个很强的保证。 ZooKeeper提供了Linearizable write，那什么是Linearizablility？Herlihy的论文同样给出了其定义，为了方便，你也可以参考这里或者这里。 Linearizability is a correctness condition for concurrent objects that provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object’s operations can be given by pre- and post-conditions. 简单而言，Linearizability是分布式系统领域的概念（区别于数据库领域与事务相关的概念Serializability），一个分布式系统若实现了linearizability，它必须能够保证系统中存在一个时间点，在此时间点之后，整个系统会提交到新的状态，且绝不会返回到旧的状态，此过程是即时的(instantaneous)，一旦这个值被提交，其它所有的进程都会看到，系统的写操作会保证是全局有序(totally ordered)。 而ZooKeeper论文提到其write具备Linearizability，确切而言是A-linearizability(asynchronous linearizability)。简而言之，Linearizability原本（原论文）是针对单个对象，单个操作(single object, single operation)而言的，但ZooKeeper扩大其应用范围，它允许客户端同时执行多个操作（读写），并且保证每个操作同样会遵循Linearizability。 值得注意的是，ZooKeeper对其操作（create,delete等）提供pipelining特性，即ZooKeeper允许客户端批量地执行异步操作（比如发送了setData操作后可以立即调用geData），而不需要等到上一个操作的结果返回。毫无疑问，这降低了操作的延迟(lantency)，增加了客户端服务的吞吐量(throughtout)，也是ZooKeeper高性能的保证。但通常情况下，这会带来一个问题，因为所有操作都是异步的，因此这些操作可能会被重排序(re-order)，这肯定不是客户端希望发生的（比如对于两个写操作而言，re-order后会产生奇怪的行为）。因此，对于特定客户端，ZooKeeper还提供client FIFO order的保证。 ZooKeeper 实现原理同分布式存储系统类似，ZooKeeper也会对数据进行冗余备份。在客户端发送请求之前，它会连接到一个ZooKeeper server，并将后续的请求提交给对应的server，当server收到请求后，有做如下三个保证：其一，若请求所操作的节点被某些客户端注册了监听事件，它会向对应的客户端推送事件通知。其二，若此请求为写操作，则server一次性只会对一个请求做处理（不会同时处理其它的读或者写请求）。其三，写操作最终是交由leader来处理（若接收请求的server并非leader，其主动会对请求进行转发），leader会利用Zab（原子广播协议，ZooKeper atomic broadcast）对此请求进行协调，最终各节点会对请求的执行结果达成一致，并将结果 replica到ensemble servers。ZooKeeper将数据存储到内存中（更快），但为了保证数据存储的可靠性，在将数据写到内存数据库前，也会将数据写到磁盘等外部存储。同时，对操作做好相应的replay log，并且其定期会对数据库进行snapshot。 若请求为读操作，则接收请求的server直接在本地对请求进行处理（因此读操作仅仅是在server的本地内存数据库进行检索处理，这也是ZooKeeper高性能的保证）。正因为如此，同GFS可能向客户端返回过期数据的特点类似，ZooKeeper也有此问题。如果应用程序不希望得到过期数据（即只允许得到最近一次写入的数据），则可以采用sync操作进行读操作前的写操作同步，即如果在读操作之前集群还有pending的写操作，会阻塞直至写操作完成。值得注意的是，每一次的读操作都会携带一个zxid，它表示ZooKeeper最近一次执行事务的编号（关于事务，后面会介绍），因此zxid定义了读操作与写操作之间的偏序关系。同时，当客户端连接到server时，如果此server发现其本地存储的当前zxid小于客户端提供的zxid的大小，其会拒绝客户端的连接请求，直至其将本地数据库同步至全局最新的状态。 在ZooKeeper内部，它会将接收到的写操作转换为事务(transaction)操作。因为ZooKeeper可能需要同时处理若干个操作，因此其会提前计算好操作被提交后数据库所处的状态。这里给出论文中提到的一个事务转换的示例：如果客户端发送一个条件更新的命令setData并附带上目标节点的version number及数据内容，当ZooKeeper server收到请求后，会根据更新后的数据，版本号以及更新的时间戳，为此请求生成一个setDataTXN事务。当事务执行出错时（比如版本号不对应），则会产生一个errorTXN的事务。 值得注意的是，ZooKeeper内部所构建的事务操作是幂等的(idempotent)。这有利于ZooKeeper执行失效恢复过程。具体而言，为了应对节点宕机等故障，ZooKeeper会定期进行snapshot操作，ZooKeeper称其为fuzzy snapshot。但与普通的分布式系统不同的是，它在进行快照时，并不会锁定当前ZooKeeper集群（一旦锁定，便不能处理客户端的写操作，且快照的时间一般也相对较长，因此会降低客户端的服务性能），它会对其树形存储进行深度优先搜索，并将搜索过程中所遍历的每一个节点的元信息及数据写到磁盘。因为ZooKeeper快照期间并没有锁定ZooKeeper的状态，因此在此过程中，若有server在同步写操作，则写操作可能只被replica到部分节点，最终使得snapshot的结果处于不一致的状态。但正是由于ZooKeeper的事务操作是idempontent，因此，在recover过程应用snapshot时，还会重新按顺序提交从快照启动开始到结束所涉及到的事务操作。原论文给出了一个快照恢复过程示例。因此我们会发现，fuzzy snapshot同样是ZooKeeper 高性能的体现。另外，事务幂等的特性也使得ZooKeeper不需要保存请求消息的ID（保存的目的是为了防止对重复执行同一请求消息），因为事务的重复执行并不会导致节点数据的不一致性。由此可见，事务幂等性的大大设计简化了ZooKeeper的请求处理过程及日志恢复的过程。 最后，关于原论文所阐述的基于ZooKeeper内核来构建协调服务的相关实例部分，参考实现代码在这里。 参考文献 [1] Hunt P, Konar M, Junqueira F P, et al. ZooKeeper: Wait-free Coordination for Internet-scale Systems[C]//USENIX annual technical conference. 2010, 8(9).[2] Herlihy M P, Wing J M. Linearizability: A correctness condition for concurrent objects[J]. ACM Transactions on Programming Languages and Systems (TOPLAS), 1990, 12(3): 463-492.[3] https://medium.com/databasss/on-ways-to-agree-part-2-path-to-atomic-broadcast-662cc86a4e5f[4] https://en.wikipedia.org/wiki/Non-blocking_algorithm[5] http://www.bailis.org/blog/linearizability-versus-serializability/]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式协调服务</tag>
        <tag>原子广播协议</tag>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 原型实现]]></title>
    <url>%2F2018%2F11%2F16%2FMapReduce-%E5%8E%9F%E5%9E%8B%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[MapReduce 最早是由谷歌于 2004 年在操作系统顶会 OSDI 上发表的一篇面向大规模数据处理的分布式计算框架（并行计算模型）论文中提出。MapReduce使用 Google File System 作为数据存储，支撑起了谷歌全网搜索等大规模数据存储与处理业务。MapReduce 对于大规模数据的高效处理体现在三个方面：其一，大规模数据并行处理，分而治之；其二，MapReduce编程模型；最后，MapReduce运行时环境（失败恢复、任务调度以及负载均衡等）。它简化了并行编程，使得开发人员很容易编写出高效且具备容错能力的并行化程序。 博客基于 MIT 6.824 (2018) 的课程 Lab1。整个实验实现了MapReduce原型，并且对其关键特性进行测试，主要包括MapReduce编程模型，集中在 Map与Reduce两个阶段，以及任务失败处理。在阅读原论文 MapReduce 的基础，Lab1 能够让我们对 MapReduce原理有更为深刻的理解，也能够提高我们实现分布式系统的实践能力，这包括节点通信模型、系统构建框架以及诸如失败恢复机制等。而且，仔细阅读整个 Lab 的代码可以学习到很多原理及设计知识，而不仅仅是完成其 Lab 任务。下文会简单介绍整个 Lab1 框架，然后阐述几个关键点（模块）。 Sequential 及 Distributed 运行模式Lab1 实现了两种不同运行模式的MapReduce原型框架：一种是Sequential运行模式，它顺序编程实现MapReduce过程，也不具备容错功能，因此并非真正意义上的实现。具体地，基于此种运行模式，所有task串行执行且Map与Reduce两个阶段也是串行执行，且未提供任务执行失败的恢复机制。大概地，它首先创建输入文件并读取Map输入，同时创建对应数量的Map task（即循环调用Map函数来处理输入文件），并顺序调度执行，将中间结果写到磁盘上，当所有Map task执行完成后，启动一定数量的Reduce task，并让Reduce task从本地磁盘相应位置读取Map task输出，同样被顺序调度执行，最后，将Reduce task输出写到本地磁盘，最终merge所有输出文件，以合并写到本地输出文件。 另一种是 Distributed运行模式，它更接近真实的MapReduce原型框架实现。客户端会依次启动一个master节点及多个slave节点(go 的goroutine)，并将输入文件信息传给master节点，此后客户端会阻塞等待master返回 。master启动后开始监听slave的连接(one client one goroutine），slave启动后会主动往master节点注册，并等待master分配任务。所有节点通过go rpc实现对等通信。一旦有slave/worker注册成功，master开始实施任务调度，通过rpc将任务信息（任务类型、任务输入文件位置等）发送给worker，而worker在注册成功后，就不断监听master的连接并调用worker的任务执行handler(doTask)， doTask会调用应用程序的Map或Reduce执行MapReduce任务，所有的worker在本节点执行任务的过程同Sequential运行模式下类似，只是各个worker并行执行，互不干扰。值得注意的是，在整个MapReduce Job调度执行过程中，worker允许动态加入，master一旦发现worker注册加入，若此时有未完成的任务等待调度，就会将此任务让新加入的worker调度执行。只有所有的Map task调度完成后，Reduce task才会被调度。当所有Reduce task执行完成后，同样会进行merge的过程，然后从MapReduce框架返回。 Map 及 Reduce 工作流程这里简要阐述 Map &amp; Reduce阶段执行流程。当worker执行map task时，包括以下几个步骤：首先从本地磁盘读取其负责处理的原始输入文件；然后，通过将文件名及文件内容作为参数传递给MapFun来执行用户自定义逻辑；最后，对于每一个Reduce task，通过迭代MapFunc返回的执行结果，并按记录(record)的key进行partition以将分配给对应的Reducer的中间输出结果写到本地磁盘对应文件。 Reduce task的执行过程大致如下：首先读取本Reduce task负责的输入文件，并使用JSON来decode文件内容，并将decode后的kev/value存储到map中，同一个key对应一个value list，然后将整个map的key进行排序，并对每一个key/value list通过调用ReduceFunc来执行用户名自定义逻辑，同时，将其返回的结果，经JSON encode后写入输出文件。这些由Reduce task输出的文件内容，会被merge到最终的输出文件。 再谈失败恢复容错（失败恢复）是MapReduce运行时的一个关键特性。且 Lab1 也模拟实现了任务执行失败后所采取的措施。任务执行失败，典型的包括两种情况：网络分区（网络故障）及节点宕机，且事实上无法很好地区分这两种情形（在两种情形下，master都会发现不能成功ping通 worker）。而实验则是采用阻止worker与master的rpc连接来模拟实现。具体地，所有worker在执行若干个rpc连接请求后（一个rpc连接请相当于一次任务分配），关闭其rpc连接，如此master不能连接worker而导致任务分配执行失败。个人认为，一般情况下会让 master缓存worker的连接handler，并不会在每次发送rpc请求时，都需要执行Dial/DialHttp，若是如此，便不能以原实验的方式来模拟任务执行失败（虽然这可能并不影响）。另外 Lab1 显式禁止了worker同时被分配两个任务的情况，这是显而易见的。 关于失败恢复（节点容错），下面讨论更多细节。容错是MapReduce的一个重要特性，因为节点失效在大数据处理工作中过于频繁，而且当发生节点宕机或者网络不可达时，整个MapReduce job会执行失败，此时MapReduce并不是重启整个job，那样会导致重新提交执行一个庞大的job而耗时（资源）过多，因此它只会重启对应worker所负责执行的task。值得注意的是，正是因为worker并不维护task相关信息，它们只是从磁盘读取输入文件或者将输出写到磁盘，也不存在与其它worker进行通信协调，因此task的执行是幂等的，两次执行会产生相同的执行结果，这也可以说是MapReduce并行执行任务的约束条件之一，也是MapReduce同其它的并行执行框架的不同之处，但无论如何，这样设计使得MapReduce执行任务更为简单。因为Map task会为Reduce task产生输入文件，因此若Reduce task已经从Map task获得了其所需要的所有输入，此时Map的失败，并不会导致其被重新执行。另外关键的是，GFS的atomic rename机制确保即使Map/Reduce task在已经溢写了部分内容到磁盘后失败了，此时重新执行也是安全的，因为GFS会保证直到所有输出写磁盘完成，才使得其输出文件可见，这种情况也会发生在两个Reduce task执行同一个任务，GFS atomic rename机制同样会保证其安全性。那么，若两个Map执行同一个task结果会如何？这种情况发生在，master错误地认为Map task宕机（可能只是发生了网络拥塞或者磁IO过慢，事实上，MapReduce的stragger worker正描述的是磁盘IO过慢的情况），此时即便两个Map task都执行成功（它们不会输出到相同的中间文件，因此不会有写冲突），MapReduce运行时也保证只告诉Reduce task从其中之一获取其输入。最后，注意MapReduce的失败恢复机制所针对的错误是fail-stop故障类型，即要么正常运行，要么宕机，不会产生不正确的输出。 参考代码在这里。 参考文献 [1] Dean J, Ghemawat S. MapReduce: simplified data processing on large clusters[J]. Communications of the ACM, 2008, 51(1): 107-113.[2].MIT 6.824]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式计算框架</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式计算框架</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 The Google File System]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%90%86%E8%A7%A3-The-Google-File-System%2F</url>
    <content type="text"><![CDATA[分布式文件系统是构建整个分布式系统的基石，为分布式计算提供底层数据存储。谷歌早在 2013 年就发表了论文 The Google File System，它在谷歌内部是配合其分布式计算框架MapReduce使用，共同为谷歌搜索等业务提供技术栈支撑。虽然数据量激增以及技术革新使得GFS不断演进，但理解其最初的设计理念、运行原理以及关键实现技术同样让人受益匪浅，并指导着我们实际的学习和工程实践。这篇博文阐述个人对原论文的一些理解与心得，并不是对原论文的完整翻译，因此你需要提前阅读论文。 设计动机与目标设计一个通用的分布式文件系统是不现实的，它不仅在实现上异常困难（因为不得不考虑所有应用场景），而且实际使用也难以满足要求（往往存在显而易见的性能或容错瓶颈）。GFS 设计初衷是利用数以千计的廉价机器为MapReduce提供底层可靠且高性能的分布式数据存储，以应对海量离线数据存储与处理的应用场景，比如存储应用程序持续产生的日志流以提供离线日志分析。由此，其设计目标为容错可靠(fault tolerance)、高性能读写(high-performance read&amp;write)以及节约网络带宽(save bandwidth)。 一致性(consistency)是分布式系统不可回避的问题。对于分布式文件系统而言，为了提供容错，必须维持数据副本(replica），那如何保证各副本间一致显得至关重要，特别是在应用并发访问场合。一致性是个极其宽泛的术语，你可以实现数据的强一致性(strong consistency)以保证用户始终读到的是最新的数据，这对于用户（客户端）而言是个极佳选择，但它提高了系统的实现难度，因为你必须设计复杂的一致性协议（如Paxos或Raft）来实现强一致性，它也会损害系统性能，典型的它需要机器之间通信以对副本状态达成一致。而弱一致性(weak consistency)则几乎相反。因此，必须根据特定的应用场景，在保证系统逻辑正确的提前下，放宽一致性要求，设计具备良好性能且能提供足够的一致性(sufficient consistency)的系统。对于GFS而言，它针对MapReduce应用程序进行了特定优化，比如，对大文件高性能读取、允许出现文件空洞(hole)、数据记录重复(record duplicate)以及偶尔读取不一致(inconsistent reads)。具体在数据读写方面，其侧重于大规模一次性写入和追加写入，而并非覆盖写和随机写；读取同样倾向于顺序读取，并不关心随机读取。 Trade-off 理念哲学GFS在设计上存在大量的trade-off。正如前文所述，你不能企图设计出一个完美的系统，而只能针对具体应用场景作出各方面的权衡考量，以达到工程最佳实践目的。 chunk大小设计。GFS针对大文件存储（数百上千兆）设计，因此若存储大量小文件则不能体现其性能。其默认块大小为 64MB。选择大文件作为存储目标原因如下：首先它减少了client与master的交互次数（即使client并不需要整个数据块，但实际上往往存在“就近原则”）；另外，这也直接减少了网络带宽；最后，它减少了存储在master内存中的元数据(metadata)的大小。但凡事总利弊相随。较大的块大小设定使得小文件也不得不占用整个块，浪费空间。 集群元数据‘存储。在master的内存中存放着三种类型的元数据：文件和chunk的名称空间(namespace)、文件到chunk的映射信息以及chunk副本的位置信息。且前两种元数据会定期通过operation log持久化到磁盘以及副本冗余。为什么将这些元信息存储到内存？一方面，缓存在内存无疑会提高性能，另外它也不会造成内存吃紧，因为每个64MB 的chunk只会占用到 64B 的内存空间（粗略估算普通 2G 内存的机器可以容纳 2PB 数据），而且为机器增加内存的代价也很小。那为什么chunk位置信息没有持久化？首先master在启动的时候可以通过heartbeat从各chunk server获取。另一方面，chunk的位置信息有时会变动频繁，比如进行chunk garbage collection、chunk re-replication以及chunk migration，因此，若master也定期持久化chunk位置信息，则master可能会成为集群性能bottleneck。从另一个角度来看，chunck是由chunk server保存，而且随时可能发生disk failure而导致chunk暂时不可被访问，因此其位置信息也应该由chunk server负责提供。 chunk副本（默认3个）存放策略。chunk副本选择目标机器的原则包括两个方面：一是最大化数据可靠性(reliability)及可用性(availability)，这就要求不能把所有的副本存放在一台机器上，如果此机器的发生disk failure，则数据的所有副本全部不可用。放在同一个机架也类似，因为机架之间的交换机或其它网络设计也可能出现故障。另外一个原则是，最大化网络带宽，如果两个副本的位置相隔太远，跨机架甚至跨数据中心，那么副本的写复制代价是巨大的。因此一般的存放位置包括本机器、同一机架不同机器以及不同机架机器。 垃圾回收。当一个文件被删除，GFS不会真正回收对应的chunk，而只是在log operation记录删除日志后，将对应的文件名设置为隐藏。在一定期限内（默认3天），用户可以执行撤销删除操作。否则，master会通过其后台进程定期扫描其文件系统，回收那些隐藏的文件，并且对应的元数据信息也会从内存中擦除。另外，master的后台进程同时还会扫描孤儿块(orphaned chunk)，即那些不能链接到任何文件的chunk，并将这些chunk的元信息删除，这样在后续的heartbeat中让chunk server将对应的chunk删除。这种垃圾回收机制的优点如下：其一，很明显允许用户作出撤销删除操作。其二，统一管理的垃圾回收机制对于故障频繁的分布式系统而言是便捷且可靠的（系统中很容易出现孤儿块）；最后，也有利于提升系统性能。垃圾回收发生在后台进程定期扫描活动中，此时masetr相对空闲，它不会一次性将大量文件从系统移除，从而导致 IO 瓶颈，换言之，其chunk回收成本被均摊(amortized）。但其同样有缺点：如果系统中一段时间内频繁出现文件删除与创建操作时，可能使得系统的存储空间紧张（原论文中也提供了解决方案）。 一致性模型 和 原子 Record Append前文提到GFS并没有采用复杂的一致性协议来保证副本数据的一致性，而是通过定义了三种不同的文件状态，并保证在这三种文件状态下，能够使得客户端看到一致的副本。三种状态描述如下：GFS将文件处于consistent状态定义为：当chunk被并发执行了操作后，不同的客户端看到的并发执行后的副本内容是一致的。而defined状态被定义为：在文件处于consistent状态的基础上，还要保证所有客户端能够看到在此期间对文件执行的所有并发操作，换言之，当文件操作并发执行时，如果它们是全局有序执行的（执行过程中没有被打断），则由此产生的文件状态为defined（当然也是consistent）。换言之，如果某一操作在执行过程中被打断，但所有的并发操作仍然成功执行，只是对文件并发操作的结果不能反映出任一并发操作，因为此时文件的内容包含的是各个并发操作的结果的混合交叉，但无论如何，所有客户端看到的副本的内容还是一致的，在这种情况下就被称为consistent。自然而然，如果并发操作文件失败，此时各客户端看到的文件内容不一致，则称文件处于undefined状态，当然也处于inconsistent状态。 我们先区分几种不同的文件写类型：write指的是由应用程序在写入文件时指定写入的offset；而append同样也是由应用程序来指定写入文件时的offeset，只是此时的offset默认为文件末尾；而record append则指的是应用程序在写入文件时，只提供文件内容，而写入的offset则由GFS来指定，并在写成功后，返回给应用程序，而record append操作正是GFS提供一致性模型的关键，因为它能够保证所有的record append都是原子的(atomic)，并且是at least once atomically。这一点并非我们想像的简单，其所谓的at least once atomic，并不表示采用了atomic record append后，即使在客户端并发操作的情况，也能保证所有的副本完全相同(bytewise idetical)，它只保证数据是以原子的形式写入的，即一次完整的从start chunk offset到end chunk offset的写入，中间不会被其它操作打断。且所有副本被数据写入的chunk offset是相同的。但存在这种情况，GFS对某一副本的执行结果可能会出现record duplicate或者inset padding，这两种情况的写入所占居的文件区域被称为是inconsistent。而最后为了保证应用程序能够从所有副本看到一致的状态，需要由应用程序协同处理。 如果文件的并发操作成功，那么根据其定义的一致性模型，文件结果状态为defined。这通过两点来保证：其一，对文件的副本应用相同的客户端操作顺序。其二，使用chunk version number来检测过期(stale)副本。 record append操作流程如下：客户端首先去请求master以获取chunk位置信息，之后当客户端完成将数据 push 到所有replica的最后一个chunk后，它会发送请求给primiary chuck server准备执行record append。primary首先为每一个客户端操作分配sequence number，然后立即检查此次的record append操作是否会使得chunk大小超过chunk预设定的值（64MB），若超过了则必须先执行insert padding，并将此操作命令同步给所有副本chunk server，然后回复客户端重新请求一个chunk并重试record append。如果未超过chunk阈值，primary会选择一个offset，然后先在本地执行record append操作，然后同样将命令发送给所有副本chunk server，最后回复写入成功给客户端。如果副本chunk server在执行record append的过程中宕机了，则primary会回复客户端此次操作失败，要求进行重试。客户端会请求master，然后重复上述流程。此时，毫无疑问会造成副本节点在相同的chunk offset存储不同的数据，因为有些副本chunk server可能上一次已经执行成功写入了所有数据(duplicate record)，或者写了部分数据(record segment)，因此，必须先进行inset padding，使得各副本能够有一个相同且可用的offset，然后才执行record append。GFS将这种包含paddings &amp; record segments的操作结果交由应用程序来处理。 应用程序的writer会为每个合法的record在其起始位置附加此record的checksum或者一个predictable magic number以检验其合法性，因此能检测出paddings &amp; record segments。如果应用程序不支持record duplicate（比如非幂等idempotent操作），则它会为每一个record赋予一个unique ID，一旦发现两个record具有相同的ID它便认为出现了duplicate record。由GFS为应用程序提供处理这些异常情况的库。 除此之外，GFS对namespace的操作也是原子的（具体通过文件与目录锁实现）。 我们再来理解为什么GFS的record append提供的是at least once atomically语义。这种一致性语义模型较为简单（简单意味着正确性易保证，且有利于工程实践落地，还能在一定程度上提升系统性能），因为如果客户端写入record失败，它只需要重试此过程直至收到操作成功的回复，而server也只需要正常对等待每一个请求，不用额外记录请求执行状态（但不表示不用执行额外的检查）。除此之外，若采用Exactly-once语义模型，那将使整个实现变得复杂：primary需要对请求执行的状态进行保存以实现duplicate detection，关键是这些状态信息必须进行冗余备份，以防primary宕机。事实上，Exactly-once的语义模型几乎不可能得到保证。另外，如果采用at most once语义模型，则因为primary可能收到相同的请求，因此它必须执行请求duplicate detection，而且还需缓存请求执行结果（而且需要处理缓存失效问题），一旦检测到重复的请求，对客户端直接回复上一次的请求执行结果。最后，数据库会采用Zero or once的事务语义(transactional semantics)模型，但严格的事务语义模型在分布式场景会严重影响系统性能。 延迟 Copy On Write快照(snapshot)是存储系统常见的功能。对于分布式系统而言，一个关键挑战是如何尽可能地降低snapshot对成百上千的客户端并发访的性能影响。GFS同样采用的是copy on write技术。事实上，它延迟了snapshot的真正执行时间点，因为在分布式系统中，副本是必须的，大多数情况下，快照涉及的副本可能不会被修改，这样可以不用对那些副本进行 copy，以最大程度提升系统性能。换言之，只有收到客户端对快照的副本执行mutations才对副本进行 copy，然后，将客户端的mutations应用到新的副本。具体的操作流程如下：当master收到客户端的snapshot指令时，首先会从primary节点revoke相应chunk的lease（或者等待lease expire），以确保客户端后续对涉及snapshot的chunk的mutations必须先与master进行交互，并对这些操作执行log operation，然后会对涉及到的chunk的metadata执行duplicate操作，并且会对chunk的reference count进行累加（换言之，那些chunk reference count大于1的chunk即表示执行了snapshot）。如此一来，当客户端发现对已经快照的chunk的操作请求时，master发现请求的chunk的reference count大于1。因此，它会先defer客户端的操作请求，然后选择对应chunk的handler并将其发送给对应的chunk server，让chunk server真正执行copy操作，最后将chunk handler等信息返回给客户端。这种delay snapshot措施能够改善系统的性能。 最后，值得注意的是，虽然客户端并不缓存实际的数据文件（为什么？），但它缓存了chunk位置信息，因此若对应的chunk server因宕机而miss了部分chunk mutations，那客户端是有可能从这些stale的replica中读取到premature数据，这种读取数据不一致的时间取决于chunk locations的过期时间以及对应的文件下一次被open的时间（因为一旦触发这两个操作之一，客户端的cache信息会被purge）。 参考文献： [1] Ghemawat S, Gobioff H, Leung S T. The Google file system[M]. ACM, 2003.[2].MIT 6.824 Lecture]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式文件系统</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式文件系统</tag>
        <tag>GFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式互斥算法解析与实现]]></title>
    <url>%2F2018%2F11%2F09%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[资源共享非常普遍，在单机系统中，进程间对共享资源的互斥访问可以通过互斥锁、信号量以及进程间通信等机制来实现。而在分布式系统中，也不可避免多个节点共享某一资源或同时执行某一函数，比如全局配置文件，因此分布式互斥算法必须保证任何时刻只允许一个进程访问资源或执行临界区(critical section)代码，即互斥算法的安全性，有些场景也有公平性要求。另外，好的互斥算法应该能尽可能降低消息带宽(message overhead)，减少进程（节点）等待时间，即时延(latency)，无系统瓶颈(bottleneck)，也能容忍消息乱序。 分布式互斥算法分为集中式算法(centralized algorithm)和分布式算法(distributed algorithm)，而分布式算法又包括了基于令牌的算法(token based algorithm)以及基于请求的算法(permission based algorithm)。无论基于何种原理实现，一般而言，理想的分布式互斥算法需要保证以下目标： 安全性，即任何时刻只能有一个进程访问共享资源，即持有互斥锁。 公平性，有些场景需要尽量保证访问共享资源的公平性，这表明：系统不能出现死锁，任何进程持有锁的时间是有限的，任何等待的进程最终都能获取锁，以及等待获取锁的进程的等待时间是有限的。 低带宽，即尽量减少消息传输的数目。 低延迟，即进程进入临界区之前的等待的时间。 动态性，即允许进程在任何时刻加入到访问共享资源的进程集合中，或者从其中退出。 容忍进程失败，即允许访问共享资源的进程集合中的进程因失败而退出，而保证整个系统不受影响。 容忍消息丢失，即在消息不能按时到达、乱序甚至丢失的情况下，整个系统依然正常工作。 在本文我们讨论前四个要求，假设进程数目是确定的，没有进程会失败，消息也不会丢失。下面我们通过简要阐述算法原理以及实现关键点来依次介绍Centralized Mutual Server算法、Ricart Agrawala算法、Lamport Distributed Mutual Exclusion算法以及Token Ring算法。 Centralized Mutual Server顾名思义，Centralized Muutal Server为集中式的互斥算法。整个系统内部包括两种消息：请求(reqeust)消息、授权(grant)消息以及释放(release)消息。核心数据结构为一个请求消息队列。算法核心为：它选取一个进程(centralized server)作为协调者，负责对名进程的请求进行即时或推迟(defer)授权。它内部维护一个互斥锁锁请求队列，当收到请求消息时，如果队列为空，则直接授权，否则将其加入到队列中。当收到释放消息时，如果列队不为空，则从队列中取出一个请求并授权响应。算法公平性依赖于队列实现，如使用FIFO则能够保证各个进程的锁请求消息能够被公平地授权。消息带宽为3(1 request, 1 grant, 1 release)，即在某一进程从准备进入临界区到退出临界区所传输消息的数量。很明显，集中式互斥算法的缺点是协调者的瓶颈。 集中式互斥算法最容易实现。在网络通信层，可以采用基于TCP的client-server通信模型。关于协调者的实现，你可能需要关注当前是否已经授权了锁请求。同时，如果有必要，注意单进程内部锁的使用。 Ricart AgrawalaRicart &amp; Agrawala算法是在1981年被提出的一个基于请求的分布式互斥算法。它基于lamport clock，即依赖于全局有序的逻辑时钟。整个系统内部包括两种消息：请求(reqeust,i,ts)消息与回复(reply,j)消息。核心数据结构包括缓存其它进程回复消息的队列(replyQueue)以及缓存推迟回复进程请求消息队列(deferQueue)。算法核心为：当进程i准备进入临界区时，必须发送一个带（逻辑）时间戳的请求消息给其它所有进程，当其收到了其它所有进程的对此请求的回复（响应）时，则进入临界区。但如果某一进程j在收到进程i的请求之前，发出了一个更早的请求消息，则它会将此进程(i)的请求消息放入到延迟队列(deferQueue)，并且先执行完临界区的代码，当准备退出临界区时，才发送请求响应给进程(i)。算法的公平性容易保证。而消息带宽为{request: n-1, reply: n-1} =&gt; 2(n-1)，其中n为进程数。 Ricart Agrawala算法的相比集中式算法在实现上更为复杂。同样在通信层，则不能构建one server, muliti-client模型，而采用peer to peer模型，因为所有进程都是对等的，即同时充当server与client，而且作为一种简化实现，所有进程在启动后，应该互相建立连接。除此之外，你需要实现（模拟）lamport clock 算法，否则互斥算法的正确性不能得到保证，注意对于某些消息（如reply）的发送事件，虽然可以更新消息时间戳，但其实不影响算法正确性。 Lamport Distributed Mutual ExclusionLmpoart Distributed Mutual Exclusion算法于1978年由 Lamport 在关于lamport clock理论论文中提出，其作为lamport clock的实际应用，因此，显然其依赖于lamport clock。事实上，此算法不仅可以作为分布式互斥算法，其内部的请求优先级队列也能作为分布式节点副本一致性的实现参考模型。但原论文提出的互斥算法基于消息按顺序到达的假设，解释如下： 比如，进程i在时间片1发送锁请求a，但因为网络原因被极端延时了。而且在其它进程收到进程i发送的请求消息a之前，进程j在时间片5发送了请求b，而请求b恰好被顺利传输，很快被其他进程接收，并且其他进程（包括进程i）立刻发送了对请求消息b的回复消息，同时回复消息也立刻被进程j接收，但此时进程j仍未收到进程i的请求消息a，所以进程j以为自己成功获取到锁（收到了其它所有进程对请求消息b的回复）。而事实上，进程i的请求消息a要比进程j的请求消息b更早发送，因此应该是进程i先获取锁。其根本原因在于，进程i在收到其他节点请求消息（进程j的请求消息b）时，没有进行额外检查，理论上它需要判定自己是否在更早前发出过请求消息，而不只是直接对请求消息回复，即使最后其在请求消息队列里移除的消息是它自己的请求消息（因为自己是请求消息是最早的）。但这造成了整个系统的不正确性。 因此，改变Lmpoart Distributed Mutual Exclusion算法在接收请求消息后发送回复消息的条件，消除了消息按序到达的假设，但同时也使得变更后的算法更为复杂。 系统内部包括三种消息：请求(reqeust,i,ts)消息、回复(reply,j)消息以及释放(release)消息。核心数据结构包括缓存其它进程回复消息的队列(replyQueue)、缓存推迟回复进程请求消息队列(deferQueue)以及一个以时间戳为依据的请求消息优先级队列(requestPriorityQueue)。算法变更的核心为：进程i在收到进程j的请求消息(request, j, t)时，（条件1）先判断自己是否发送过更早的请求消息，（条件2）并且未收到进程j针对此请求消息的回复消息。如果二者之中任一个未被满足，则对进程i的请求消息发送回复，否则将其加入到deferQueue。原因如下：条件1是明显的；关于条件2，如果进程j已经收到了进程i的消息回复，说明进程i先前发出的请求消息肯定已经被进程j接收（换言之，进程i若发送过请求消息，则此请求消息必定已经缓存到了进程j的requestPriorityQueue），因此消除了消息延迟（乱序）的影响。另一方面，当进程i收到请求回复消息时，它会先将其加入到replyQueue，并判断发送此回复消息的进程是否被加入到了其deferQueue中，如果已经加入到了，则将其移除，然后对此进程发送回复消息（因为进程i确认它已经收到被移除进程的回复消息）。其它的算法逻辑同论文中描述一致。事实上，消除消息按序到达的关键为deferQueue。算法的公平性容易保证。而消息带宽为{request: n-1, reply: n-1, release: n-1} =&gt; 3(n-1)，其中n为进程数。 Lmpoart Distributed Mutual Exclusion算法的相比Ricart Agrawala算法在实现上更为复杂。通信层采用peer to peer模型。 Token RingToken Ring是基于令牌的互斥算法。是一种简单的互斥算法模型，局限性也较大。系统内部只有一种消息：传递 token 的(OK)消息。算法核心为：将所有进程在逻辑上组成一个环，并将 token 在环上依次传递，获取到 token 的进程则具备进行临界区的条件，未收到 token 的进程则必须等待。在进程启动时，必须先将 token 传递给某一进程，若此接收进程需要锁，则进入临界区，执行完临界区代码后，再将 token 传递给相邻的下一个进程。否则直接将 token 传递给相邻下一个进程。算法的公平性同样易保证。消息带宽为n-1，其中n为进程数。 Token Ring算法较易实现，同样采用peer to peer通信模型。注意进程启动时，初始的 token 持有者。 关于测试在实现上述四种算法时(go语言)，采用TCP协议（可靠的）。测试的流程包含两个独立的阶段：Phase a. 每个进程独立的重复以下操作若干次。 执行本地操作。采用 sleep [100, 300]ms 来模拟。 开始进入临界区(critical section)。执行获取互斥锁逻辑。 执行临界区代码。对一个共享变量进行累加，在 [100, 200]ms超时时间内，每隔100ms，对共享变量随机增加 [1,10]。将累加过程写入文件，同时将累加的中间值记录到全局数组。 退出临界区。执行释放互斥锁逻辑。 Phase b. 每个进程独立的重复以下操作若干次。 进程号为偶数的进程 sleep [100, 300]ms，然后重复 Phase a 操作流程。进程号奇数的进程直接重复 Phase b 流程。 对上述四个分布式互斥算法的测试结果的验证侧重于两个方面： 算法正确性。通过检查 Phase a&amp;b 中全局数组的记录情况来确保共享资源的互斥访问。另外，核查 Phase a&amp;b 中进程访问共享资源的访问日志文件。 带宽与延时。统计每个进程的消息读写数目，及获取互斥锁的延时，并计算平均延时。 参考代码在这里。 参考文献： [1] Ricart G, Agrawala A K. An Algorithm for Mutual Exclusion in Computer Networks[R]. MARYLAND UNIV COLLEGE PARK DEPT OF COMPUTER SCIENCE, 1980.[2] Lamport L. Time, clocks, and the ordering of events in a distributed system[J]. Communications of the ACM, 1978, 21(7): 558-565.[3].CMU Distributed System Lecture.]]></content>
      <categories>
        <category>分布式系统</category>
        <category>互斥算法</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>互斥算法</tag>
        <tag>资源共享</tag>
        <tag>lamport clock</tag>
        <tag>逻辑时钟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统时间、时钟与事件顺序]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%97%B6%E9%97%B4%E3%80%81%E6%97%B6%E9%92%9F%E4%B8%8E%E4%BA%8B%E4%BB%B6%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[如何确定分布式系统各节点（进程）中事件发生的先后顺序至关重要。时钟不一致会导致系统发生不可预料的逻辑错误。然而绝大部分情况下，不能依赖于物理时钟，因为不同的系统的物理时钟总会存在不同程度的时钟漂移（多处理器机器中也类似），即便各节点定期通过网络从时钟源进行时钟同步，也无法确保各节点时钟完全一致。因此，早在1978年，Leslie Lamport 便提出了逻辑时钟的概念，并描述了如何利用逻辑时钟来定义分布式系统中事件的发生顺序。它大致基于事件发生的因果关系，并保证能够正确排列系统中具有因果关系的事件，这使得分布式系统在逻辑上不会将具有因果关系事件的发生顺序倒置。 时钟同步事实上，计算机的时钟会以不同速率来计时，普通的石英钟漂移(skew drift)1秒所需的时间大概为11-12天。因此如果使用物理时钟physical clock所定义的时间戳来确定系统中事件发生顺序，需要对物理时钟进行定期同步(clock synchronization)。在理想网络环境下，通过网络将带有时间戳的消息在时钟源（UTC,Coordinated Universal Time）与本地机器之间传输，能够保证本地时间与时钟源基本一致。但事实上，网络是异步的且有延时，因此无法保证不同节点之间的时钟完全同步。尽管如此，我们可以通过算法来尽可能提高时钟同步精度。著名的时钟同步算法如Cristian&#39;s Time Sync和Berkeley algorithm。 Lamport Clock相比于通过同步物理时钟的方式来协调各节点的时间，在分布式系统中，更为普遍且合理的方式是使用逻辑时钟(logical clock)。Lamport 提出的逻辑时钟舍弃了物理时钟固有的无限粒度的性质，它基于事件发生的因果关系(causality)。换言之，所有的事件通过happened before来关联，以-&gt;表示。对于事件a与b，a-&gt;b表示a happened before b，它是一种偏序关系(partial order)，分布式系统中所阐述的事件发生的先后顺序一般为偏序。Lamport 在分布式系统内定义了三种类型的事件，包括进程（节点）内事件、进程发送消息事件以及进程接收消息事件。a happened before b由以下三个条件中任一一个触发： 若a与b表示同一进程内的事件，并且a发生在b之前，则有a-&gt;b。 若a代表某一进程发送消息的事件，b代表另一进程接收此消息的事件，则有a-&gt;b。 happened before关系满足传递性。 如下图（水平方向表示物理时钟增加方向，垂直方向表示不同进程），由规则(1): a-&gt;b及c-&gt;d; 由规则(2): b-&gt;c和d-&gt;f; 由规则(3): b-&gt;f。但并非所有的事件都能通过-&gt;关联，比如a与e为不同进程不同消息链上的事件，则只能被定义为并发的两个事件，记作a||e。事实上，事件a与e没有因果关系，因此，从系统正确性的角度而言，它们之间真正的发生顺序不会影响到系统的正确性，所以我们不需要关注它们发生的先后顺序。 如果将系统中所有发生的事件e标记一个单调递增的时间戳(L(e))（与物理时钟没有关系），也称为lamport timestamp/clock，每一个进程都会维护自己的逻辑时钟，时间戳标记原理如下： 每个事件对应一个初始的时间戳，初始值为0。 如果发生的事件为进程内事件，则时间戳加1。 如果事件为发送事件，则将时间戳加1，并在消息中带上该时间戳。 如果事件为接收事件，则其时间戳为max(进程时间戳，消息中附带的时间戳)+1。 如下图，p1、p2以及p3都有自己的初始的逻辑时钟0；进程的全局（当前）逻辑时钟即为当某一事件发生之后的逻辑时钟值，如事件a与b的逻辑时钟值分别为0+1=1和1+1=2。而发p1发送消息m1给p2后，c的逻辑时钟值为max(0, 2)+1=3。 在lamport clock表示法中，对于事件e1与e2，e1-&gt;e2能推断出L(e1)&lt;L(e2)，但反之不成立，即L(e1)&lt;L(e2)不能推断出e1-&gt;e2，如在图(2)中，L(b)&gt;L(e)，但实际上有b||e。另外，并发事件也是类似的，即若L(e1)=L(e2)可以推断出e1||e2，但反之不成立。对于lamport clock，并发事件没有可比性，正如上文所述，并发事件发生的先后顺序并不影响系统逻辑正确性。 在lamport clock表示法中，无法确定没有因果关系的事件的先后顺序，而大多数分布式系统确实需要对所有的事件进行全局排序(total order)，而不仅仅得到影响系统正确性的事件之间的偏序关系(partial order)。换言之，为了得到一个全局的事件发生顺序，必须对并发事件进行先后发生顺序的判定。因为并发事件真正发生的先后顺序不影响系统的准确性，因此可以为它们统一制定一个任意顺序规则（事实上，lamport clock就是这么考虑的）。比如同其它因果关系事件类似，以逻辑时钟(L(e))的大小来判定，逻辑时钟小的发生在前，反之则发生在后。而对于逻辑时钟相同的并发事件，在lamport clock算法当中，给出的解释是根据进程号(PID)的大小来确定，进程编号更小的发生在前。其实，Lamport 在论文中提到过，也可以采用其它方式来确定并发事件的先后顺序，这似乎没有理论依据，但是正如前面所述，通过引入lamport clock，可以在逻辑上保证系统的正确性，我们不关心那些不影响系统正确运行的事件之间的顺序。但以进程编号作为依据，似乎影响到了系统的公平性，比如当两个进程竞争同一物理资源，物理时间上先发出请求的进程不一定能先锁定资源，但这并不会造成系统逻辑错误。在lamport clock原论文中，给出的实例便是分布式系统资源竞争或者互斥占用，大家可以参考原论文。 Vector Clock前文提到lamport clock存在一个缺点，即对于事件e1与e2，L(e1)&lt;L(e2)并不能推导出e1 happened before e2，换言之，其只能确定单方的因果关系关联。vector clock是在lamport clock上演进的一种逻辑时钟表示法，它完善了lamport clock这一缺陷，能提供完整的因果关系关联。在vector clock表示法中，每个进程维护的不仅仅是本进程的时间戳，而是通过一个向量(vector)来记录所有进程的lamport clock以此作为进程的逻辑时钟，即进程事件的逻辑时钟被表示为：v(e)[c1, c2..., cn]，其中ci为进程i中先于事件e发生的事件。vector clock的逻辑时钟标记原理同lamport clock原理类似。 如下图，在进程p1中，事件a的逻辑时钟为(1,0,0)，发送消息m的事件b的逻辑时钟为(2,0,0)。在进程p2中，其接收消息m1的事件c的逻辑时钟为max((0,0,0),(2,0,0))+1=(2,1,0)。此时，对于事件e1与e2，由e1 happened before e2推断出v(e1)&lt;v(e2)，反之亦然。在vector clock表示法中，事件c与e是并行事件，记作c&lt;-&gt;e，因为我们不能推导出v(c)&lt;=v(e)，也不能推导出V(e)&lt;=v(c)。注意，此时逻辑时钟值的比较(&lt;|&gt;|&lt;=|&gt;=)是对向量的分量逐一比较。 参考资料：（插图出自 CMU Lecture）[1] https://en.wikipedia.org/wiki/Cristian%27s_algorithm[2] https://en.wikipedia.org/wiki/Berkeley_algorithm[3] Lamport L. Time, clocks, and the ordering of events in a distributed system[J]. Communications of the ACM, 1978, 21(7): 558-565.[4].CMU 15-440 Distributed System Lecutre 9]]></content>
      <categories>
        <category>分布式系统</category>
        <category>逻辑时钟</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>lamport clock</tag>
        <tag>逻辑时钟</tag>
        <tag>时钟同步</tag>
        <tag>物理时钟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[groupcache 设计原理剖析]]></title>
    <url>%2F2018%2F10%2F29%2Fgroupcache-%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[groupcache是一个用go实现的分布式k/v缓存及缓存填充库，它的作者也是memcached的作者，它已在Google多个生产环境中使用。它非常小巧精致，比较适用于分布式缓存的学习。它本身只是一个代码包（大约2000行代码，不需要配置服务器，在不同的请求处理场合，它可以充当客户端或者服务器的角色。它支持一致性哈希，即通过一致性哈希来对查询请求进行路由。对于缓存的具体策略，groupcache采用的是LRU，使用了一个List和一个Map来实现，非常简单。下面先简述本地缓存的基本模型和常见问题，然后剖析groupcache的设计原理。 单机缓存或者本地缓存是简单的，通过在内存中维护一个cache，当收到查询时，先查询cache是否已缓存查询结果，如果命中则直接返回，否则必须到存储系统执行查询，然后将结果先缓存到cache，然后返回结果。当然，这是本地缓存的基本模型，一般而言，缓存系统都面临着诸如缓存穿透、缓存雪崩及缓存击穿等问题。 缓存穿透指的是查询一定不存在的数据，此时从数据源查询不到结果，因此也无法对结果进行缓存，这直接导致此类型的查询请求每次都会落到数据源层，不仅使得缓存失效，当请求数量过多时也会浪费资源。 缓存雪崩指的是大量的缓存的过期时间被设置为相同或近似，使得缓存失效时，所有的查询请求全部落地到数据源层，同样，此时数据源层存在服务不可用的可能性。 缓存击穿则指的是对于那些热点数据，在缓存失效时，高并发的查询请求也会导致后端数据源层崩溃。 对于groupcache的设计，文章从一致性哈希、缓存命名空间、热数据扩散以及缓存过滤几个方面进行阐述。 一致性哈希一致性哈希最初是在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中提出，目标是致力于解决因特网中的热点(Hot spot)问题，并真正应用于p2p环境，它弥补简单的哈希算法的不足。一般可以从四个方面来衡量哈希算法的适用性。 平衡性。平衡性即哈希的结果能够尽可能的分散到所有节点或缓冲，以保证缓冲空间被最大程度使用。 单调性。单调性即如果当前缓存系统已经存在被映射的缓冲内容，当有新的节点加入到系统时，哈希算法应该能够尽可能保证原有已分配的的缓冲内容只能被映射到原有的对应节点或者新的节点，而不能被映射到旧的节点集合的其它节点。 分散性。分布式环境中，不同终端所见的节点范围有可能不同（因为可能只能看见部分节点），这会导致不同终端的哈希结果不一致，最终，相同的内容被映射到了不同的节点。而分散性则专门用于描述此种情况发生的严重程度。好的哈希算法应该尽量避免发生这种情况，即降低分散性。 负载。本质上与分散性阐述的是同一问题。但它从节点出发，即某一特定的节点应该尽可能被相同的缓冲内容所映射到，换言之，避免（不同终端）将相同的内容映射到不同的节点。 所谓一致性哈希，简而言之，即将节点与缓冲内容分别映射到一个巨大的环形空间中，最终内容的缓存节点为在顺时针方向上最靠近它的节点。可以发现，系统中节点的添加与删除，一致性哈希算法仍能基本满足以上四个特性。另外一个关键问题是，当集群中节点数量较少时，节点分布不均匀（即节点所负责的内容范围相差较大）会直接导致内容（数据）倾斜，因此一般会引入虚拟节点，即将节点映射为虚拟节点。如此，整个缓存映射过程便拆分为两个阶段：对于特定缓冲内容，先找到其映射的虚拟节点，然后再由虚拟节点映射到物理节点。 一致性哈希在分布式缓存中充当查询路由角色，因为不同节点负责特定的key集合。因此，如果此时当查询没能在本节点缓存中命中时，则需通过一致性哈希路由特定节点(peer)，然后借助http发送数据查询请求，请求的协议格式为: GET http://peer/key。因此，所有节点必须监听其它节点的数据查询请求，同时具备相应的请求处理模块。 缓存命名空间即便是在单个节点上，也可以创建若干个不同名称的缓存命名空间，以使得不同命名空间的缓存相互独立。如此，可以在原本针对key进行分片的基础上，丰富缓存功能。因此，节点间的数据查询请求协议格式变更为：GET http://peer/groupname/key。 热点数据扩散分布式缓存系统，不同的节点会负责特定的key集合的查询请求。但因为并非所有的key的访问量是均匀的，因此，存在这种情况：某些key属于热点数据而被大量访问，这可能导致包含该key的节点无法及时处理甚至瘫痪。考虑到这一点，groupcache增加了热点数据自动扩展的功能。即针对每一个节点，除了会缓存本节点存在且大量被访问的key之外（缓存这些key的对象被称之为maincache），也会缓存那些不属于本节点，但同样被大量访问（发生大量地miss cache）的key，而缓存这些key的对象被称这为hotcache，如此便能缓解热点数据的查询请求集中某一个节点的问题。 缓存过滤机制groupcache的singleflight模块实现了缓存过滤机制。即在大量相同的请求并发访问时，若缓存未能命中，则会触发大量的Load过程。即所有的查询请求全部会落到数据源（如DB）或从其它节点加载数据，因此考虑到节点可靠性，此时DB存在因压力过大而导致服务不可用的情况，同时也浪费资源。groupcache设计所提供的解决方案是：尽管存在并发的查询，但能保证只有一个请求能够真正的转发到DB执行查询，而其余的请求都会阻塞等待，直至第一个请求的查询结果返回，同时，其它请求会使用第一个请求的查询结果，最后再返回给客户端。singleflight通过go的sync.WaitGroup实现同一时间相同查询请求的合并。 最后，虽然官方声称groupcache在很多场景下已经成为memcached的替代版，但其本身存在固有的”局限性”。 groupcache采用的是LRU缓存机制，使用List和Map实现，不支持过期机制（不支持设置过期时间），也没有明确的回收机制（只是简单地将队尾的数据移除），但能够控制缓存总大小在用户设置的阈值之下。 groupcache不支持set、update以及delete，即对于客户端而言，只能执行get操作。 groupcache针对key不支持多个版本的值。 总而言之，groupcache是一个值得学习的开源分布式缓存系统，通过阅读源码，一方面可以了解分布式缓存相关的设计原则，也能学习编程相关的设计经验。]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式缓存</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式缓存</tag>
        <tag>LRU缓存</tag>
        <tag>一致性哈希</tag>
        <tag>缓存过滤机制</tag>
        <tag>缓存击穿</tag>
        <tag>热点数据扩散</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Live Sequence Protocol 实现]]></title>
    <url>%2F2018%2F10%2F25%2FLive-Sequence-Protocol-%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[分布式环境中，网络不稳定导致消息（数据包）的传输存在乱序、重复和丢失的情况，同时，节点宕机也不可避免。如何优雅地处理这些问题，是构建一个健壮的分布式系统的关键。网络的复杂性使得数据包传输协议至关重要。低级别的IP协议提供不可靠的数据报服务，即消息可能延时、重复或丢失，另外，它也限制了在网络节点中传输的消息的最大字节数，因此很少直接利用IP协议来构建分布式应用。在传输层，UDP也不提供可靠的数据报服务，但它可以通过端口定向传输报文。而TCP则会保证消息传输的可靠性、有序性，并允许任意字节大小的消息传递，还提供额外的功能，如流量控制、拥塞控制。 我们的目的是实现一个基于UDP、具备TCP几个关键特性的消息传输协议 (Live Sequence Protocol），同时它还具备如下功能： 不同于UDP或TCP，它支持 client-server通信模型 。 server会维护若干个client的连接。 server与client的通信是通过向对方发送消息来实现，消息大小限制与UDP相同。 消息传输是可靠的：消息一旦发送，就会被顺序接收，且每个消息只会被接收一次。 server与client可以检测连接的状态。 协议具体的工作原理、关键特性、运行流程及开放使用的接口可以参考p1.pdf。下面我会讨论协议实现过程中的几个关键点，以及个人在实现过程中遇到的棘手的问题。 系统逻辑框架构建清晰且优雅地构建整个系统的逻辑框架至关重要，代码框架设计关系到后期功能模块调试与扩展，不合理的系统逻辑框架设计会使得后期的扩展寸步难行，也会导致代码的可调试性、可读性变差。因此，在编写出你的第一个可用的版本之前，尽可能合理地安排系统框架，这需要理解并梳理系统的主干及各分支（异常）运行流程，为了更简单、高效且合理地实现模块功能，必须尽可能熟悉(go)语言的特性(channel、gorountine及interface)。 协议实现文档清晰地描述了协议的完整工作流程，按照此流程，其核心是epoch event触发后，协议的应对逻辑，可以实现出一个可运行的版本。合理安排程序框架关键在于处理好以下三个方面的问题： 哪些功能逻辑应该被顺序执行，如何保证同步顺序执行。比如，当创建client后，只有当其与server建立连接connection（抽象连接，并非消息传输所使用的连接）后才能返回，同时启动后台服务。注意client创建UDP连接到server可能会尝试多次，因为server可能存在慢启动问题，而且Connect消息也可能丢失。 系统需要哪些后台服务(background goroutine)， 后台服务如何可靠地同主线程协调交互。比如，对于client而言，至少需要三个goroutine来处理消息。 read goroutine持续从连接中读取消息，直到连接关闭。 write goroutine，因为写操作的调用是非阻塞的，但由于滑动窗口大小限制，并非所有消息都能立刻cache到滑动窗口并立即发送出去，因此，可以将用户wirte的消息放入到消息的write channel中，然后由专门的后台服务从channel中取消息，并在恰当的时候发送消息。 epoch event trigger goroutine，即处理与epoch相关的逻辑，超时如何处理？接收到Ack消息或Data消息如何处理？达到max epoch时如何处理？ 确保开放接口的实现符合协议规范中预定义的准则要求。比如，server的Read接口的调用会阻塞直到其从任一client收到消息，然后返回消息的payload及对应的connection ID。如果连接丢失，关闭或者Server主动关闭终止，都应该返回错误提示。这个方法不应该被简单地设计成从连接中持续读取数据，因为Server可能连接多个client，针对每一个client 连接的读取，必须启用单独的goroutine。所以，一种简单的设计是server并发地从各连接读取数据，若通过了校验（如保证用户调用Read所返回的数据正是用户所期望的），则将数据放入到channel，让Read持续从channel中取数据，注意数据一旦添加到channel中，则会以放入的顺序被Read取出，并返回给用户。 理解UDP通信本质大家可能对TCP原理及编程更为熟悉，UDP相对简单，但因为lsp(Live Sequence Protocol)基于UDP，并在更高的协议抽象层面具备TCP的特性，所以，不要混淆了二者的通信原理。UDP是无连接的！它会完全按照程序员的意愿发送消息，它不考虑对方主机是否存在或正常工作，也不会主动重发消息，因此，也就无法保证消息的可靠接收与发送。 所以，server不需要也不能维护其与client的连接！但应当在sever端创建并维护与其通信的client关联的信息实体（需包含哪些数据？），那何时创建？答案是当server读取到数据时，因为此时可以获取读取所返回的client地址，server可以通过cache已经连接的信息来判断此次读取对应的连接是否是新的连接。若不是，则直接进入消息读取处理逻辑，否则需要先初始化server维护的client相关联的信息实体。 最后，注意server与client使用的是不同的UDP读写通信接口。（client直接持有与server通信的连接，而server是通过指定地址（IP+port）发送与接收消息）。 如何实现滑动窗口滑动窗口sliding window是协议实现流量控制的关键，是整个协议的功能核心，并且其与TCP的滑动窗口机制类似。关于滑动窗口，在理解它的工作原理后，重点考虑以下三个方面： 设计滑动窗口的数据结构。 消息应该被有序添加到滑动窗口。 发送消息窗口需要标识每一条消息是否已经被ack。 发送消息所关联的滑动窗口latestSentDataMsg。以client作为示例，维持其发送消息的窗口，以便对未按时返回Ack的消息进行重发（已发送的data消息可能会丢失，或者接收主机响应的Ack消息丢失）。 因为窗口内的消息所返回的Ack是无序的（消息异步发送，网络传输也不能保证消息按序到达），所以，需要维护一个指针，表示当前返回的Ack消息的最小的序号receivedAckSeqNum，以作为窗口向前推进的依据。 当client发送data消息时，需同时将其cache到latestSentDataMsg。而当其接收到Ack消息时，需要执行更新此指针receivedAckSeqNum的逻辑。而server则需要对其所维护的每一个连接构建对应的发送消息窗口，但处理逻辑类似。 接收消息所关联的滑动窗口latestReceivedDataMsg。同样以client作为示例，维持其接收消息的窗口，以便在计时器超时后，对最近收到的若干个data消息，重发Ack消息。 同样，接收消息窗口也是无序的，因此，为了保证返回给用户的消息有序，需要维护一个指针，表示下一个期望接收到的data消息序号nextReceiveDataSeqNum（或者是当前已经接收到的最大的data消息的消息序号），它是依次递增的。对于接收到的任何data消息，若其SeqNum在此指针之后，都应该直接添加（暂时缓存）到latestReceivedDataMsg中，而不应该作为Read调用的返回结果。 当client收到server的data消息时，也需要将其cache到latestReceivedDataMsg，并判断是否需要更新nextReceiveDataSeqNum，若需要更新，则应当将更新过程中所涉及到的cache在接收消息窗口中的data消息按序添加到供Read接口所读取的channel。server同样是一个连接对应一个接收消息窗口。 如何实现流量控制流量控制表示若当前主机有过多的消息未被ack（网络拥塞），因此发送主机需要对用户调用Write接口的data消息进行阻塞以延缓发送。其实现关键是滑动窗口机制。具体实现原理为： 当用户调用Write接口以发送消息时，将消息添加到消息发送队列channel，然后返回，不能阻塞。 后台服务write goroutine从消息发送队列中不断的取消息，但在消息正式发送前，需要检测消息发送滑动窗口是否空闲idle，并且包含多少空闲的slot。 空闲的slot数目可以根据以下表达式计算：idleSlotNum := cli.params.WindowSize-(lastMsg.SeqNum-cli.receivedAckSeqNum)，其中，lastMsg为消息发送窗口中最后一个消息，即SeqNum最大的消息。 如果idleSlotNum大于0，则可以发送对应数目的消息，并将已经发送的消息记录到消息发送窗口，同时递增nextSendDataSeqNum指针。否则，write goroutine应该被阻塞住。那如何解除阻塞？每当client在接收到Ack消息时都要去尝试解除阻塞。 如图所示，当滑动窗口处于(a)的情况下，当用户调用Write以发送消息时，消息会被阻塞在write channel中，因为此时receivedAckSeqNum为9，消息发送窗口的idle的slot数目为：5-(14-9)=0。而当client接收若干Ack消息后，滑动窗口转移到(b)状态时，注意到receivedAckSeqNum从9逐一递增到11，消息发送窗口idleSlotNum为：5-(14-11)=2，因此窗口前移，并可以从write channel中顺序取出两个消息，进行发送。 如何检测消息重复消息重复主要包括data和Ack消息的重复接收。以client作为示例。 data消息的重复接收。 当client读取到data消息后，需要判断消息是否已经接收过。若消息重复，则直接返回Ack消息，否则应该先将消息cache到latestReceivedDataMsg。 可以通过消息的SeqNum来去重。这涉及两种情况：其一，消息已经被Ack，并且已经从latestReceivedDataMsg中移除，我们称之为消息被丢弃(discarded)。其二，消息被Ack，但仍然cache在latestReceivedDataMsg中。 Ack消息的重复接收。Ack消息的去重逻辑同data消息类似。 如何保证消息顺序发送主机异步发送消息，且消息在网络中传输也有不同程度的延迟，因此接收主机接收的消息序列的顺序很可能与发送主机发送的消息顺序不同。如何保证消息顺序？准确而言，如何以发送主机发送消息的顺序来返回给用户。 针对具备滑动窗口机制的消息传输，可以保证滑动窗口前所接收的消息，即已经被discarded的消息肯定是有序返回给用户的。而滑动窗口内的消息，因为无法规避从网络中读取乱序消息的问题，但在读取到消息后可以控制以何种顺序将消息返回给用户。简单而言，将收到的data消息先cache在latestReceivedDataMsg中，然后通过指针nextReceiveDataSeqNum来判断是否应该将窗口中cache的消息返回给用户。 如何优雅地关闭连接保证连接优雅地关闭是一个非常棘手的问题。其中，相比于client端的连接关闭，server的关闭又更为复杂。协议规范清晰地描述了client及server在关闭连接时需要注意的问题。其核心是： 当存在pending消息时，需要将其处理完成（即需保证接收到Ack消息）。 同时，一旦data消息被加入到write channel，它必须保证最后能够被发送出去。client的关闭相对简单，具体处理逻辑为：当用户调用Close接口时，需要判断是否存在pending消息，如何检测？两个条件： 保证消息发送窗口的最后一个消息的SeqNum恰好为其持有的receivedAckSeqNum的值。 保证write channel中没有任何未被处理的消息。因此如果此时存在pending消息，Close会被阻塞。那如何解除阻塞？每当client在接收到Ack消息时都要去尝试解除阻塞。此外，值得注意的是，在阻塞的过程中，如果触发了max epoch event，则client应该立刻返回，因为这表明连接已经discarded，此时要么所有pending消息已被处理，要么server主动关闭了连接。server的CloseConn接口可以看作是client的Close接口的非阻塞版本。而Close接口需要协调所有的connection的关闭。同样，server的某个连接也可能到达max epoch，此时其对应的连接应该被关闭。当所有连接都关闭时，Close才能返回。在连接关闭时，需要及时退出对应的background goroutine。 需注意的细节问题往往一些编程方面的细节，包括逻辑漏洞或者被忽视的语法问题会造成很长时间的调试。而且，当通信过程中，数据交换复杂变得越发复杂时，很难从庞大的日志文件中找出错误的根源。个人在实现的过程中，遇到两个问题： dead lock。死锁很容易产生，一般有两个原因，其一，资源的相互持有，造成两个线程都无法向前推进。其二，没有正确嵌套使用锁，你需要清楚锁是否可重入。 buffered channel。其导致的问题比较隐蔽，你首先要明确是使用带缓冲的channel或者不带缓冲的channel，如果是buffered channel，你需要确定它的大小，如果你不确定缓冲区数量是否足够，建议设置的稍大一些，但这个前提是，必须在合适的时机清空buffered channel，避免在复用buffered channel之后导致逻辑受到影响。 最后，需要提醒的是，分布式程序异步、并发，且网络复杂的特性导致其很难debug。所以，尽可能设计完善的日志流程，以帮助跟踪未符合期望的执行逻辑，并定位问题。 另外，cmu提供较为完善的测试程序，如果程序出现问题，可以对某一个或几个子测试用例进行单独测试，熟悉测试用例代码，了解测试用例流程是有必要的。 参考代码在这里]]></content>
      <categories>
        <category>分布式系统</category>
        <category>传输协议</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>网络编程</tag>
        <tag>传输协议</tag>
        <tag>可靠服务</tag>
        <tag>流量控制</tag>
      </tags>
  </entry>
</search>
