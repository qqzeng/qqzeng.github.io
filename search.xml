<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[理解分布式协调服务 zookeeper]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%90%86%E8%A7%A3%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E6%9C%8D%E5%8A%A1-ZooKeeper%2F</url>
    <content type="text"><![CDATA[ZooKeeper是 Yahoo! 于 2010 年在 USENIX 会议上发表的一篇论文中提出的，被用作分布式应用程序的协调服务(coordination service)。虽然ZooKeeper被认为是 Google Chubby的开源实现，但其设计理念却存在较大差异：ZooKeeper致力于提供一个简单且高性能(high performance)的内核(kernel)以为客户端（应用程序）构建更复杂、更高层(high level)的协调原语(coordination primitives)。换言之，ZooKeeper并不针对特定应用或者具体某一协调服务而设计实现，它只提供构建应用协调原语的内核，而将具体协调原语的构建逻辑放权给客户端，并且，它确保了客户端在不需要更改内核服务的前提下，能够灵活构建出新的、更高级的且更强大的协调原语，比如分布式互斥锁、分布式队列等。ZooKeeper为每个客户端操作提供FIFO顺序保证，并且为所有写操作提供linearlizablity保证。ZooKeeper的实现原理为构建在其之上的服务提供高性能保证。 Zookeeper 为分布式应用提供诸如配置管理(configuration managation)、leader 选举等协调服务，这通过为应用程序提供构建协调原语的 API来实现。并且，与那些提供阻塞原语的服务不同，ZooKeeper实现的 wait-free 数据对象确保其容错和高性能特性，因为若利用阻塞原语来构建协调服务，可能会导致那些慢的(slow)或者有错误的(faulty)的客户端影响正常的客户端的服务性能。此博客阐述个人对ZooKeeper的理解，并从一个ZooKeeper的应用实例开始讨论，分别阐述ZooKeeper两个ordering guarantees、。因为本文并非对原论文的完整翻译，因此你需要提前阅读原论文，确保熟知ZooKeeper数据模型以及客户端API等内容，而且，博客也会省略论文所阐述的利用ZooKeeper来实现部分协调服务部分，具体内容可以参考原论文。 一个应用实例阐述我们知道MapReduce需要知道集群master的ip:port以使得其它节点能够与master建立连接通信，为此，MapReduce可以利用ZooKeeper作为动态配置服务，让master candidate在ZooKeeper上并发注册（创建）各自ephemeral类型的ip:port节点，并让slave监听对应节点的watch event，因此一旦有master candidate注册成功（且只能有一个创建成功），则其它节点将能获取到master的ip:port。 若使用基于raft构建的复制状态机实现，比如在raft集群上构建一个key/value存储系统来存放GFS master的元信息。则整个过程大致如下：首先，master candidate向raft发送Put(&quot;gfs-master&quot;, &quot;ip:port&quot;)命令日志，当raft集群apply此命令日志后，其它节点可通过向raft发送Get(&quot;gfs-master&quot;)命令来获取master的ip:port。但此过程存在几个问题：其一，若多个master candidate同时向raft发送节点地址的注册命令日志，此时将产生race condition，其会导致后发送的命令被应用到状态机，因此master candidate需要进一步判断自己是否成为真正的master（不能仅通过发送了节点地址命令日志来确定）；其二，若master失效，其地址项日志必须要从存储中移除，那么谁来执行此操作？因此，必须对master的元数据信息设置timeout timestamp，并且让master通过定期向raft发送Put(ip:port, timestamp)日志来更新timeout的timestamp，而集群其它节点通过向raft轮询(poll)此timestamp来确保master正常工作，毫无疑问，这将产生大量不必要的poll cost。对比使用ZooKeeper来提供此协调服务（上一段），问题是如何被ZooKeeper高效便捷地解决呢？首先它会确保在多个master candidate同时注册地址信息时，只会有一个操作成功；其次，ZooKeeper的session机制简化了timestamp timeout设置，一旦master宕机，其在ZooKeeper上注册的元信息节点将会自动清除。而且，对应的节点移除消息也会通知到其它节点，避免了slave的大量的轮询消耗。由此可见，使用ZooKeeper来进行集群配置信息的管理，有利于简化服务实现的逻辑。 ZooKeeper 两个 ordering guarantees在讨论ZooKeeper两个基本的ordering guarantees之前，先了解什么是 wait-free，你可以从维基或者 Herlihy的论文 上找到其明确定义： A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. Wait-freedom is the strongest non-blocking guarantee of progress, combining guaranteed system-wide throughput with starvation-freedom. An algorithm is wait-free if every operation has a bound on the number of steps the algorithm will take before the operation completes 而对于ZooKeeper而言，其提供的API被称为是wait-free的，因为ZooKeeper直接响应客户端请求，即此请求的返回并不会受到其它客户端操作的影响（通常是slow或者faulty）。换言之，若此客户端请求为写节点数据操作，只要ZooKeeper收到状态变更，则会立即响应此客户端。如果在这之前某一客户端监听了此节点的数据变更事件，则一旦此节点的数据发生变化，则ZooKeeper会推送变更事件给监听的客户端，然后立即返回给写数据的客户端，并不会等待此监听客户端确认此事件。相比于同步阻塞的调用，wait-free明显提供更好的性能，因为客户端不用同步等待每次调用的返回，且其可以进行异步的批量调用batch call操作，以均摊(amortize)网络传输和IO开销。wait-free的API是ZooKeeper具备高性能的基础，因此也是ZooKeeper的设计核心。 ZooKeeper提供了两个基本的ordering guarantees：Linearizable writes及FIFO client order。Linearizable write表示对ZooKeeper的节点状态更新的请求都是线性化的(serializable)，而FIFO client order则表示对于同一个客户端而言，ZooKeeper会保证其操作的执行顺序与客户端发送此操作的顺序一致。毫无疑问，这是两个很强的保证。 ZooKeeper提供了Linearizable write，那什么是Linearizablility？Herlihy的论文同样给出了其定义，为了方便，你也可以参考这里或者这里。 Linearizability is a correctness condition for concurrent objects that provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object’s operations can be given by pre- and post-conditions. 简单而言，Linearizability是分布式系统领域的概念（区别于数据库领域与事务相关的概念Serializability），一个分布式系统若实现了linearizability，它必须能够保证系统中存在一个时间点，在此时间点之后，整个系统会提交到新的状态，且绝不会返回到旧的状态，此过程是即时的(instantaneous)，一旦这个值被提交，其它所有的进程都会看到，系统的写操作会保证是全局有序(totally ordered)。 而ZooKeeper论文提到其write具备Linearizability，确切而言是A-linearizability(asynchronous linearizability)。简而言之，Linearizability原本（原论文）是针对单个对象，单个操作(single object, single operation)而言的，但ZooKeeper扩大其应用范围，它允许客户端同时执行多个操作（读写），并且保证每个操作同样会遵循Linearizability。 值得注意的是，ZooKeeper对其操作（create,delete等）提供pipelining特性，即ZooKeeper允许客户端批量地执行异步操作（比如发送了setData操作后可以立即调用geData），而不需要等到上一个操作的结果返回。毫无疑问，这降低了操作的延迟(lantency)，增加了客户端服务的吞吐量(throughtout)，也是ZooKeeper高性能的保证。但通常情况下，这会带来一个问题，因为所有操作都是异步的，因此这些操作可能会被重排序(re-order)，这肯定不是客户端希望发生的（比如对于两个写操作而言，re-order后会产生奇怪的行为）。因此，对于特定客户端，ZooKeeper还提供client FIFO order的保证。 ZooKeeper 实现原理同分布式存储系统类似，ZooKeeper也会对数据进行冗余备份。在客户端发送请求之前，它会连接到一个ZooKeeper server，并将后续的请求提交给对应的server，当server收到请求后，有做如下三个保证：其一，若请求所操作的节点被某些客户端注册了监听事件，它会向对应的客户端推送事件通知。其二，若此请求为写操作，则server一次性只会对一个请求做处理（不会同时处理其它的读或者写请求）。其三，写操作最终是交由leader来处理（若接收请求的server并非leader，其主动会对请求进行转发），leader会利用Zab（原子广播协议，ZooKeper atomic broadcast）对此请求进行协调，最终各节点会对请求的执行结果达成一致，并将结果 replica到ensemble servers。ZooKeeper将数据存储到内存中（更快），但为了保证数据存储的可靠性，在将数据写到内存数据库前，也会将数据写到磁盘等外部存储。同时，对操作做好相应的replay log，并且其定期会对数据库进行snapshot。 若请求为读操作，则接收请求的server直接在本地对请求进行处理（因此读操作仅仅是在server的本地内存数据库进行检索处理，这也是ZooKeeper高性能的保证）。正因为如此，同GFS可能向客户端返回过期数据的特点类似，ZooKeeper也有此问题。如果应用程序不希望得到过期数据（即只允许得到最近一次写入的数据），则可以采用sync操作进行读操作前的写操作同步，即如果在读操作之前集群还有pending的写操作，会阻塞直至写操作完成。值得注意的是，每一次的读操作都会携带一个zxid，它表示ZooKeeper最近一次执行事务的编号（关于事务，后面会介绍），因此zxid定义了读操作与写操作之间的偏序关系。同时，当客户端连接到server时，如果此server发现其本地存储的当前zxid小于客户端提供的zxid的大小，其会拒绝客户端的连接请求，直至其将本地数据库同步至全局最新的状态。 在ZooKeeper内部，它会将接收到的写操作转换为事务(transaction)操作。因为ZooKeeper可能需要同时处理若干个操作，因此其会提前计算好操作被提交后数据库所处的状态。这里给出论文中提到的一个事务转换的示例：如果客户端发送一个条件更新的命令setData并附带上目标节点的version number及数据内容，当ZooKeeper server收到请求后，会根据更新后的数据，版本号以及更新的时间戳，为此请求生成一个setDataTXN事务。当事务执行出错时（比如版本号不对应），则会产生一个errorTXN的事务。 值得注意的是，ZooKeeper内部所构建的事务操作是幂等的(idempotent)。这有利于ZooKeeper执行失效恢复过程。具体而言，为了应对节点宕机等故障，ZooKeeper会定期进行snapshot操作，ZooKeeper称其为fuzzy snapshot。但与普通的分布式系统不同的是，它在进行快照时，并不会锁定当前ZooKeeper集群（一旦锁定，便不能处理客户端的写操作，且快照的时间一般也相对较长，因此会降低客户端的服务性能），它会对其树形存储进行深度优先搜索，并将搜索过程中所遍历的每一个节点的元信息及数据写到磁盘。因为ZooKeeper快照期间并没有锁定ZooKeeper的状态，因此在此过程中，若有server在同步写操作，则写操作可能只被replica到部分节点，最终使得snapshot的结果处于不一致的状态。但正是由于ZooKeeper的事务操作是idempontent，因此，在recover过程应用snapshot时，还会重新按顺序提交从快照启动开始到结束所涉及到的事务操作。原论文给出了一个快照恢复过程示例。因此我们会发现，fuzzy snapshot同样是ZooKeeper 高性能的体现。另外，事务幂等的特性也使得ZooKeeper不需要保存请求消息的ID（保存的目的是为了防止对重复执行同一请求消息），因为事务的重复执行并不会导致节点数据的不一致性。由此可见，事务幂等性的大大设计简化了ZooKeeper的请求处理过程及日志恢复的过程。 参考文献： [1] Hunt P, Konar M, Junqueira F P, et al. ZooKeeper: Wait-free Coordination for Internet-scale Systems[C]//USENIX annual technical conference. 2010, 8(9).[2] Herlihy M P, Wing J M. Linearizability: A correctness condition for concurrent objects[J]. ACM Transactions on Programming Languages and Systems (TOPLAS), 1990, 12(3): 463-492.[3] https://medium.com/databasss/on-ways-to-agree-part-2-path-to-atomic-broadcast-662cc86a4e5f[4] https://en.wikipedia.org/wiki/Non-blocking_algorithm[5] http://www.bailis.org/blog/linearizability-versus-serializability/]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式协调服务</tag>
        <tag>分布式锁</tag>
        <tag>原子广播协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 原型实现]]></title>
    <url>%2F2018%2F11%2F16%2FMapReduce-%E5%8E%9F%E5%9E%8B%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[MapReduce 最早是由谷歌于 2004 年在操作系统顶会 OSDI 上发表的一篇面向大规模数据处理的分布式计算框架（并行计算模型）论文中提出。MapReduce使用 Google File System 作为数据存储，支撑起了谷歌全网搜索等大规模数据存储与处理业务。MapReduce 对于大规模数据的高效处理体现在三个方面：其一，大规模数据并行处理，分而治之；其二，MapReduce编程模型；最后，MapReduce运行时环境（失败恢复、任务调度以及负载均衡等）。它简化了并行编程，使得开发人员很容易编写出高效且具备容错能力的并行化程序。 博客基于 MIT 6.824 (2018) 的课程 Lab1。整个实验实现了MapReduce原型，并且对其关键特性进行测试，主要包括MapReduce编程模型，集中在 Map与Reduce两个阶段，以及任务失败处理。在阅读原论文 MapReduce 的基础，Lab1 能够让我们对 MapReduce原理有更为深刻的理解，也能够提高我们实现分布式系统的实践能力，这包括节点通信模型、系统构建框架以及诸如失败恢复机制等。而且，仔细阅读整个 Lab 的代码可以学习到很多原理及设计知识，而不仅仅是完成其 Lab 任务。下文会简单介绍整个 Lab1 框架，然后阐述几个关键点（模块）。 Sequential 及 Distributed 运行模式Lab1 实现了两种不同运行模式的MapReduce原型框架：一种是Sequential运行模式，它顺序编程实现MapReduce过程，也不具备容错功能，因此并非真正意义上的实现。具体地，基于此种运行模式，所有task串行执行且Map与Reduce两个阶段也是串行执行，且未提供任务执行失败的恢复机制。大概地，它首先创建输入文件并读取Map输入，同时创建对应数量的Map task（即循环调用Map函数来处理输入文件），并顺序调度执行，将中间结果写到磁盘上，当所有Map task执行完成后，启动一定数量的Reduce task，并让Reduce task从本地磁盘相应位置读取Map task输出，同样被顺序调度执行，最后，将Reduce task输出写到本地磁盘，最终merge所有输出文件，以合并写到本地输出文件。 另一种是 Distributed运行模式，它更接近真实的MapReduce原型框架实现。客户端会依次启动一个master节点及多个slave节点(go 的goroutine)，并将输入文件信息传给master节点，此后客户端会阻塞等待master返回 。master启动后开始监听slave的连接(one client one goroutine），slave启动后会主动往master节点注册，并等待master分配任务。所有节点通过go rpc实现对等通信。一旦有slave/worker注册成功，master开始实施任务调度，通过rpc将任务信息（任务类型、任务输入文件位置等）发送给worker，而worker在注册成功后，就不断监听master的连接并调用worker的任务执行handler(doTask)， doTask会调用应用程序的Map或Reduce执行MapReduce任务，所有的worker在本节点执行任务的过程同Sequential运行模式下类似，只是各个worker并行执行，互不干扰。值得注意的是，在整个MapReduce Job调度执行过程中，worker允许动态加入，master一旦发现worker注册加入，若此时有未完成的任务等待调度，就会将此任务让新加入的worker调度执行。只有所有的Map task调度完成后，Reduce task才会被调度。当所有Reduce task执行完成后，同样会进行merge的过程，然后从MapReduce框架返回。 Map 及 Reduce 工作流程这里简要阐述 Map &amp; Reduce阶段执行流程。当worker执行map task时，包括以下几个步骤：首先从本地磁盘读取其负责处理的原始输入文件；然后，通过将文件名及文件内容作为参数传递给MapFun来执行用户自定义逻辑；最后，对于每一个Reduce task，通过迭代MapFunc返回的执行结果，并按记录(record)的key进行partition以将分配给对应的Reducer的中间输出结果写到本地磁盘对应文件。 Reduce task的执行过程大致如下：首先读取本Reduce task负责的输入文件，并使用JSON来decode文件内容，并将decode后的kev/value存储到map中，同一个key对应一个value list，然后将整个map的key进行排序，并对每一个key/value list通过调用ReduceFunc来执行用户名自定义逻辑，同时，将其返回的结果，经JSON encode后写入输出文件。这些由Reduce task输出的文件内容，会被merge到最终的输出文件。 再谈失败恢复容错（失败恢复）是MapReduce运行时的一个关键特性。且 Lab1 也模拟实现了任务执行失败后所采取的措施。任务执行失败，典型的包括两种情况：网络分区（网络故障）及节点宕机，且事实上无法很好地区分这两种情形（在两种情形下，master都会发现不能成功ping通 worker）。而实验则是采用阻止worker与master的rpc连接来模拟实现。具体地，所有worker在执行若干个rpc连接请求后（一个rpc连接请相当于一次任务分配），关闭其rpc连接，如此master不能连接worker而导致任务分配执行失败。个人认为，一般情况下会让 master缓存worker的连接handler，并不会在每次发送rpc请求时，都需要执行Dial/DialHttp，若是如此，便不能以原实验的方式来模拟任务执行失败（虽然这可能并不影响）。另外 Lab1 显式禁止了worker同时被分配两个任务的情况，这是显而易见的。 关于失败恢复（节点容错），下面讨论更多细节。容错是MapReduce的一个重要特性，因为节点失效在大数据处理工作中过于频繁，而且当发生节点宕机或者网络不可达时，整个MapReduce job会执行失败，此时MapReduce并不是重启整个job，那样会导致重新提交执行一个庞大的job而耗时（资源）过多，因此它只会重启对应worker所负责执行的task。值得注意的是，正是因为worker并不维护task相关信息，它们只是从磁盘读取输入文件或者将输出写到磁盘，也不存在与其它worker进行通信协调，因此task的执行是幂等的，两次执行会产生相同的执行结果，这也可以说是MapReduce并行执行任务的约束条件之一，也是MapReduce同其它的并行执行框架的不同之处，但无论如何，这样设计使得MapReduce执行任务更为简单。因为Map task会为Reduce task产生输入文件，因此若Reduce task已经从Map task获得了其所需要的所有输入，此时Map的失败，并不会导致其被重新执行。另外关键的是，GFS的atomic rename机制确保即使Map/Reduce task在已经溢写了部分内容到磁盘后失败了，此时重新执行也是安全的，因为GFS会保证直到所有输出写磁盘完成，才使得其输出文件可见，这种情况也会发生在两个Reduce task执行同一个任务，GFS atomic rename机制同样会保证其安全性。那么，若两个Map执行同一个task结果会如何？这种情况发生在，master错误地认为Map task宕机（可能只是发生了网络拥塞或者磁IO过慢，事实上，MapReduce的stragger worker正描述的是磁盘IO过慢的情况），此时即便两个Map task都执行成功（它们不会输出到相同的中间文件，因此不会有写冲突），MapReduce运行时也保证只告诉Reduce task从其中之一获取其输入。最后，注意MapReduce的失败恢复机制所针对的错误是fail-stop故障类型，即要么正常运行，要么宕机，不会产生不正确的输出。 参考代码在这里。 参考文献 [1] Dean J, Ghemawat S. MapReduce: simplified data processing on large clusters[J]. Communications of the ACM, 2008, 51(1): 107-113. [2].MIT 6.824]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式计算框架</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式计算框架</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 The Google File System]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%90%86%E8%A7%A3-The-Google-File-System%2F</url>
    <content type="text"><![CDATA[分布式文件系统是构建整个分布式系统的基石，为分布式计算提供底层数据存储。谷歌早在 2013 年就发表了论文 The Google File System，它在谷歌内部是配合其分布式计算框架MapReduce使用，共同为谷歌搜索等业务提供技术栈支撑。虽然数据量激增以及技术革新使得GFS不断演进，但理解其最初的设计理念、运行原理以及关键实现技术同样让人受益匪浅，并指导着我们实际的学习和工程实践。这篇博文阐述个人对原论文的一些理解与心得，并不是对原论文的完整翻译，因此你需要提前阅读论文。 设计动机与目标设计一个通用的分布式文件系统是不现实的，它不仅在实现上异常困难（因为不得不考虑所有应用场景），而且实际使用也难以满足要求（往往存在显而易见的性能或容错瓶颈）。GFS 设计初衷是利用数以千计的廉价机器为MapReduce提供底层可靠且高性能的分布式数据存储，以应对海量离线数据存储与处理的应用场景，比如存储应用程序持续产生的日志流以提供离线日志分析。由此，其设计目标为容错可靠(fault tolerance)、高性能读写(high-performance read&amp;write)以及节约网络带宽(save bandwidth)。 一致性(consistency)是分布式系统不可回避的问题。对于分布式文件系统而言，为了提供容错，必须维持数据副本(replica），那如何保证各副本间一致显得至关重要，特别是在应用并发访问场合。一致性是个极其宽泛的术语，你可以实现数据的强一致性(strong consistency)以保证用户始终读到的是最新的数据，这对于用户（客户端）而言是个极佳选择，但它提高了系统的实现难度，因为你必须设计复杂的一致性协议（如Paxos或Raft）来实现强一致性，它也会损害系统性能，典型的它需要机器之间通信以对副本状态达成一致。而弱一致性(weak consistency)则几乎相反。因此，必须根据特定的应用场景，在保证系统逻辑正确的提前下，放宽一致性要求，设计具备良好性能且能提供足够的一致性(sufficient consistency)的系统。对于GFS而言，它针对MapReduce应用程序进行了特定优化，比如，对大文件高性能读取、允许出现文件空洞(hole)、数据记录重复(record duplicate)以及偶尔读取不一致(inconsistent reads)。具体在数据读写方面，其侧重于大规模一次性写入和追加写入，而并非覆盖写和随机写；读取同样倾向于顺序读取，并不关心随机读取。 Trade-off 理念哲学GFS在设计上存在大量的trade-off。正如前文所述，你不能企图设计出一个完美的系统，而只能针对具体应用场景作出各方面的权衡考量，以达到工程最佳实践目的。 chunk大小设计。GFS针对大文件存储（数百上千兆）设计，因此若存储大量小文件则不能体现其性能。其默认块大小为 64MB。选择大文件作为存储目标原因如下：首先它减少了client与master的交互次数（即使client并不需要整个数据块，但实际上往往存在“就近原则”）；另外，这也直接减少了网络带宽；最后，它减少了存储在master内存中的元数据(metadata)的大小。但凡事总利弊相随。较大的块大小设定使得小文件也不得不占用整个块，浪费空间。 集群元数据‘存储。在master的内存中存放着三种类型的元数据：文件和chunk的名称空间(namespace)、文件到chunk的映射信息以及chunk副本的位置信息。且前两种元数据会定期通过operation log持久化到磁盘以及副本冗余。为什么将这些元信息存储到内存？一方面，缓存在内存无疑会提高性能，另外它也不会造成内存吃紧，因为每个64MB 的chunk只会占用到 64B 的内存空间（粗略估算普通 2G 内存的机器可以容纳 2PB 数据），而且为机器增加内存的代价也很小。那为什么chunk位置信息没有持久化？首先master在启动的时候可以通过heartbeat从各chunk server获取。另一方面，chunk的位置信息有时会变动频繁，比如进行chunk garbage collection、chunk re-replication以及chunk migration，因此，若master也定期持久化chunk位置信息，则master可能会成为集群性能bottleneck。从另一个角度来看，chunck是由chunk server保存，而且随时可能发生disk failure而导致chunk暂时不可被访问，因此其位置信息也应该由chunk server负责提供。 chunk副本（默认3个）存放策略。chunk副本选择目标机器的原则包括两个方面：一是最大化数据可靠性(reliability)及可用性(availability)，这就要求不能把所有的副本存放在一台机器上，如果此机器的发生disk failure，则数据的所有副本全部不可用。放在同一个机架也类似，因为机架之间的交换机或其它网络设计也可能出现故障。另外一个原则是，最大化网络带宽，如果两个副本的位置相隔太远，跨机架甚至跨数据中心，那么副本的写复制代价是巨大的。因此一般的存放位置包括本机器、同一机架不同机器以及不同机架机器。 垃圾回收。当一个文件被删除，GFS不会真正回收对应的chunk，而只是在log operation记录删除日志后，将对应的文件名设置为隐藏。在一定期限内（默认3天），用户可以执行撤销删除操作。否则，master会通过其后台进程定期扫描其文件系统，回收那些隐藏的文件，并且对应的元数据信息也会从内存中擦除。另外，master的后台进程同时还会扫描孤儿块(orphaned chunk)，即那些不能链接到任何文件的chunk，并将这些chunk的元信息删除，这样在后续的heartbeat中让chunk server将对应的chunk删除。这种垃圾回收机制的优点如下：其一，很明显允许用户作出撤销删除操作。其二，统一管理的垃圾回收机制对于故障频繁的分布式系统而言是便捷且可靠的（系统中很容易出现孤儿块）；最后，也有利于提升系统性能。垃圾回收发生在后台进程定期扫描活动中，此时masetr相对空闲，它不会一次性将大量文件从系统移除，从而导致 IO 瓶颈，换言之，其chunk回收成本被均摊(amortized）。但其同样有缺点：如果系统中一段时间内频繁出现文件删除与创建操作时，可能使得系统的存储空间紧张（原论文中也提供了解决方案）。 一致性模型 和 原子 Record Append前文提到GFS并没有采用复杂的一致性协议来保证副本数据的一致性，而是通过定义了三种不同的文件状态，并保证在这三种文件状态下，能够使得客户端看到一致的副本。三种状态描述如下：GFS将文件处于consistent状态定义为：当chunk被并发执行了操作后，不同的客户端看到的并发执行后的副本内容是一致的。而defined状态被定义为：在文件处于consistent状态的基础上，还要保证所有客户端能够看到在此期间对文件执行的所有并发操作，换言之，当文件操作并发执行时，如果它们是全局有序执行的（执行过程中没有被打断），则由此产生的文件状态为defined（当然也是consistent）。换言之，如果某一操作在执行过程中被打断，但所有的并发操作仍然成功执行，只是对文件并发操作的结果不能反映出任一并发操作，因为此时文件的内容包含的是各个并发操作的结果的混合交叉，但无论如何，所有客户端看到的副本的内容还是一致的，在这种情况下就被称为consistent。自然而然，如果并发操作文件失败，此时各客户端看到的文件内容不一致，则称文件处于undefined状态，当然也处于inconsistent状态。 我们先区分几种不同的文件写类型：write指的是由应用程序在写入文件时指定写入的offset；而append同样也是由应用程序来指定写入文件时的offeset，只是此时的offset默认为文件末尾；而record append则指的是应用程序在写入文件时，只提供文件内容，而写入的offset则由GFS来指定，并在写成功后，返回给应用程序，而record append操作正是GFS提供一致性模型的关键，因为它能够保证所有的record append都是原子的(atomic)，并且是at least once atomically。这一点并非我们想像的简单，其所谓的at least once atomic，并不表示采用了atomic record append后，即使在客户端并发操作的情况，也能保证所有的副本完全相同(bytewise idetical)，它只保证数据是以原子的形式写入的，即一次完整的从start chunk offset到end chunk offset的写入，中间不会被其它操作打断。且所有副本被数据写入的chunk offset是相同的。但存在这种情况，GFS对某一副本的执行结果可能会出现record duplicate或者inset padding，这两种情况的写入所占居的文件区域被称为是inconsistent。而最后为了保证应用程序能够从所有副本看到一致的状态，需要由应用程序协同处理。 如果文件的并发操作成功，那么根据其定义的一致性模型，文件结果状态为defined。这通过两点来保证：其一，对文件的副本应用相同的客户端操作顺序。其二，使用chunk version number来检测过期(stale)副本。 record append操作流程如下：客户端首先去请求master以获取chunk位置信息，之后当客户端完成将数据 push 到所有replica的最后一个chunk后，它会发送请求给primiary chuck server准备执行record append。primary首先为每一个客户端操作分配sequence number，然后立即检查此次的record append操作是否会使得chunk大小超过chunk预设定的值（64MB），若超过了则必须先执行insert padding，并将此操作命令同步给所有副本chunk server，然后回复客户端重新请求一个chunk并重试record append。如果未超过chunk阈值，primary会选择一个offset，然后先在本地执行record append操作，然后同样将命令发送给所有副本chunk server，最后回复写入成功给客户端。如果副本chunk server在执行record append的过程中宕机了，则primary会回复客户端此次操作失败，要求进行重试。客户端会请求master，然后重复上述流程。此时，毫无疑问会造成副本节点在相同的chunk offset存储不同的数据，因为有些副本chunk server可能上一次已经执行成功写入了所有数据(duplicate record)，或者写了部分数据(record segment)，因此，必须先进行inset padding，使得各副本能够有一个相同且可用的offset，然后才执行record append。GFS将这种包含paddings &amp; record segments的操作结果交由应用程序来处理。 应用程序的writer会为每个合法的record在其起始位置附加此record的checksum或者一个predictable magic number以检验其合法性，因此能检测出paddings &amp; record segments。如果应用程序不支持record duplicate（比如非幂等idempotent操作），则它会为每一个record赋予一个unique ID，一旦发现两个record具有相同的ID它便认为出现了duplicate record。由GFS为应用程序提供处理这些异常情况的库。 除此之外，GFS对namespace的操作也是原子的（具体通过文件与目录锁实现）。 我们再来理解为什么GFS的record append提供的是at least once atomically语义。这种一致性语义模型较为简单（简单意味着正确性易保证，且有利于工程实践落地，还能在一定程度上提升系统性能），因为如果客户端写入record失败，它只需要重试此过程直至收到操作成功的回复，而server也只需要正常对等待每一个请求，不用额外记录请求执行状态（但不表示不用执行额外的检查）。除此之外，若采用Exactly-once语义模型，那将使整个实现变得复杂：primary需要对请求执行的状态进行保存以实现duplicate detection，关键是这些状态信息必须进行冗余备份，以防primary宕机。事实上，Exactly-once的语义模型几乎不可能得到保证。另外，如果采用at most once语义模型，则因为primary可能收到相同的请求，因此它必须执行请求duplicate detection，而且还需缓存请求执行结果（而且需要处理缓存失效问题），一旦检测到重复的请求，对客户端直接回复上一次的请求执行结果。最后，数据库会采用Zero or once的事务语义(transactional semantics)模型，但严格的事务语义模型在分布式场景会严重影响系统性能。 延迟 Copy On Write快照(snapshot)是存储系统常见的功能。对于分布式系统而言，一个关键挑战是如何尽可能地降低snapshot对成百上千的客户端并发访的性能影响。GFS同样采用的是copy on write技术。事实上，它延迟了snapshot的真正执行时间点，因为在分布式系统中，副本是必须的，大多数情况下，快照涉及的副本可能不会被修改，这样可以不用对那些副本进行 copy，以最大程度提升系统性能。换言之，只有收到客户端对快照的副本执行mutations才对副本进行 copy，然后，将客户端的mutations应用到新的副本。具体的操作流程如下：当master收到客户端的snapshot指令时，首先会从primary节点revoke相应chunk的lease（或者等待lease expire），以确保客户端后续对涉及snapshot的chunk的mutations必须先与master进行交互，并对这些操作执行log operation，然后会对涉及到的chunk的metadata执行duplicate操作，并且会对chunk的reference count进行累加（换言之，那些chunk reference count大于1的chunk即表示执行了snapshot）。如此一来，当客户端发现对已经快照的chunk的操作请求时，master发现请求的chunk的reference count大于1。因此，它会先defer客户端的操作请求，然后选择对应chunk的handler并将其发送给对应的chunk server，让chunk server真正执行copy操作，最后将chunk handler等信息返回给客户端。这种delay snapshot措施能够改善系统的性能。 最后，值得注意的是，虽然客户端并不缓存实际的数据文件（为什么？），但它缓存了chunk位置信息，因此若对应的chunk server因宕机而miss了部分chunk mutations，那客户端是有可能从这些stale的replica中读取到premature数据，这种读取数据不一致的时间取决于chunk locations的过期时间以及对应的文件下一次被open的时间（因为一旦触发这两个操作之一，客户端的cache信息会被purge）。 参考文献： [1] Ghemawat S, Gobioff H, Leung S T. The Google file system[M]. ACM, 2003.[2].MIT 6.824 Lecture]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式文件系统</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式文件系统</tag>
        <tag>GFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式互斥算法解析与实现]]></title>
    <url>%2F2018%2F11%2F09%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[资源共享非常普遍，在单机系统中，进程间对共享资源的互斥访问可以通过互斥锁、信号量以及进程间通信等机制来实现。而在分布式系统中，也不可避免多个节点共享某一资源或同时执行某一函数，比如全局配置文件，因此分布式互斥算法必须保证任何时刻只允许一个进程访问资源或执行临界区(critical section)代码，即互斥算法的安全性，有些场景也有公平性要求。另外，好的互斥算法应该能尽可能降低消息带宽(message overhead)，减少进程（节点）等待时间，即时延(latency)，无系统瓶颈(bottleneck)，也能容忍消息乱序。 分布式互斥算法分为集中式算法(centralized algorithm)和分布式算法(distributed algorithm)，而分布式算法又包括了基于令牌的算法(token based algorithm)以及基于请求的算法(permission based algorithm)。无论基于何种原理实现，一般而言，理想的分布式互斥算法需要保证以下目标： 安全性，即任何时刻只能有一个进程访问共享资源，即持有互斥锁。 公平性，有些场景需要尽量保证访问共享资源的公平性，这表明：系统不能出现死锁，任何进程持有锁的时间是有限的，任何等待的进程最终都能获取锁，以及等待获取锁的进程的等待时间是有限的。 低带宽，即尽量减少消息传输的数目。 低延迟，即进程进入临界区之前的等待的时间。 动态性，即允许进程在任何时刻加入到访问共享资源的进程集合中，或者从其中退出。 容忍进程失败，即允许访问共享资源的进程集合中的进程因失败而退出，而保证整个系统不受影响。 容忍消息丢失，即在消息不能按时到达、乱序甚至丢失的情况下，整个系统依然正常工作。 在本文我们讨论前四个要求，假设进程数目是确定的，没有进程会失败，消息也不会丢失。下面我们通过简要阐述算法原理以及实现关键点来依次介绍Centralized Mutual Server算法、Ricart Agrawala算法、Lamport Distributed Mutual Exclusion算法以及Token Ring算法。 Centralized Mutual Server顾名思义，Centralized Muutal Server为集中式的互斥算法。整个系统内部包括两种消息：请求(reqeust)消息、授权(grant)消息以及释放(release)消息。核心数据结构为一个请求消息队列。算法核心为：它选取一个进程(centralized server)作为协调者，负责对名进程的请求进行即时或推迟(defer)授权。它内部维护一个互斥锁锁请求队列，当收到请求消息时，如果队列为空，则直接授权，否则将其加入到队列中。当收到释放消息时，如果列队不为空，则从队列中取出一个请求并授权响应。算法公平性依赖于队列实现，如使用FIFO则能够保证各个进程的锁请求消息能够被公平地授权。消息带宽为3(1 request, 1 grant, 1 release)，即在某一进程从准备进入临界区到退出临界区所传输消息的数量。很明显，集中式互斥算法的缺点是协调者的瓶颈。 集中式互斥算法最容易实现。在网络通信层，可以采用基于TCP的client-server通信模型。关于协调者的实现，你可能需要关注当前是否已经授权了锁请求。同时，如果有必要，注意单进程内部锁的使用。 Ricart AgrawalaRicart &amp; Agrawala算法是在1981年被提出的一个基于请求的分布式互斥算法。它基于lamport clock，即依赖于全局有序的逻辑时钟。整个系统内部包括两种消息：请求(reqeust,i,ts)消息与回复(reply,j)消息。核心数据结构包括缓存其它进程回复消息的队列(replyQueue)以及缓存推迟回复进程请求消息队列(deferQueue)。算法核心为：当进程i准备进入临界区时，必须发送一个带（逻辑）时间戳的请求消息给其它所有进程，当其收到了其它所有进程的对此请求的回复（响应）时，则进入临界区。但如果某一进程j在收到进程i的请求之前，发出了一个更早的请求消息，则它会将此进程(i)的请求消息放入到延迟队列(deferQueue)，并且先执行完临界区的代码，当准备退出临界区时，才发送请求响应给进程(i)。算法的公平性容易保证。而消息带宽为{request: n-1, reply: n-1} =&gt; 2(n-1)，其中n为进程数。 Ricart Agrawala算法的相比集中式算法在实现上更为复杂。同样在通信层，则不能构建one server, muliti-client模型，而采用peer to peer模型，因为所有进程都是对等的，即同时充当server与client，而且作为一种简化实现，所有进程在启动后，应该互相建立连接。除此之外，你需要实现（模拟）lamport clock 算法，否则互斥算法的正确性不能得到保证，注意对于某些消息（如reply）的发送事件，虽然可以更新消息时间戳，但其实不影响算法正确性。 Lamport Distributed Mutual ExclusionLmpoart Distributed Mutual Exclusion算法于1978年由 Lamport 在关于lamport clock理论论文中提出，其作为lamport clock的实际应用，因此，显然其依赖于lamport clock。事实上，此算法不仅可以作为分布式互斥算法，其内部的请求优先级队列也能作为分布式节点副本一致性的实现参考模型。但原论文提出的互斥算法基于消息按顺序到达的假设，解释如下： 比如，进程i在时间片1发送锁请求a，但因为网络原因被极端延时了。而且在其它进程收到进程i发送的请求消息a之前，进程j在时间片5发送了请求b，而请求b恰好被顺利传输，很快被其他进程接收，并且其他进程（包括进程i）立刻发送了对请求消息b的回复消息，同时回复消息也立刻被进程j接收，但此时进程j仍未收到进程i的请求消息a，所以进程j以为自己成功获取到锁（收到了其它所有进程对请求消息b的回复）。而事实上，进程i的请求消息a要比进程j的请求消息b更早发送，因此应该是进程i先获取锁。其根本原因在于，进程i在收到其他节点请求消息（进程j的请求消息b）时，没有进行额外检查，理论上它需要判定自己是否在更早前发出过请求消息，而不只是直接对请求消息回复，即使最后其在请求消息队列里移除的消息是它自己的请求消息（因为自己是请求消息是最早的）。但这造成了整个系统的不正确性。 因此，改变Lmpoart Distributed Mutual Exclusion算法在接收请求消息后发送回复消息的条件，消除了消息按序到达的假设，但同时也使得变更后的算法更为复杂。 系统内部包括三种消息：请求(reqeust,i,ts)消息、回复(reply,j)消息以及释放(release)消息。核心数据结构包括缓存其它进程回复消息的队列(replyQueue)、缓存推迟回复进程请求消息队列(deferQueue)以及一个以时间戳为依据的请求消息优先级队列(requestPriorityQueue)。算法变更的核心为：进程i在收到进程j的请求消息(request, j, t)时，（条件1）先判断自己是否发送过更早的请求消息，（条件2）并且未收到进程j针对此请求消息的回复消息。如果二者之中任一个未被满足，则对进程i的请求消息发送回复，否则将其加入到deferQueue。原因如下：条件1是明显的；关于条件2，如果进程j已经收到了进程i的消息回复，说明进程i先前发出的请求消息肯定已经被进程j接收（换言之，进程i若发送过请求消息，则此请求消息必定已经缓存到了进程j的requestPriorityQueue），因此消除了消息延迟（乱序）的影响。另一方面，当进程i收到请求回复消息时，它会先将其加入到replyQueue，并判断发送此回复消息的进程是否被加入到了其deferQueue中，如果已经加入到了，则将其移除，然后对此进程发送回复消息（因为进程i确认它已经收到被移除进程的回复消息）。其它的算法逻辑同论文中描述一致。事实上，消除消息按序到达的关键为deferQueue。算法的公平性容易保证。而消息带宽为{request: n-1, reply: n-1, release: n-1} =&gt; 3(n-1)，其中n为进程数。 Lmpoart Distributed Mutual Exclusion算法的相比Ricart Agrawala算法在实现上更为复杂。通信层采用peer to peer模型。 Token RingToken Ring是基于令牌的互斥算法。是一种简单的互斥算法模型，局限性也较大。系统内部只有一种消息：传递 token 的(OK)消息。算法核心为：将所有进程在逻辑上组成一个环，并将 token 在环上依次传递，获取到 token 的进程则具备进行临界区的条件，未收到 token 的进程则必须等待。在进程启动时，必须先将 token 传递给某一进程，若此接收进程需要锁，则进入临界区，执行完临界区代码后，再将 token 传递给相邻的下一个进程。否则直接将 token 传递给相邻下一个进程。算法的公平性同样易保证。消息带宽为n-1，其中n为进程数。 Token Ring算法较易实现，同样采用peer to peer通信模型。注意进程启动时，初始的 token 持有者。 关于测试在实现上述四种算法时(go语言)，采用TCP协议（可靠的）。测试的流程包含两个独立的阶段：Phase a. 每个进程独立的重复以下操作若干次。 执行本地操作。采用 sleep [100, 300]ms 来模拟。 开始进入临界区(critical section)。执行获取互斥锁逻辑。 执行临界区代码。对一个共享变量进行累加，在 [100, 200]ms超时时间内，每隔100ms，对共享变量随机增加 [1,10]。将累加过程写入文件，同时将累加的中间值记录到全局数组。 退出临界区。执行释放互斥锁逻辑。 Phase b. 每个进程独立的重复以下操作若干次。 进程号为偶数的进程 sleep [100, 300]ms，然后重复 Phase a 操作流程。进程号奇数的进程直接重复 Phase b 流程。 对上述四个分布式互斥算法的测试结果的验证侧重于两个方面： 算法正确性。通过检查 Phase a&amp;b 中全局数组的记录情况来确保共享资源的互斥访问。另外，核查 Phase a&amp;b 中进程访问共享资源的访问日志文件。 带宽与延时。统计每个进程的消息读写数目，及获取互斥锁的延时，并计算平均延时。 参考代码在这里。 参考文献： [1] Ricart G, Agrawala A K. An Algorithm for Mutual Exclusion in Computer Networks[R]. MARYLAND UNIV COLLEGE PARK DEPT OF COMPUTER SCIENCE, 1980.[2] Lamport L. Time, clocks, and the ordering of events in a distributed system[J]. Communications of the ACM, 1978, 21(7): 558-565.[3].CMU Distributed System Lecture.]]></content>
      <categories>
        <category>分布式系统</category>
        <category>互斥算法</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>逻辑时钟</tag>
        <tag>lamport clock</tag>
        <tag>互斥算法</tag>
        <tag>资源共享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统时间、时钟与事件顺序]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%97%B6%E9%97%B4%E3%80%81%E6%97%B6%E9%92%9F%E4%B8%8E%E4%BA%8B%E4%BB%B6%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[如何确定分布式系统各节点（进程）中事件发生的先后顺序至关重要。时钟不一致会导致系统发生不可预料的逻辑错误。然而绝大部分情况下，不能依赖于物理时钟，因为不同的系统的物理时钟总会存在不同程度的时钟漂移（多处理器机器中也类似），即便各节点定期通过网络从时钟源进行时钟同步，也无法确保各节点时钟完全一致。因此，早在1978年，Leslie Lamport 便提出了逻辑时钟的概念，并描述了如何利用逻辑时钟来定义分布式系统中事件的发生顺序。它大致基于事件发生的因果关系，并保证能够正确排列系统中具有因果关系的事件，这使得分布式系统在逻辑上不会将具有因果关系事件的发生顺序倒置。 时钟同步事实上，计算机的时钟会以不同速率来计时，普通的石英钟漂移(skew drift)1秒所需的时间大概为11-12天。因此如果使用物理时钟physical clock所定义的时间戳来确定系统中事件发生顺序，需要对物理时钟进行定期同步(clock synchronization)。在理想网络环境下，通过网络将带有时间戳的消息在时钟源（UTC,Coordinated Universal Time）与本地机器之间传输，能够保证本地时间与时钟源基本一致。但事实上，网络是异步的且有延时，因此无法保证不同节点之间的时钟完全同步。尽管如此，我们可以通过算法来尽可能提高时钟同步精度。著名的时钟同步算法如Cristian&#39;s Time Sync和Berkeley algorithm。 Lamport Clock相比于通过同步物理时钟的方式来协调各节点的时间，在分布式系统中，更为普遍且合理的方式是使用逻辑时钟(logical clock)。Lamport 提出的逻辑时钟舍弃了物理时钟固有的无限粒度的性质，它基于事件发生的因果关系(causality)。换言之，所有的事件通过happened before来关联，以-&gt;表示。对于事件a与b，a-&gt;b表示a happened before b，它是一种偏序关系(partial order)，分布式系统中所阐述的事件发生的先后顺序一般为偏序。Lamport 在分布式系统内定义了三种类型的事件，包括进程（节点）内事件、进程发送消息事件以及进程接收消息事件。a happened before b由以下三个条件中任一一个触发： 若a与b表示同一进程内的事件，并且a发生在b之前，则有a-&gt;b。 若a代表某一进程发送消息的事件，b代表另一进程接收此消息的事件，则有a-&gt;b。 happened before关系满足传递性。 如下图（水平方向表示物理时钟增加方向，垂直方向表示不同进程），由规则(1): a-&gt;b及c-&gt;d; 由规则(2): b-&gt;c和d-&gt;f; 由规则(3): b-&gt;f。但并非所有的事件都能通过-&gt;关联，比如a与e为不同进程不同消息链上的事件，则只能被定义为并发的两个事件，记作a||e。事实上，事件a与e没有因果关系，因此，从系统正确性的角度而言，它们之间真正的发生顺序不会影响到系统的正确性，所以我们不需要关注它们发生的先后顺序。 如果将系统中所有发生的事件e标记一个单调递增的时间戳(L(e))（与物理时钟没有关系），也称为lamport timestamp/clock，每一个进程都会维护自己的逻辑时钟，时间戳标记原理如下： 每个事件对应一个初始的时间戳，初始值为0。 如果发生的事件为进程内事件，则时间戳加1。 如果事件为发送事件，则将时间戳加1，并在消息中带上该时间戳。 如果事件为接收事件，则其时间戳为max(进程时间戳，消息中附带的时间戳)+1。 如下图，p1、p2以及p3都有自己的初始的逻辑时钟0；进程的全局（当前）逻辑时钟即为当某一事件发生之后的逻辑时钟值，如事件a与b的逻辑时钟值分别为0+1=1和1+1=2。而发p1发送消息m1给p2后，c的逻辑时钟值为max(0, 2)+1=3。 在lamport clock表示法中，对于事件e1与e2，e1-&gt;e2能推断出L(e1)&lt;L(e2)，但反之不成立，即L(e1)&lt;L(e2)不能推断出e1-&gt;e2，如在图(2)中，L(b)&gt;L(e)，但实际上有b||e。另外，并发事件也是类似的，即若L(e1)=L(e2)可以推断出e1||e2，但反之不成立。对于lamport clock，并发事件没有可比性，正如上文所述，并发事件发生的先后顺序并不影响系统逻辑正确性。 在lamport clock表示法中，无法确定没有因果关系的事件的先后顺序，而大多数分布式系统确实需要对所有的事件进行全局排序(total order)，而不仅仅得到影响系统正确性的事件之间的偏序关系(partial order)。换言之，为了得到一个全局的事件发生顺序，必须对并发事件进行先后发生顺序的判定。因为并发事件真正发生的先后顺序不影响系统的准确性，因此可以为它们统一制定一个任意顺序规则（事实上，lamport clock就是这么考虑的）。比如同其它因果关系事件类似，以逻辑时钟(L(e))的大小来判定，逻辑时钟小的发生在前，反之则发生在后。而对于逻辑时钟相同的并发事件，在lamport clock算法当中，给出的解释是根据进程号(PID)的大小来确定，进程编号更小的发生在前。其实，Lamport 在论文中提到过，也可以采用其它方式来确定并发事件的先后顺序，这似乎没有理论依据，但是正如前面所述，通过引入lamport clock，可以在逻辑上保证系统的正确性，我们不关心那些不影响系统正确运行的事件之间的顺序。但以进程编号作为依据，似乎影响到了系统的公平性，比如当两个进程竞争同一物理资源，物理时间上先发出请求的进程不一定能先锁定资源，但这并不会造成系统逻辑错误。在lamport clock原论文中，给出的实例便是分布式系统资源竞争或者互斥占用，大家可以参考原论文。 Vector Clock前文提到lamport clock存在一个缺点，即对于事件e1与e2，L(e1)&lt;L(e2)并不能推导出e1 happened before e2，换言之，其只能确定单方的因果关系关联。vector clock是在lamport clock上演进的一种逻辑时钟表示法，它完善了lamport clock这一缺陷，能提供完整的因果关系关联。在vector clock表示法中，每个进程维护的不仅仅是本进程的时间戳，而是通过一个向量(vector)来记录所有进程的lamport clock以此作为进程的逻辑时钟，即进程事件的逻辑时钟被表示为：v(e)[c1, c2..., cn]，其中ci为进程i中先于事件e发生的事件。vector clock的逻辑时钟标记原理同lamport clock原理类似。 如下图，在进程p1中，事件a的逻辑时钟为(1,0,0)，发送消息m的事件b的逻辑时钟为(2,0,0)。在进程p2中，其接收消息m1的事件c的逻辑时钟为max((0,0,0),(2,0,0))+1=(2,1,0)。此时，对于事件e1与e2，由e1 happened before e2推断出v(e1)&lt;v(e2)，反之亦然。在vector clock表示法中，事件c与e是并行事件，记作c&lt;-&gt;e，因为我们不能推导出v(c)&lt;=v(e)，也不能推导出V(e)&lt;=v(c)。注意，此时逻辑时钟值的比较(&lt;|&gt;|&lt;=|&gt;=)是对向量的分量逐一比较。 参考资料：（插图出自 CMU Lecture）[1] https://en.wikipedia.org/wiki/Cristian%27s_algorithm[2] https://en.wikipedia.org/wiki/Berkeley_algorithm[3] Lamport L. Time, clocks, and the ordering of events in a distributed system[J]. Communications of the ACM, 1978, 21(7): 558-565.[4].CMU 15-440 Distributed System Lecutre 9]]></content>
      <categories>
        <category>分布式系统</category>
        <category>逻辑时钟</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>时钟同步</tag>
        <tag>逻辑时钟</tag>
        <tag>物理时钟</tag>
        <tag>lamport clock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[groupcache 设计原理剖析]]></title>
    <url>%2F2018%2F10%2F29%2Fgroupcache-%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[groupcache是一个用go实现的分布式k/v缓存及缓存填充库，它的作者也是memcached的作者，它已在Google多个生产环境中使用。它非常小巧精致，比较适用于分布式缓存的学习。它本身只是一个代码包（大约2000行代码，不需要配置服务器，在不同的请求处理场合，它可以充当客户端或者服务器的角色。它支持一致性哈希，即通过一致性哈希来对查询请求进行路由。对于缓存的具体策略，groupcache采用的是LRU，使用了一个List和一个Map来实现，非常简单。下面先简述本地缓存的基本模型和常见问题，然后剖析groupcache的设计原理。 单机缓存或者本地缓存是简单的，通过在内存中维护一个cache，当收到查询时，先查询cache是否已缓存查询结果，如果命中则直接返回，否则必须到存储系统执行查询，然后将结果先缓存到cache，然后返回结果。当然，这是本地缓存的基本模型，一般而言，缓存系统都面临着诸如缓存穿透、缓存雪崩及缓存击穿等问题。 缓存穿透指的是查询一定不存在的数据，此时从数据源查询不到结果，因此也无法对结果进行缓存，这直接导致此类型的查询请求每次都会落到数据源层，不仅使得缓存失效，当请求数量过多时也会浪费资源。 缓存雪崩指的是大量的缓存的过期时间被设置为相同或近似，使得缓存失效时，所有的查询请求全部落地到数据源层，同样，此时数据源层存在服务不可用的可能性。 缓存击穿则指的是对于那些热点数据，在缓存失效时，高并发的查询请求也会导致后端数据源层崩溃。 对于groupcache的设计，文章从一致性哈希、缓存命名空间、热数据扩散以及缓存过滤几个方面进行阐述。 一致性哈希一致性哈希最初是在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中提出，目标是致力于解决因特网中的热点(Hot spot)问题，并真正应用于p2p环境，它弥补简单的哈希算法的不足。一般可以从四个方面来衡量哈希算法的适用性。 平衡性。平衡性即哈希的结果能够尽可能的分散到所有节点或缓冲，以保证缓冲空间被最大程度使用。 单调性。单调性即如果当前缓存系统已经存在被映射的缓冲内容，当有新的节点加入到系统时，哈希算法应该能够尽可能保证原有已分配的的缓冲内容只能被映射到原有的对应节点或者新的节点，而不能被映射到旧的节点集合的其它节点。 分散性。分布式环境中，不同终端所见的节点范围有可能不同（因为可能只能看见部分节点），这会导致不同终端的哈希结果不一致，最终，相同的内容被映射到了不同的节点。而分散性则专门用于描述此种情况发生的严重程度。好的哈希算法应该尽量避免发生这种情况，即降低分散性。 负载。本质上与分散性阐述的是同一问题。但它从节点出发，即某一特定的节点应该尽可能被相同的缓冲内容所映射到，换言之，避免（不同终端）将相同的内容映射到不同的节点。 所谓一致性哈希，简而言之，即将节点与缓冲内容分别映射到一个巨大的环形空间中，最终内容的缓存节点为在顺时针方向上最靠近它的节点。可以发现，系统中节点的添加与删除，一致性哈希算法仍能基本满足以上四个特性。另外一个关键问题是，当集群中节点数量较少时，节点分布不均匀（即节点所负责的内容范围相差较大）会直接导致内容（数据）倾斜，因此一般会引入虚拟节点，即将节点映射为虚拟节点。如此，整个缓存映射过程便拆分为两个阶段：对于特定缓冲内容，先找到其映射的虚拟节点，然后再由虚拟节点映射到物理节点。 一致性哈希在分布式缓存中充当查询路由角色，因为不同节点负责特定的key集合。因此，如果此时当查询没能在本节点缓存中命中时，则需通过一致性哈希路由特定节点(peer)，然后借助http发送数据查询请求，请求的协议格式为: GET http://peer/key。因此，所有节点必须监听其它节点的数据查询请求，同时具备相应的请求处理模块。 缓存命名空间即便是在单个节点上，也可以创建若干个不同名称的缓存命名空间，以使得不同命名空间的缓存相互独立。如此，可以在原本针对key进行分片的基础上，丰富缓存功能。因此，节点间的数据查询请求协议格式变更为：GET http://peer/groupname/key。 热点数据扩散分布式缓存系统，不同的节点会负责特定的key集合的查询请求。但因为并非所有的key的访问量是均匀的，因此，存在这种情况：某些key属于热点数据而被大量访问，这可能导致包含该key的节点无法及时处理甚至瘫痪。考虑到这一点，groupcache增加了热点数据自动扩展的功能。即针对每一个节点，除了会缓存本节点存在且大量被访问的key之外（缓存这些key的对象被称之为maincache），也会缓存那些不属于本节点，但同样被大量访问（发生大量地miss cache）的key，而缓存这些key的对象被称这为hotcache，如此便能缓解热点数据的查询请求集中某一个节点的问题。 缓存过滤机制groupcache的singleflight模块实现了缓存过滤机制。即在大量相同的请求并发访问时，若缓存未能命中，则会触发大量的Load过程。即所有的查询请求全部会落到数据源（如DB）或从其它节点加载数据，因此考虑到节点可靠性，此时DB存在因压力过大而导致服务不可用的情况，同时也浪费资源。groupcache设计所提供的解决方案是：尽管存在并发的查询，但能保证只有一个请求能够真正的转发到DB执行查询，而其余的请求都会阻塞等待，直至第一个请求的查询结果返回，同时，其它请求会使用第一个请求的查询结果，最后再返回给客户端。singleflight通过go的sync.WaitGroup实现同一时间相同查询请求的合并。 最后，虽然官方声称groupcache在很多场景下已经成为memcached的替代版，但其本身存在固有的”局限性”。 groupcache采用的是LRU缓存机制，使用List和Map实现，不支持过期机制（不支持设置过期时间），也没有明确的回收机制（只是简单地将队尾的数据移除），但能够控制缓存总大小在用户设置的阈值之下。 groupcache不支持set、update以及delete，即对于客户端而言，只能执行get操作。 groupcache针对key不支持多个版本的值。 总而言之，groupcache是一个值得学习的开源分布式缓存系统，通过阅读源码，一方面可以了解分布式缓存相关的设计原则，也能学习编程相关的设计经验。]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式缓存</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式缓存</tag>
        <tag>LRU缓存</tag>
        <tag>一致性哈希</tag>
        <tag>缓存过滤机制</tag>
        <tag>缓存击穿</tag>
        <tag>热点数据扩散</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Live Sequence Protocol 实现]]></title>
    <url>%2F2018%2F10%2F25%2FLive-Sequence-Protocol-%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[分布式环境中，网络不稳定导致消息（数据包）的传输存在乱序、重复和丢失的情况，同时，节点宕机也不可避免。如何优雅地处理这些问题，是构建一个健壮的分布式系统的关键。网络的复杂性使得数据包传输协议至关重要。低级别的IP协议提供不可靠的数据报服务，即消息可能延时、重复或丢失，另外，它也限制了在网络节点中传输的消息的最大字节数，因此很少直接利用IP协议来构建分布式应用。在传输层，UDP也不提供可靠的数据报服务，但它可以通过端口定向传输报文。而TCP则会保证消息传输的可靠性、有序性，并允许任意字节大小的消息传递，还提供额外的功能，如流量控制、拥塞控制。 我们的目的是实现一个基于UDP、具备TCP几个关键特性的消息传输协议 (Live Sequence Protocol），同时它还具备如下功能： 不同于UDP或TCP，它支持 client-server通信模型 。 server会维护若干个client的连接。 server与client的通信是通过向对方发送消息来实现，消息大小限制与UDP相同。 消息传输是可靠的：消息一旦发送，就会被顺序接收，且每个消息只会被接收一次。 server与client可以检测连接的状态。 协议具体的工作原理、关键特性、运行流程及开放使用的接口可以参考p1.pdf。下面我会讨论协议实现过程中的几个关键点，以及个人在实现过程中遇到的棘手的问题。 系统逻辑框架构建清晰且优雅地构建整个系统的逻辑框架至关重要，代码框架设计关系到后期功能模块调试与扩展，不合理的系统逻辑框架设计会使得后期的扩展寸步难行，也会导致代码的可调试性、可读性变差。因此，在编写出你的第一个可用的版本之前，尽可能合理地安排系统框架，这需要理解并梳理系统的主干及各分支（异常）运行流程，为了更简单、高效且合理地实现模块功能，必须尽可能熟悉(go)语言的特性(channel、gorountine及interface)。 协议实现文档清晰地描述了协议的完整工作流程，按照此流程，其核心是epoch event触发后，协议的应对逻辑，可以实现出一个可运行的版本。合理安排程序框架关键在于处理好以下三个方面的问题： 哪些功能逻辑应该被顺序执行，如何保证同步顺序执行。比如，当创建client后，只有当其与server建立连接connection（抽象连接，并非消息传输所使用的连接）后才能返回，同时启动后台服务。注意client创建UDP连接到server可能会尝试多次，因为server可能存在慢启动问题，而且Connect消息也可能丢失。 系统需要哪些后台服务(background goroutine)， 后台服务如何可靠地同主线程协调交互。比如，对于client而言，至少需要三个goroutine来处理消息。 read goroutine持续从连接中读取消息，直到连接关闭。 write goroutine，因为写操作的调用是非阻塞的，但由于滑动窗口大小限制，并非所有消息都能立刻cache到滑动窗口并立即发送出去，因此，可以将用户wirte的消息放入到消息的write channel中，然后由专门的后台服务从channel中取消息，并在恰当的时候发送消息。 epoch event trigger goroutine，即处理与epoch相关的逻辑，超时如何处理？接收到Ack消息或Data消息如何处理？达到max epoch时如何处理？ 确保开放接口的实现符合协议规范中预定义的准则要求。比如，server的Read接口的调用会阻塞直到其从任一client收到消息，然后返回消息的payload及对应的connection ID。如果连接丢失，关闭或者Server主动关闭终止，都应该返回错误提示。这个方法不应该被简单地设计成从连接中持续读取数据，因为Server可能连接多个client，针对每一个client 连接的读取，必须启用单独的goroutine。所以，一种简单的设计是server并发地从各连接读取数据，若通过了校验（如保证用户调用Read所返回的数据正是用户所期望的），则将数据放入到channel，让Read持续从channel中取数据，注意数据一旦添加到channel中，则会以放入的顺序被Read取出，并返回给用户。 理解UDP通信本质大家可能对TCP原理及编程更为熟悉，UDP相对简单，但因为lsp(Live Sequence Protocol)基于UDP，并在更高的协议抽象层面具备TCP的特性，所以，不要混淆了二者的通信原理。UDP是无连接的！它会完全按照程序员的意愿发送消息，它不考虑对方主机是否存在或正常工作，也不会主动重发消息，因此，也就无法保证消息的可靠接收与发送。 所以，server不需要也不能维护其与client的连接！但应当在sever端创建并维护与其通信的client关联的信息实体（需包含哪些数据？），那何时创建？答案是当server读取到数据时，因为此时可以获取读取所返回的client地址，server可以通过cache已经连接的信息来判断此次读取对应的连接是否是新的连接。若不是，则直接进入消息读取处理逻辑，否则需要先初始化server维护的client相关联的信息实体。 最后，注意server与client使用的是不同的UDP读写通信接口。（client直接持有与server通信的连接，而server是通过指定地址（IP+port）发送与接收消息）。 如何实现滑动窗口滑动窗口sliding window是协议实现流量控制的关键，是整个协议的功能核心，并且其与TCP的滑动窗口机制类似。关于滑动窗口，在理解它的工作原理后，重点考虑以下三个方面： 设计滑动窗口的数据结构。 消息应该被有序添加到滑动窗口。 发送消息窗口需要标识每一条消息是否已经被ack。 发送消息所关联的滑动窗口latestSentDataMsg。以client作为示例，维持其发送消息的窗口，以便对未按时返回Ack的消息进行重发（已发送的data消息可能会丢失，或者接收主机响应的Ack消息丢失）。 因为窗口内的消息所返回的Ack是无序的（消息异步发送，网络传输也不能保证消息按序到达），所以，需要维护一个指针，表示当前返回的Ack消息的最小的序号receivedAckSeqNum，以作为窗口向前推进的依据。 当client发送data消息时，需同时将其cache到latestSentDataMsg。而当其接收到Ack消息时，需要执行更新此指针receivedAckSeqNum的逻辑。而server则需要对其所维护的每一个连接构建对应的发送消息窗口，但处理逻辑类似。 接收消息所关联的滑动窗口latestReceivedDataMsg。同样以client作为示例，维持其接收消息的窗口，以便在计时器超时后，对最近收到的若干个data消息，重发Ack消息。 同样，接收消息窗口也是无序的，因此，为了保证返回给用户的消息有序，需要维护一个指针，表示下一个期望接收到的data消息序号nextReceiveDataSeqNum（或者是当前已经接收到的最大的data消息的消息序号），它是依次递增的。对于接收到的任何data消息，若其SeqNum在此指针之后，都应该直接添加（暂时缓存）到latestReceivedDataMsg中，而不应该作为Read调用的返回结果。 当client收到server的data消息时，也需要将其cache到latestReceivedDataMsg，并判断是否需要更新nextReceiveDataSeqNum，若需要更新，则应当将更新过程中所涉及到的cache在接收消息窗口中的data消息按序添加到供Read接口所读取的channel。server同样是一个连接对应一个接收消息窗口。 如何实现流量控制流量控制表示若当前主机有过多的消息未被ack（网络拥塞），因此发送主机需要对用户调用Write接口的data消息进行阻塞以延缓发送。其实现关键是滑动窗口机制。具体实现原理为： 当用户调用Write接口以发送消息时，将消息添加到消息发送队列channel，然后返回，不能阻塞。 后台服务write goroutine从消息发送队列中不断的取消息，但在消息正式发送前，需要检测消息发送滑动窗口是否空闲idle，并且包含多少空闲的slot。 空闲的slot数目可以根据以下表达式计算：idleSlotNum := cli.params.WindowSize-(lastMsg.SeqNum-cli.receivedAckSeqNum)，其中，lastMsg为消息发送窗口中最后一个消息，即SeqNum最大的消息。 如果idleSlotNum大于0，则可以发送对应数目的消息，并将已经发送的消息记录到消息发送窗口，同时递增nextSendDataSeqNum指针。否则，write goroutine应该被阻塞住。那如何解除阻塞？每当client在接收到Ack消息时都要去尝试解除阻塞。 如图所示，当滑动窗口处于(a)的情况下，当用户调用Write以发送消息时，消息会被阻塞在write channel中，因为此时receivedAckSeqNum为9，消息发送窗口的idle的slot数目为：5-(14-9)=0。而当client接收若干Ack消息后，滑动窗口转移到(b)状态时，注意到receivedAckSeqNum从9逐一递增到11，消息发送窗口idleSlotNum为：5-(14-11)=2，因此窗口前移，并可以从write channel中顺序取出两个消息，进行发送。 如何检测消息重复消息重复主要包括data和Ack消息的重复接收。以client作为示例。 data消息的重复接收。 当client读取到data消息后，需要判断消息是否已经接收过。若消息重复，则直接返回Ack消息，否则应该先将消息cache到latestReceivedDataMsg。 可以通过消息的SeqNum来去重。这涉及两种情况：其一，消息已经被Ack，并且已经从latestReceivedDataMsg中移除，我们称之为消息被丢弃(discarded)。其二，消息被Ack，但仍然cache在latestReceivedDataMsg中。 Ack消息的重复接收。Ack消息的去重逻辑同data消息类似。 如何保证消息顺序发送主机异步发送消息，且消息在网络中传输也有不同程度的延迟，因此接收主机接收的消息序列的顺序很可能与发送主机发送的消息顺序不同。如何保证消息顺序？准确而言，如何以发送主机发送消息的顺序来返回给用户。 针对具备滑动窗口机制的消息传输，可以保证滑动窗口前所接收的消息，即已经被discarded的消息肯定是有序返回给用户的。而滑动窗口内的消息，因为无法规避从网络中读取乱序消息的问题，但在读取到消息后可以控制以何种顺序将消息返回给用户。简单而言，将收到的data消息先cache在latestReceivedDataMsg中，然后通过指针nextReceiveDataSeqNum来判断是否应该将窗口中cache的消息返回给用户。 如何优雅地关闭连接保证连接优雅地关闭是一个非常棘手的问题。其中，相比于client端的连接关闭，server的关闭又更为复杂。协议规范清晰地描述了client及server在关闭连接时需要注意的问题。其核心是： 当存在pending消息时，需要将其处理完成（即需保证接收到Ack消息）。 同时，一旦data消息被加入到write channel，它必须保证最后能够被发送出去。client的关闭相对简单，具体处理逻辑为：当用户调用Close接口时，需要判断是否存在pending消息，如何检测？两个条件： 保证消息发送窗口的最后一个消息的SeqNum恰好为其持有的receivedAckSeqNum的值。 保证write channel中没有任何未被处理的消息。因此如果此时存在pending消息，Close会被阻塞。那如何解除阻塞？每当client在接收到Ack消息时都要去尝试解除阻塞。此外，值得注意的是，在阻塞的过程中，如果触发了max epoch event，则client应该立刻返回，因为这表明连接已经discarded，此时要么所有pending消息已被处理，要么server主动关闭了连接。server的CloseConn接口可以看作是client的Close接口的非阻塞版本。而Close接口需要协调所有的connection的关闭。同样，server的某个连接也可能到达max epoch，此时其对应的连接应该被关闭。当所有连接都关闭时，Close才能返回。在连接关闭时，需要及时退出对应的background goroutine。 需注意的细节问题往往一些编程方面的细节，包括逻辑漏洞或者被忽视的语法问题会造成很长时间的调试。而且，当通信过程中，数据交换复杂变得越发复杂时，很难从庞大的日志文件中找出错误的根源。个人在实现的过程中，遇到两个问题： dead lock。死锁很容易产生，一般有两个原因，其一，资源的相互持有，造成两个线程都无法向前推进。其二，没有正确嵌套使用锁，你需要清楚锁是否可重入。 buffered channel。其导致的问题比较隐蔽，你首先要明确是使用带缓冲的channel或者不带缓冲的channel，如果是buffered channel，你需要确定它的大小，如果你不确定缓冲区数量是否足够，建议设置的稍大一些，但这个前提是，必须在合适的时机清空buffered channel，避免在复用buffered channel之后导致逻辑受到影响。 最后，需要提醒的是，分布式程序异步、并发，且网络复杂的特性导致其很难debug。所以，尽可能设计完善的日志流程，以帮助跟踪未符合期望的执行逻辑，并定位问题。 另外，cmu提供较为完善的测试程序，如果程序出现问题，可以对某一个或几个子测试用例进行单独测试，熟悉测试用例代码，了解测试用例流程是有必要的。 参考代码在这里]]></content>
      <categories>
        <category>分布式系统</category>
        <category>传输协议</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>网络编程</tag>
        <tag>传输协议</tag>
        <tag>可靠服务</tag>
        <tag>流量控制</tag>
      </tags>
  </entry>
</search>
