<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[etcd raftexample 源码简析]]></title>
    <url>%2F2019%2F01%2F09%2Fetcd-raftexample-%E6%BA%90%E7%A0%81%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[最近集中了解了ZAB、Raft及Paxos协议的基本理论，因此想进一步深入到源代码仔细体验一致性协议如何在分布式系统中发挥作用。虽然在 MIT 6.824 课程中有简单实现Raft协议，并基于Raft构建了一个粗糙的 kv 存储系统。但还是想了解下工业生产级别的Raft协议的实现内幕，故选择etcd进行解读。etcd是 CoreOS 基于Raft协议使用 go 开发的分布式 kv 存储系统，可用于服务发现、共享配置及其它利用一致性保障的功能（如leader选举及分布式锁、队列等）。这些功能ZooKeeper不也有提供？没错。它们都可以作为其它分布式应用的独立协调服务，这通过通用的一致性元信息存储来实现。但在易用性上，etcd可谓略胜一筹。因此，后续的一系列博客会简单对etcd各重要组成部分的源码进行简要分析（重点在Raft实现）。本文主要是分析etcd的raftexample的代码。它是etcd官方提供的如何使用etcd内部的Raft协议组件来构建分布式应用的一个简单示例。 etcd内部使用Raft协议对集群各节点的状态（数据、日志及快照等）进行同步。类似于ZooKeeper利用ZAB协议作为底层的可靠的事务广播协议。但etcd对Raft的实现有点特殊，它底层的Raft组件库只实现了Raft协议最核心的部分，这主要包括选主逻辑、一致性具体实现以及成员关系变化。而将诸如WAL、snapshot以及网络传输等模块让用户来实现，这明显增加了使用的难度，但对于应用本质上也更灵活。 本文会简单分析etcd提供的如何其核心的Raft协议组件来构建一个简单的高可用内存 kv 存储（其本质是一个状态机），用户可以通过 http 协议来访问应用（kv 存储系统），以对数据进行读写操作，在对日志进行读写过程中，Raft组件库能够保证各节点数据的一致性。其对应的源码目录为/etcd-io/etcd/tree/master/contrib/raftexample。另外，需要强调的是，本文的主题是利用Raft协议库来构建一个简单的 kv 存储，关于Raft协议库实现的细节不会过多阐述。若读者想继续了解此文，个人建议clone源代码，在阅读源代码的过程中，参考本文效果可能会更好，如果有理解错误的地方，欢迎指正！ 数据结构在按raftexample/main的示例完整解读整个流程之前，先熟悉几个重要的数据结构会有好处。此示例构建的应用为 kv 存储系统，因此，先来了解 kvstore定义的相关字段： 1234567// a key-value store backed by rafttype kvstore struct &#123; proposeC chan&lt;- string // channel for proposing updates mu sync.RWMutex kvStore map[string]string // current committed key-value pairs snapshotter *snap.Snapshotter&#125; // kvstore.go 关键结构成员解释如下： proposeC: 应用与底层Raft核心库之间的通信channel，当用户向应用通过 http 发送更新请求时，应用会将此请求通过channel传递给底层的Raft库。 kvStore: kv 结构的内存存储，即对应应用的状态机。 snapshotter: 由应用管理的快照snapshot接口。 接下来分析一下应用封装底层Raft核心库的结构raftNode，应用通过与raftNode结构进行交互来使用底层的Raft核心协议，它封装完整的Raft协议相关的逻辑（如WAL及snapshot等）。我们先列举它的相关处理逻辑，然后展示其结构内容。具体地逻辑如下： 将应用的更新请求传递给Raft核心来执行。 同时，将Raft协议已提交的日志传回给应用，以指示应用来将日志请求应用到状态机。 另外，它也处理由Raft协议相关的指令，包括选举、成员变化等。 处理WAL日志相关逻辑。 处理快照相关的逻辑。 将底层Raft协议的指令消息传输到集群其它节点。 123456789101112131415161718192021222324252627282930313233// A key-value stream backed by rafttype raftNode struct &#123; proposeC &lt;-chan string // proposed messages (k,v) confChangeC &lt;-chan raftpb.ConfChange // proposed cluster config changes commitC chan&lt;- *string // entries committed to log (k,v) errorC chan&lt;- error // errors from raft session id int // client ID for raft session peers []string // raft peer URLs join bool // node is joining an existing cluster waldir string // path to WAL directory snapdir string // path to snapshot directory getSnapshot func() ([]byte, error) lastIndex uint64 // index of log at start confState raftpb.ConfState snapshotIndex uint64 appliedIndex uint64 // raft backing for the commit/error channel node raft.Node raftStorage *raft.MemoryStorage wal *wal.WAL snapshotter *snap.Snapshotter snapshotterReady chan *snap.Snapshotter // signals when snapshotter is ready snapCount uint64 transport *rafthttp.Transport stopc chan struct&#123;&#125; // signals proposal channel closed httpstopc chan struct&#123;&#125; // signals http server to shutdown httpdonec chan struct&#123;&#125; // signals http server shutdown complete&#125; // raft.go 关键结构成员解释如下： proposeC: 同kvStore.proposeC通道类似，事实上，kvStore会将用户的更新请求传递给raftNode以使得其最终能传递给底层的Raft协议库。 confChangeC: Raft协议通过此channel来传递集群配置变更的请求给应用。 commitC: 底层Raft协议通过此channel可以向应用传递准备提交或应用的channel，最终kvStore会反复从此通道中读取可以提交的日志entry，然后正式应用到状态机。 node: 即底层Raft协议组件，raftNode可以通过node提供的接口来与Raft组件进行交互。 raftStorage: Raft协议的状态存储组件，应用在更新kvStore状态机时，也会更新此组件，并且通过raft.Config传给Raft协议。 wal: 管理WAL日志，前文提过etcd将日志的相关逻辑交由应用来管理。 snapshotter: 管理 snapshot文件，快照文件也是由应用来管理。 transport: 应用通过此接口与集群中其它的节点(peer)通信，比如传输日志同步消息、快照同步消息等。网络传输也是由应用来处理。 其它的相关的数据结构不再展开，具体可以查看源代码，辅助注释理解。 关键流程我们从main.go中开始通过梳理一个典型的由客户端发起的状态更新请求的完整流程来理解如何利用Raft协议库来构建应用状态机。main.go的主要逻辑如下： 123456789101112131415161718func main() &#123; // 解析客户端请求参数信息 ... proposeC := make(chan string) defer close(proposeC) confChangeC := make(chan raftpb.ConfChange) defer close(confChangeC) // raft provides a commit stream for the proposals from the http api var kvs *kvstore getSnapshot := func() ([]byte, error) &#123; return kvs.getSnapshot() &#125; commitC, errorC, snapshotterReady := newRaftNode(*id, strings.Split(*cluster, ","), *join, getSnapshot, proposeC, confChangeC) kvs = newKVStore(&lt;-snapshotterReady, proposeC, commitC, errorC) // the key-value http handler will propose updates to raft serveHttpKVAPI(kvs, *kvport, confChangeC, errorC)&#125; // main.go 显然，此示例的步骤较为清晰。主要包括三方面逻辑：其一，初始化raftNode，并通过 go routine 来启动相关的逻辑，实际上，这也是初始化并启动Raft协议组件，后面会详细相关流程。其二，初始化应用状态机，它会反复从commitC通道中读取raftNode/Raft传递给它的准备提交应用的日志。最后，启动 http 服务以接收客户端读写请求，并设置监听。下面会围绕这三个功能相关的逻辑进行阐述。 Raft 初始化首先我们来理顺Raft初始化的逻辑，这部分相对简单。 12345678910111213141516171819202122232425262728func newRaftNode(id int, peers []string, join bool, getSnapshot func() ([]byte, error), proposeC &lt;-chan string, confChangeC &lt;-chan raftpb.ConfChange) (&lt;-chan *string, &lt;-chan error, &lt;-chan *snap.Snapshotter) &#123; commitC := make(chan *string) errorC := make(chan error) rc := &amp;raftNode&#123; proposeC: proposeC, confChangeC: confChangeC, commitC: commitC, errorC: errorC, id: id, peers: peers, join: join, waldir: fmt.Sprintf("raftexample-%d", id), snapdir: fmt.Sprintf("raftexample-%d-snap", id), getSnapshot: getSnapshot, snapCount: defaultSnapshotCount, // 只有当日志数量达到此阈值时才执行快照 stopc: make(chan struct&#123;&#125;), httpstopc: make(chan struct&#123;&#125;), httpdonec: make(chan struct&#123;&#125;), snapshotterReady: make(chan *snap.Snapshotter, 1), // rest of structure populated after WAL replay &#125; go rc.startRaft() // 通过 go routine 来启动 raftNode 的相关处理逻辑 return commitC, errorC, rc.snapshotterReady&#125; // raft.go newRaftNode初始化一个Raft实例，并且将commitC、errorC及snapshotterReady三个通道返回给raftNode。raftNode初始化所需要的信息包括集群中其它peer的地址、WAL管理日志以及snapshot管理快照的目录等。接下来，分析稍为复杂的startRaft的逻辑： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657func (rc *raftNode) startRaft() &#123; if !fileutil.Exist(rc.snapdir) &#123; // 若快照目录不存在，则创建 if err := os.Mkdir(rc.snapdir, 0750); err != nil &#123; log.Fatalf("raftexample: cannot create dir for snapshot (%v)", err) &#125; &#125; rc.snapshotter = snap.New(zap.NewExample(), rc.snapdir) rc.snapshotterReady &lt;- rc.snapshotter oldwal := wal.Exist(rc.waldir) //判断是否已存在 WAL 日志（在节点宕机重启时会执行） rc.wal = rc.replayWAL() // 重放 WAL 日志以应用到 raft 实例中 rpeers := make([]raft.Peer, len(rc.peers)) for i := range rpeers &#123; // 创建集群节点标识 rpeers[i] = raft.Peer&#123;ID: uint64(i + 1)&#125; &#125; c := &amp;raft.Config&#123; // 初始化底层 raft 协议实例的配置结构 ID: uint64(rc.id), ElectionTick: 10, HeartbeatTick: 1, Storage: rc.raftStorage, MaxSizePerMsg: 1024 * 1024, MaxInflightMsgs: 256, MaxUncommittedEntriesSize: 1 &lt;&lt; 30, &#125; if oldwal &#123; // 若已存在 WAL 日志，则重启节点（并非第一次启动） rc.node = raft.RestartNode(c) &#125; else &#123; startPeers := rpeers if rc.join &#123; // 节点可以通过两种不同的方式来加入集群，应用以 join 字段来区分 startPeers = nil &#125; // 启动底层 raft 的协议实体 node rc.node = raft.StartNode(c, startPeers) &#125; // 初始化集群网格传输组件 rc.transport = &amp;rafthttp.Transport&#123; Logger: zap.NewExample(), ID: types.ID(rc.id), ClusterID: 0x1000, Raft: rc, ServerStats: stats.NewServerStats("", ""), LeaderStats: stats.NewLeaderStats(strconv.Itoa(rc.id)), ErrorC: make(chan error), &#125; // 启动（初始化）transport 的相关内容 rc.transport.Start() for i := range rc.peers &#123; // 为每一个节点添加集群中其它的 peer，并且会启动数据传输通道 if i+1 != rc.id &#123; rc.transport.AddPeer(types.ID(i+1), []string&#123;rc.peers[i]&#125;) &#125; &#125; // 启动 go routine 来处理本节点与其它节点通信的 http 服务监听 go rc.serveRaft() // 启动 go routine 来处理 raftNode 与 底层 raft 通过通道来进行通信 go rc.serveChannels()&#125; 应用初始化应用初始化相关代码较为简单，它只需要初始化内存状态机，并且监听从raftNode传来的准备提交的日志的channel即可，以将commitC读到的日志应用到内存状态机。应用初始化相关代码如下： 12345678func newKVStore(snapshotter *snap.Snapshotter, proposeC chan&lt;- string, commitC &lt;-chan *string, errorC &lt;-chan error) *kvstore &#123; s := &amp;kvstore&#123;proposeC: proposeC, kvStore: make(map[string]string), snapshotter: snapshotter&#125; // replay log into key-value map s.readCommits(commitC, errorC) // read commits from raft into kvStore map until error go s.readCommits(commitC, errorC) return s&#125; // kvstore.go 其中readComits即循环监听通道，并从其中取出日志的函数。并且如果本地存在snapshot，则先将日志重放到内存状态机中。 12345678910111213141516171819202122232425262728293031323334func (s *kvstore) readCommits(commitC &lt;-chan *string, errorC &lt;-chan error) &#123; for data := range commitC &#123; if data == nil &#123; // done replaying log; new data incoming // OR signaled to load snapshot snapshot, err := s.snapshotter.Load() if err == snap.ErrNoSnapshot &#123; return &#125; if err != nil &#123; log.Panic(err) &#125; log.Printf("loading snapshot at term %d and index %d", snapshot.Metadata.Term, snapshot.Metadata.Index) // 将之前某时刻快照重新设置为状态机目前的状态 if err := s.recoverFromSnapshot(snapshot.Data); err != nil &#123; log.Panic(err) &#125; continue &#125; // 先对数据解码 var dataKv kv dec := gob.NewDecoder(bytes.NewBufferString(*data)) if err := dec.Decode(&amp;dataKv); err != nil &#123; log.Fatalf("raftexample: could not decode message (%v)", err) &#125; s.mu.Lock() s.kvStore[dataKv.Key] = dataKv.Val s.mu.Unlock() &#125; if err, ok := &lt;-errorC; ok &#123; log.Fatal(err) &#125;&#125; // kvstore.go 开启 http 服务监听此应用对用户（客户端）提供 http 接口服务。用户可以通过此 http 接口来提交对应用的数据更新请求，应用启动对外服务及设置监听相关逻辑如下： 1234567891011121314151617181920// serveHttpKVAPI starts a key-value server with a GET/PUT API and listens.func serveHttpKVAPI(kv *kvstore, port int, confChangeC chan&lt;- raftpb.ConfChange, errorC &lt;-chan error) &#123; srv := http.Server&#123; Addr: ":" + strconv.Itoa(port), Handler: &amp;httpKVAPI&#123; store: kv, confChangeC: confChangeC, &#125;, &#125; go func() &#123; if err := srv.ListenAndServe(); err != nil &#123; log.Fatal(err) &#125; &#125;() // exit when raft goes down if err, ok := &lt;-errorC; ok &#123; log.Fatal(err) &#125;&#125; // httpapi.go 而接收并解析用户的请求相关逻辑如下所示，它将从用户接收到的对应用的读写请求，传递给raftNode，由raftNode传递至底层的raft协议核心组件来处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func (h *httpKVAPI) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; key := r.RequestURI switch &#123; case r.Method == "PUT": v, err := ioutil.ReadAll(r.Body) if err != nil &#123; log.Printf("Failed to read on PUT (%v)\n", err) http.Error(w, "Failed on PUT", http.StatusBadRequest) return &#125; // 将请求传递至 raftNode 组件，最终会传递到底层的 raft 核心协议模块 h.store.Propose(key, string(v)) // Optimistic-- no waiting for ack from raft. Value is not yet // committed so a subsequent GET on the key may return old value w.WriteHeader(http.StatusNoContent) case r.Method == "GET": if v, ok := h.store.Lookup(key); ok &#123; w.Write([]byte(v)) &#125; else &#123; http.Error(w, "Failed to GET", http.StatusNotFound) &#125; case r.Method == "POST": url, err := ioutil.ReadAll(r.Body) if err != nil &#123; log.Printf("Failed to read on POST (%v)\n", err) http.Error(w, "Failed on POST", http.StatusBadRequest) return &#125; nodeId, err := strconv.ParseUint(key[1:], 0, 64) if err != nil &#123; log.Printf("Failed to convert ID for conf change (%v)\n", err) http.Error(w, "Failed on POST", http.StatusBadRequest) return &#125; cc := raftpb.ConfChange&#123; Type: raftpb.ConfChangeAddNode, NodeID: nodeId, Context: url, &#125; h.confChangeC &lt;- cc // As above, optimistic that raft will apply the conf change w.WriteHeader(http.StatusNoContent) case r.Method == "DELETE": nodeId, err := strconv.ParseUint(key[1:], 0, 64) if err != nil &#123; log.Printf("Failed to convert ID for conf change (%v)\n", err) http.Error(w, "Failed on DELETE", http.StatusBadRequest) return &#125; cc := raftpb.ConfChange&#123; Type: raftpb.ConfChangeRemoveNode, NodeID: nodeId, &#125; h.confChangeC &lt;- cc // .. &#125;&#125; // httpapi.go 状态机更新请求在 httpapi.go 的逻辑中，我们选择 PUT 请求分支来进行分析。当它接收到用户发送的更新请求时。它会调用 kvstore的Propose函数，并将更新请求相关参数传递过去： 12345678func (s *kvstore) Propose(k string, v string) &#123; var buf bytes.Buffer // 编码后，传递至 raftNode if err := gob.NewEncoder(&amp;buf).Encode(kv&#123;k, v&#125;); err != nil &#123; log.Fatal(err) &#125; s.proposeC &lt;- buf.String()&#125; // kvstore.go 在kvstore将请求 buf 压到管道后，raftNode可以在管道的另一端取出，即在serverChannel函数取出请求，并交由底层 raft协议核心库来保证此次集群状态的更新。相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889func (rc *raftNode) serveChannels() &#123; snap, err := rc.raftStorage.Snapshot() if err != nil &#123; panic(err) &#125; // 利用 raft 实例的内存状态机初始化 snapshot 相关属性 rc.confState = snap.Metadata.ConfState rc.snapshotIndex = snap.Metadata.Index rc.appliedIndex = snap.Metadata.Index defer rc.wal.Close() // 初始化一个定时器，每次触发 tick 都会调用底层 node.Tick()函数，以表示一次心跳事件， // 不同角色的事件处理函数不同。 ticker := time.NewTicker(100 * time.Millisecond) defer ticker.Stop() // send proposals over raft // 开启 go routine 以接收应用层(kvstore)的请求（包括正常的日志请求及集群配置变更请求） go func() &#123; confChangeCount := uint64(0) // 循环监听来自 kvstore 的请求消息 for rc.proposeC != nil &amp;&amp; rc.confChangeC != nil &#123; select &#123; // 1. 正常的日志请求 case prop, ok := &lt;-rc.proposeC: if !ok &#123; rc.proposeC = nil &#125; else &#123; // blocks until accepted by raft state machine // 调用底层的 raft 核心库的 node 的 Propose 接口来处理请求 rc.node.Propose(context.TODO(), []byte(prop)) &#125; // 2. 配置变更请求类似处理 case cc, ok := &lt;-rc.confChangeC: if !ok &#123; rc.confChangeC = nil &#125; else &#123; confChangeCount++ cc.ID = confChangeCount rc.node.ProposeConfChange(context.TODO(), cc) &#125; &#125; &#125; // client closed channel; shutdown raft if not already close(rc.stopc) &#125;() // event loop on raft state machine updates // 开启 go routine 以循环处理底层 raft 核心库通过 Ready 通道发送给 raftNode 的指令 for &#123; select &#123; // 触发定时器事件 case &lt;-ticker.C: rc.node.Tick() // store raft entries to wal, then publish over commit channel // 1.通过 Ready 获取 raft 核心库传递的指令 case rd := &lt;-rc.node.Ready(): // 2. 先写 WAL 日志 rc.wal.Save(rd.HardState, rd.Entries) if !raft.IsEmptySnap(rd.Snapshot) &#123; rc.saveSnap(rd.Snapshot) rc.raftStorage.ApplySnapshot(rd.Snapshot) rc.publishSnapshot(rd.Snapshot) &#125; // 3. 更新 raft 实例的内存状态 rc.raftStorage.Append(rd.Entries) // 4. 将接收到消息传递通过 transport 组件传递给集群其它 peer rc.transport.Send(rd.Messages) // 5. 将已经提交的请求日志应用到状态机 if ok := rc.publishEntries(rc.entriesToApply(rd.CommittedEntries)); !ok &#123; rc.stop() return &#125; // 6. 如果有必要，则会触发一次快照 rc.maybeTriggerSnapshot() // 7. 通知底层 raft 核心库，当前的指令已经提交应用完成，这使得 raft 核心库可以发送下一个 Ready 指令了。 rc.node.Advance() case err := &lt;-rc.transport.ErrorC: rc.writeError(err) return case &lt;-rc.stopc: rc.stop() return &#125; &#125;&#125; // raft.go 上述关于 raftNode与底层Raft核心库交互的相关逻辑大致已经清楚。大概地，raftNode会将从kvstore接收到的用户对状态机的更新请求传递给底层raft核心库来处理。此后，raftNode会阻塞直至收到由raft组件传回的Ready指令。根据指令的内容，先写WAL日志，更新内存状态存储，并分发至其它节点。最后如果指令已经可以提交，即底层raft组件判定请求在集群多数节点已经完成状态复制后，则应用到状态机，具体由kvstore来执行。并且若触发了快照的条件，则执行快照操作，最后才通知raft核心库可以准备下一个Ready指令。关于 Ready结构具体内容，我们可以大致看一下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// Ready encapsulates the entries and messages that are ready to read,// be saved to stable storage, committed or sent to other peers.// All fields in Ready are read-only.// Ready 结构包装了事务日志，以及需要发送给其它 peer 的消息指令，这些字段都是只读的，且有些必须进行持久化，或者已经可以提交应用。type Ready struct &#123; // The current volatile state of a Node. // SoftState will be nil if there is no update. // It is not required to consume or store SoftState. // 包含了内存中的状态，即瞬时状态数据 *SoftState // The current state of a Node to be saved to stable storage BEFORE // Messages are sent. // HardState will be equal to empty state if there is no update. // 包含了持久化的状态，即在消息发送给其它节点前需要保存到磁盘 pb.HardState // ReadStates can be used for node to serve linearizable read requests locally // when its applied index is greater than the index in ReadState. // Note that the readState will be returned when raft receives msgReadIndex. // The returned is only valid for the request that requested to read. // 用于节点提供本地的线性化读请求，但其条件是节点的 appliedIndex 必须要大于 ReadState 中的 index，这容易理解，否则会造成客户端的读的数据的不一致 ReadStates []ReadState // Entries specifies entries to be saved to stable storage BEFORE // Messages are sent. // 表示在发送其它节点之前需要被持久化的状态数据 Entries []pb.Entry // Snapshot specifies the snapshot to be saved to stable storage. // 与快照相关，指定了可以持久化的 snapshot 数据 Snapshot pb.Snapshot // CommittedEntries specifies entries to be committed to a // store/state-machine. These have previously been committed to stable // store. // 可以被提交应用到状态机的状态数据 CommittedEntries []pb.Entry // Messages specifies outbound messages to be sent AFTER Entries are // committed to stable storage. // If it contains a MsgSnap message, the application MUST report back to raft // when the snapshot has been received or has failed by calling ReportSnapshot. // 当 Entries 被持久化后，需要转发到其它节点的消息 Messages []pb.Message // MustSync indicates whether the HardState and Entries must be synchronously // written to disk or if an asynchronous write is permissible. MustSync bool&#125; // /etcd/raft/node.go 日志管理raftexample中使用了etcd提供的通用日志库来管理WAL日志，我们下面来分析下应用管理日志的相关逻辑。在上面的状态机更新请求中，注意到当raftNode接收到raft核心传递的Ready指令，第一步就进行写WAL日志操作，这种操作较为常见，以避免更新丢失。值得一提的的，WAL日志也会在各节点进行同步。另外在startRaft函数中，即启动raftNode相关逻辑时，便进行了WAL日志重放rc.wal = rc.replayWAL()，我们详细看一下日志重放的流程： 123456789101112131415161718192021222324252627282930313233// replayWAL replays WAL entries into the raft instance.// 重放节点 WAL 日志，以将重新初始化 raft 实例的内存状态func (rc *raftNode) replayWAL() *wal.WAL &#123; log.Printf("replaying WAL of member %d", rc.id) // 1. 加载快照数据 snapshot := rc.loadSnapshot() // 2. 借助快照数据（的相关属性）来打开 WAL 日志。应用只会重放快照时间点（索引）之后的日志，因为快照数据直接记录着状态机的状态数据（这等同于将快照数据所对应的 WAL 日志重放），因此可以直接应用到内存状态结构。换言之，不需要重放 WAL 包含的所有的日志项，这明显可以加快日志重放的速度。结合 openWAL 函数可以得出结论。 w := rc.openWAL(snapshot) // 3. 从 WAL 日志中读取事务日志 _, st, ents, err := w.ReadAll() if err != nil &#123; log.Fatalf("raftexample: failed to read WAL (%v)", err) &#125; // 4. 构建 raft 实例的内存状态结构 rc.raftStorage = raft.NewMemoryStorage() if snapshot != nil &#123; // 5. 将快照数据直接加载应用到内存结构 rc.raftStorage.ApplySnapshot(*snapshot) &#125; rc.raftStorage.SetHardState(st) // append to storage so raft starts at the right place in log // 6. 将 WAL 记录的日志项更新到内存状态结构 rc.raftStorage.Append(ents) // send nil once lastIndex is published so client knows commit channel is current if len(ents) &gt; 0 &#123; // 更新最后一条日志索引的记录 rc.lastIndex = ents[len(ents)-1].Index &#125; else &#123; rc.commitC &lt;- nil &#125; return w&#125; // raft.go 通过查看上述的流程，关于 WAL日志重放的流程也很清晰。 快照管理快照(snapshot)本质是对日志进行压缩，它是对状态机某一时刻（或者日志的某一索引）的状态的保存。快照操作可以缓解日志文件无限制增长的问题，一旦达日志项达到某一临界值，可以将内存的状态数据进行压缩成为snapshot文件并存储在快照目录，这使得快照之前的日志项都可以被舍弃，节约了磁盘空间。我们在上文的状态机更新请求相关逻辑中，发现程序有可能会对日志项进行快照操作即这一行代码逻辑rc.maybeTriggerSnapshot()，那我们来具体了解快照是如何创建的： 1234567891011121314151617181920212223242526272829303132333435func (rc *raftNode) maybeTriggerSnapshot() &#123; // 1. 只有当前已经提交应用的日志的数据达到 rc.snapCount 才会触发快照操作 if rc.appliedIndex-rc.snapshotIndex &lt;= rc.snapCount &#123; return &#125; log.Printf("start snapshot [applied index: %d | last snapshot index: %d]", rc.appliedIndex, rc.snapshotIndex) // 2. 生成此时应用的状态机的状态数据，此函数由应用提供，可以在 kvstore.go 找到它的定义 data, err := rc.getSnapshot() if err != nil &#123; log.Panic(err) &#125; // 2. 结合已经提交的日志以及配置状态数据正式生成快照 snap, err := rc.raftStorage.CreateSnapshot(rc.appliedIndex, &amp;rc.confState, data) if err != nil &#123; panic(err) &#125; // 4. 快照存盘 if err := rc.saveSnap(snap); err != nil &#123; panic(err) &#125; compactIndex := uint64(1) // 5. 判断是否达到阶段性整理内存日志的条件，若达到，则将内存中的数据进行阶段性整理标记 if rc.appliedIndex &gt; snapshotCatchUpEntriesN &#123; compactIndex = rc.appliedIndex - snapshotCatchUpEntriesN &#125; if err := rc.raftStorage.Compact(compactIndex); err != nil &#123; panic(err) &#125; log.Printf("compacted log at index %d", compactIndex) // 6. 最后更新当前已快照的日志索引 rc.snapshotIndex = rc.appliedIndex&#125; // raft.go 需要注意的是，每次生成的快照实体包含两个方面的数据：一个显然是实际的内存状态机中的数据，一般将它存储到当前的快照目录中。另外一个为快照的索引数据，即当前快照的索引信息，换言之，即记录下当前已经被执行快照的日志的索引编号，因为在此索引之前的日志不需要执行重放操作，因此也不需要被WAL日志管理。快照的索引数据一般存储在日志目录下。 另外关于快照的操作还有利用快照进行恢复操作。这段逻辑较为简单，因为快照就代表内存状态机的瞬时的状态数据，因此，将此数据执行反序列化，并加载到内存状态机即可： 12345678910func (s *kvstore) recoverFromSnapshot(snapshot []byte) error &#123; var store map[string]string if err := json.Unmarshal(snapshot, &amp;store); err != nil &#123; return err &#125; s.mu.Lock() s.kvStore = store s.mu.Unlock() return nil&#125; // kvstore.go 至此，raftexmaple主要流程已经简单分析完毕。这是一个简单的应用etcd提供的raft核心库来构建一个 kv 存储的示例，虽然示例的逻辑较为简单，但它却符合前面提到的一点：raft核心库只实现了raft协议的核心部分（包括集群选举、成员变更等），而将日志管理、快照管理、应用状态机实现以及消息转发传输相关逻辑交给应用来处理。这使得底层的raft核心库的逻辑简单化，只要实现协议的核心功能（一致性主义的保证），然后提供与上层应用的接口，并通过channel与上层应用组件交互，如此来构建基于Raft协议的分布式高可靠应用。 参考文献 [1]. etcd-raftexample [2]. etcd-raft示例分析]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式存储</tag>
        <tag>分布式缓存</tag>
        <tag>一致性协议</tag>
        <tag>分布式协调服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单对比 Raft 及 ZAB 协议]]></title>
    <url>%2F2019%2F01%2F08%2F%E7%AE%80%E5%8D%95%E5%AF%B9%E6%AF%94-Raft-%E5%8F%8A-ZAB-%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[如果你了解过Raft协议、ZAB(ZooKeeper&#39;s Atomic Broadcast)协议及Paxos算法，你会发现它们本质上都是为了解决共识问题，即属于一种一致性算法（原子广播协议通常意义上可以等同于一致性协议）。但你可能会觉得相比于Paxos，ZAB与Raft可能更相似。从直观感受上，Paxos协议（Basic Paxos）更像是一种广义上的一致性算法的理论版本，它泛化了很多问题，并且没有基于特定场景的（工程）设计，因此相对而言也更难理解。而ZAB及Raft则像是具化的一致性化算法，并且简化了一些问题的前提设定，这也使得它们更易理解，也更易实现。本文对Raft协议及ZAB协议进行简单理解对比，主要讨论它们的不同之处。考虑到Raft论文给出了关于实现的详细细节，但官方提供的ZAB论文并没有涉及太多实现细节（Andr´e Medeiros 于 2012 年发表了一篇理论结合实践的论文），因此关于ZAB的细节是针对ZooKeeper的实现而言的。 首先，考虑一个问题，为什么需要选举出一个leader？我们知道，在Basic Paxos中并没有强调一定需要一个leader。但在Raft中包含了leader的强领导原则，而ZAB协议，正常的Broadcast阶段也需要一个leader。很自然地，若能够选举出一个leader节点，由其来统筹所有的客户端请求，可以方便并发控制，而且，因为leader是具备最新日志的节点，这使得日志同步过程也变得更简单，单向地由leader流向follower。另外，其实在日志恢复过程中，需要挑选出包含最新日志的节点，如果将它作为leader，那将使得失败恢复过程加快。最后，根本上而言，Raft及ZAB的对日志的应用都差不多归纳为一个二阶段过程，先收集follower反馈，然后，根据特定规则决定是否提交。那么收集反馈的工作若交由leader来处理，明显简化了协议流程。 接下来，我们简述Raft协议与ZAB协议中选举流程的对比情况。明显地，二者都是先选投票给自己，然后广播投票信息，另外它们都包含了选举轮次的概念（在Raft中为任期term，在ZAB中为round，两者的选举过程可能会涉及多轮），这确实比较类似，但需要注意的是，选举完成后，对于Raft而言，term即为leader所在的任期，而ZAB协议却额外使用了一个任期概念(epoch)。在具体的选举过程中，Raft协议规定一旦节点认为它能够为候选者投票，则在此轮投票过程中，都不会改变。而在ZAB协议中，集群中各节点反复交换选票信息（里面包含各自已提交的历史事务日志），以更新选票信息。二者都有quorum选票成功的概念。 与选举流程相关的另一个问题就是如何定义节点包含更新的事务日志。在Raft中，是通过依次比较term及index来确定。而ZAB协议是依次比较epoch及counter来决定（即通过比较zxid），值得注意的是选举轮次round也会作为比较因素。另外，在Raft中有一个很重要的一点为，被选举出来的leader只能提交本term的事务日志（不能显式提交之前term的未提交的事务日志，论文中详细阐述了原因），即在提交当前term的事务日志时，隐式（顺便）提交了之前term的未提交的（但已被复制到quorum节点）事务日志。在ZAB协议中，当leader选举未完成后，不会存在这样的情况，因为在Broadcast阶段之前，Synchronization阶段（Raft协议并未提供此阶段）会保证各节点的日志处于完全一致的状态。 另外，ZAB与Raft协议在选举阶段都使用了超时机制，以保证节点在超时时间内未收到投票信息，会自动转入下一轮的选举。具体而言，Raft的选举流程还可能会出现瓜分选票的情况(split vote)，因此，Raft通过随机化超时(randomized timeout)时间来缓解这个问题（不是解决）。而ZAB协议不会存在瓜分选票的情况，唯一依据是节点的选票的新旧程度。因此，理论上Raft可能存在活性的问题，即不会选举过程不会终止。而ZAB的选举时间应该会比Raft的选举时间更长（更频繁的交换选票信息）。 其次，在ZAB论文中有提到过，follower及leader由Broadcast阶段进入选举阶段，有各自判定依据，或者，这可以表述为，各节点如何触发leader选举过程。明显，在集群刚启动时，节点会先进行选举。另外，Raft协议通过周期性地由leader向follower发送心跳，心巩固leader的领导地位，一旦超时时间内，follower未收到心跳信息，则转为candidate状态、递增term，并触发选举流程（当leader发现消息回复中包含更高term时，便转为follower状态）。而在ZAB协议中，也是通过leader周期性向follower发送心跳，一旦leader未检测到quorum个回复，则会转为election状态，并进入选举流程（它会断开与follower的连接）。而此时follower一旦检测到leader已经卸任，同样会进入election状态，进入选举流程。 如果不幸leader发生了宕机，集群因此重新进行了选举，并生成了新的leader，上一个term并不会影响到当前的leader的工作。这在Raft及ZAB协议中分别可以通过term及epoch来判定决定。那上一任期遗留的事务日志如何处理？典型地，这包含是否已被quorum节点复制的日志。而对于之前term的事务日志，Raft的策略在前文已经叙述，不会主动提交，若已经被过半复制，则会隐式提交。而那些未过半复制的，可能会被删除。而ZAB协议则采取更激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机。 最后，是关于如何让一个新的节点加入协议流程的问题。在Raft中，leader会周期性地向follower发送心跳信息，里面包含了leader信息，因此，此节点可以重构其需要的信息。在ZAB中会有所不同，刚启动后，它会向转入election状态，并向所有节点发送投票信息，因此，正常情况下它会收到集群中其它的follower节点发送的关于leader的投票信息，当然也会收到leader的消息，然后从这些回复中判断当前的leader节点的信息，然后转入following状态，会周期性收到leader的心跳消息。需要注意的一点是，对于Raft而言，一个节点加入协议（不是新机器）不会阻塞整个协议的运行，因为leader保存有节点目前已同步的信息，或者说下一个需要同步的日志的索引，因此它只需要将后续的日志通过心跳发送给follower即可。而ZAB协议中是会阻塞leader收到客户端的写请求。因此，leader向follower同步日志的过程，需要获取leader数据的读锁，然后，确定需要同步给follower的事务日志，确定之后才能释放锁。值得注意的是，Raft的日志被设计成是连续的。而ZAB的日志被设计成允许存在空洞。具体而言，leader为每个follower保存了一个队列，用于存放所有变更。当follower在与leader进行同步时，需要阻塞leader的写请求，只有等到将follower和leader之间的差异数据先放入队列完成之后，才能解除阻塞。这是为了保证所有请求的顺序性，因为在同步期间的数据需要被添加在了上述队列末尾，从而保证了队列中的数据是有序的，从而进一步保证leader发给follower的数据与其接受到客户端的请求的顺序相同，而follower也是一个个进行确认请求（这不同于Raft，后者可以批量同步事务日志），所以对于leader的请求回复也是严格有序的。 最后，从论文来看，二者的快照也略有不同。Raft的快照机制对应了某一个时刻状态机数据（即采取的是准确式快照）。而ZooKeeper为了保证快照的高性能，采用一种fuzzy snapshot机制（这在ZooKeeper博文中有介绍），大概地，它会记录从快照开始的事务标识，并且此时不会阻塞写请求（不锁定内存），因此，它会对部分新的事务日志应用多次（事务日志的幂等特性保证了这种做法的正确性）。 顺便提一下，ZooKeepr为保证读性能的线性扩展，让任何节点都能处理读请求。但这带来的代价是过期数据。（虽然可通过sync read来强制读取最新数据）。而Raft不会出现过期数据的情况（具体如何保证取决于实现，如将读请求转发到leader）。 本文是从协议流程的各个阶段来对比Raft及ZAB协议。这里也提供更系统、更理论、更深入的对比（加入了Viewstamped Replication和Paxos一致性协议），它简要概括了论文。 关于ZAB协议与Paxos的区别，这里便不多阐述了。在ZAB文章中有简略介绍。另外，也可以在这里进行了解。这篇博文主要参考了文献[1]。 参考文献 [1]. Raft对比ZAB协议[2]. Vive La Différence: Paxos vs Viewstamped Replication vs Zab[3]. Van Renesse R, Schiper N, Schneider F B. Vive la différence: Paxos vs. viewstamped replication vs. zab[J]. IEEE Transactions on Dependable and Secure Computing, 2015, 12(4): 472-484.]]></content>
      <categories>
        <category>分布式系统</category>
        <category>一致性算法</category>
      </categories>
      <tags>
        <tag>一致性算法</tag>
        <tag>原子广播协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 ZAB 协议]]></title>
    <url>%2F2019%2F01%2F05%2F%E7%90%86%E8%A7%A3-ZAB-%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[ZAB 协议是应用于 ZooKeeper 分布式协调框架中的可靠原子广播协议(atomic broadcast protocol)（或者称之为全局有序的广播协议totaly ordered broadcast protocol，二者基本等价），这使得ZooKeeper实现了一个主从(primary-backup)模式的架构以通过主服务器接受客户端的数据变更请求，并使用ZAB协议将数据变更请求增量的传播(progpagate)到集群副本节点。在一定程度上，原子广播协议等价于一致性算法(consensus algorithm)，但它们的侧重点有所不同。本质上而言，ZooKeeper依赖于ZAB协议为其它分布式应用提供诸如配置管理、分布式互斥锁以及leader选举等协调原语服务。另一方面，ZooKeeper之所以能提供高可用(highly-available)（比如支持支持崩溃恢复efﬁcient crash-recovery）及高性能(highly-performance)（包括低延迟low latency、高吞吐量good throughput）的协调服务，部分原因是ZAB协议的核心设计（区别于paxos）及工程实现上的优化。大致地，ZAB协议可以分为四个阶段：leader 选举(leader election)、发现(Discovery)、同步(Synchronization)以及广播(Broadcast)，论文中将阶段一与二合并了，ZAB的实际工程实现耦合了阶段二与三（与论文论述并发完全一致），因此也可以称之为三个阶段。 本文主要阐述自己对ZAB协议的理解，这源自于ZAB相关的三篇论文的总结，但并非对原论文的完整翻译，因此更准确、更完整且更正式的内容可以参考原论文。值得注意的是，本论文并非如原论文那般详细、正式且全面地阐述ZAB协议，因此读者最好先阅读原论文，可以参考本文的协议解读。另外，本文不会过多阐述ZooKeeper的关键原理及系统架构，读者有兴趣可以参考文章，以大致了解ZooKeeper协调服务，并从应用层面整体把握ZAB协议。本文先介绍ZAB协议与二阶段提交的关系及与paxos作简单地对比论述。然后按照ZAB协议的四个阶段展开论述。因为本人暂未详细阅读过 Apache ZooKeeper/ZAB的实现源码，因此本文基本不会涉及与实现相关的细节，最后，考虑到本人知识的局限性，如有论述不当之处，谢谢指正！ 在阅读ZAB相关之前，本人已初步了解过raft和paxos这两个一致性算法，如果你有了解过raft或者paxos，那么ZAB也较容易理解。直观上理解，paxos和ZAB都可以视作改进的二阶段提交的协议，因为原始的二阶段（包括三阶段）提交协议因为至少受到网络分区影响而不能称被直接应用于分布式系统构建。实际上，ZAB协议本质上是一个简化的二阶段协议，从协议构成的阶段形式上看，leader首先提出一个请求(称之为request或者proposal)，等待follower对请求的投票结果返回，最后综合投票结果以提交请求。但相比原始的二阶段提交，ZAB中follower（或者称backup，协议不同阶段的不同称呼）不会abort来自leader的请求，具体地，它只要么接受(acknowledge)leader的proposal，要么放弃此leader，重新进入新的一轮选举。另外，避免abort操作也意味着在ZAB协议中，leader提交请求并不需要经集群中所有的follower的同意，即只要quorum个follower给leader返回了ACK，则leader即请求已经在集群中达成一致。简化的二阶段提交也使得ZAB不得不面临leader失败的情况，因此，ZAB整个协议流程中必须考虑如何从leader失败中恢复的问题。在二阶段提交中，如果协调者失败，可以选择abort事务（准确而言是三阶段，在这里我们并不作严格区分）。 那么对比于paxos算法，ZAB协议有什么优势（即利用ZAB可以方便、正确且高效实现或满足，但paxos则不能达到此要求）？这包括两个方面：其一，ZAB协议允许客户端并发地发送请求消息，换言之，ZAB（ZAB的primary）能够同时处理若干个消息请求，并能保证请求消息以客户端提出的顺序（请求消息的FIFO顺序）被广播到backup节点。事实上，ZAB的能够提供这样的保证的原因是，ZAB中所有的请求消息 （准确而言，所有的写请求消息，因为只有写请求消息才需要被广播，以保持数据的一致性）都由ZAB中的（唯一一个）primary进行广播。因此，ZAB需要保证协议的始终只存在一个primary节点。然而，paxos协议却不能简单直接地保证此属性。简单而言，在paxos协议中，若各primary并发地提出请求（请求之间遵循一定的依赖关系，即只能按照其提出的顺序应用到集群），那么learner并不能保证按照primary提出事务请求的顺序来学习（应用）消息请求。虽然可以一次性将多个proposal进行打包形成一个单独的proposal，即对这些请求进行批处理，但这会影响到整个算法的性能，而且单个打包的proposal数量也不能简单求得。 其二，ZAB协议被设计成能够迅速从失败（可能是由于leader或follower崩溃或者网络故障而断连）中恢复，即efficient recovery。ZAB使用事务标识机制(trasaction identification scheme)来全局排序事务日志，并保证准leader(prospective leader)能够容易获知需要同步或截断的日志项。详细而言，ZAB采用&lt;value, (epoch|counter)&gt;来唯一标识一条事务日志，其中value为事务日志的内容。epoch（也被称为是instance）为leader的任期，每一个任期内保证只存在一个leader，每当重新进入leader选举时，需要递增此任期，事实上，任期可用于保证当上一任的leader失败重启后不会干扰到当前任期的leader的广播操作（这同raft类似，都采用了epoch以在一段逻辑时间内唯一标识leader）。counter为事务消息计数器，每次重新选举时，需要清空counter，此值随着客户端发送的请求消息而递增。epoch与counter各占 32 位以构成事务的zxid，即作为事务日志的标识。这提供了一种简单且方便的方式来比较事务日志的新旧：先比较epoch，epoch越大，日志越新，当epoch相等时，比较counter，counter越大，日志越新。在此种事务日志标识机制下，只有具备了最新的事务日志的节点才允许将其日志项拷贝到准leader。换言之，准leader只需从各节点返回的所有的日志中选择包含最新的日志的节点，以从此节点拷贝其缺失的事务日志（若需要的话）（需要注意的是，事实上这属于Discover阶段中的协议内容，若把此阶段的协议归并到leader选举中，则选举算法阶段会直接选择包含最新的事务日志的节点作为准leader，因此避免了准leader去包含最新的日志项的节点去拷贝操作）。而paxos协议并未要求失败恢复的高效执行。详细地，在其恢复阶段，只凭借拥有最大的日志编号（在paxos中proposer提出的每一条日志都有一个全局唯一的编号）并不能要求其对应的值被新的leader接受(accpet)（更多可以参考paxos论文或者这里 ），因此，新的leader必须为其缺少的日志编号所对应的日志项重新执行paxos协议阶段一的协议内容。 另外值得注意的是，ZAB采用了TCP（可靠的）作为节点之间的通信协议，因此避免了部分网络故障问题（如消息乱序、重复及丢失），TCP协议能够保证消息能够按照其发出的顺序(FIFO)达到目标节点。但paxos和raft协议并不依赖此条件。 在介绍ZAB协议的各阶段前，先简要声明一些术语。在ZAB协议中，每个节点可能处于三种状态中的一种：following、leading及election。所有的leader和follower都会依次循环执行前述的三个阶段：Discover（发现集群中全局最新的事务）、Synchronization（由leader向follower同步其缺失的事务日志）及Broadcast（由leader向follower广播复制客户端的事务日志），且在阶段一之前，节点处于election状态，当它通过执行leader选举流程后，它会判断自己是否有资格成为leader（收到quorum张选票），否则成为follower，我们暂且将leader选举作为协议的第零个阶段。显然，正常情况下，协议只循环在Broadcast阶段中执行，一旦发生follower与leader断连，则节点自动切换到选举阶段。在节点进入Broadcast前，必须保证集群的数据处于一致的状态。另外，在本文中节点、机器或者server同义；请求日志、事务日志、提案及日志命令等也作同义处理（不严谨，但读者需明白它们的细微区别）。下面各阶段涉及的术语： − history: 已被节点所接收的提案日志信息− acceptedEpoch: 接收到的最后一个NEWEPOCH消息的epoch（由准leader生成的epoch）− currentEpoch: 接收到的最后一个NEWLEADER消息的epoch（旧的leader的epoch）− lastZxid: history中最后一个（最新的）事务提案的Zxid编号 Leader Election在leader选举阶段，所有节点的初始状态为election，当选举结束后，节点将选举的结果持久化。在此阶段，若节点p给节点q投票，则节点q称节点p的准leader(prospective leader)，直至进入阶段三Broadcast，准leader才能被称为正式的leader(estabilshed leader)，同时它也会担任primary的角色（这样设计有许多优点）。ZAB协议中，leader与primary的称呼基本表示同一个节点，只不过它们是作为同一节点不同阶段（承担不同功能）的称呼。在leader选举过程中，所有的节点最开始都会为自己投票，若经过若干轮的投票广播后，发现自己不够”资格”成为leader时，就会转入following的状态，否则转为leadering状态。leader选举阶段需要为后面的阶段(Broadcast)提供一个后置条件(postcondition)，以保证在进入Broadcast阶段前，各节点的数据处于一致的状态，所谓的postcondition可以表述为leader必须包含所有已提交(commit)的事务日志。 前文提到，部分leader选举实现会直接选择包含最新的日志的节点作为准leader，FLP(Fast Leader Election)正是这样一种选举算法的实现。它通过选择包含有最大的lastZxid（历史日志中最后一条日志记录的zxid）值的节点作为准leader（因为具有最大lastZxid日志的节点必定具有最全的历史日志提交记录），这可以为后阶段的事务广播提供postcondition保证，FLE由若干轮(round)选举组成，在每一轮选举中，状态为election节点之间互相交换投票信息，并根据自己获得的选票信息(发现更好的候选者)不断地更新自己手中的选票。注意，在FLE执行过程中，节点并不会持久化相关状态属性（因此round的值不会被存盘）。 − recvSet: 用于收集状态为election、following及leading的节点的投票信息− outOfElection: 用于收集状态为following及leading的节点的投票信息（说明选举过程已完成） 具体的选举的流程大致如下（更详细的流程可以参考论文)： 一旦开始选举，节点的初始状态为election，初始化选举超时时间，初始化recvSet及outOfElection。每个节点先为自己投票，递增round值，并把投票(vote包含节点的lastZxid及id)的消息（notification包含vote, id, state及round）广播给其它节点，即将投票信息发送到各节点的消息队列，并等待节点的回复，此后节点循环从其消息队列中取出其它节点发送给它的消息： 若接收到的消息中的round小于其当前的round，则忽略此消息。 若接收到的消息中的round大于节点当前的round，则更新自己的 round，并清空上一轮自己获得的选票的信息集合recvSet。此时，如果消息中的选票的lastZxid比自己的要新，则在本地记录自己为此节点投票，即更新recvSet，否则在本地记录为自己投票。最后将投票信息广播到其它节点的消息队列中。 如果收到的消息的round与节点本地的round相等，即表示两个节点在进行同一轮选举。并且若此消息的state为election并且选票的lastZxid比自己的要新，则在本地记录自己为此节点投票，并广播记录的投票结果。若消息的提案号比自己旧或者跟自己一样，则记录这张选票。 整个选举过程中（节点的状态保持为election，即节点消息队列中的消息包含的状态），若节点检测到自己或其它某个节点得到超过集群半数的选票，自己切换为leading/following状态，随即进入阶段二(Recovery)（FLE选举后，leader具备最新的历史日志，因此，跳过了Discovery阶段，直接进入Synchronization阶段。否则进入Discovery阶段）。 另外，如果在选举过程中，从消息队列中检索出的消息的状态为following或者leading，说明此时选举过程已经完成，因此，消息中的vote即为leader的相关的信息。 具体而言，如果此时消息中的round与节点相同，先在本地记录选票信息，然后若同时检测到消息中的状态为leading，则节点转为following状态，进入下一阶段，否则若非leading状态，则需检查recvSet来判断消息中的节点是否有资格成为leader。 否则，如果round不同，此时很有可能是选举已经完成。此时节点需要判断消息被投票的节点（有可能为leader）是否在recvSet或outOfElection字典中具备quorum张选票，同时，还要检查此节点是否给自己发送给投票信息，而正式确认此节点的leading状态。这个额外的检查的目的是为了避免这种情况：当协议非正常运行时，如leader检测到与follower失去了心跳连接，则其会自动转入election状态，但此时follower可能并没有意识到leader已经失效（这需要一定的时间，因为不同于raft，在ZAB协议中，leader及follower是通过各自的方式来检测到需要重新进行选举过程）。如果在follower还未检测到的期间内，恰好有新的节点加入到集群，则新加入的节点可能会收到集群中quorum个当前处于following状态的节点对先前的leader的投票（此时它已转入election状态），因此，此时仍需要此新加入的节点进行额外的判断，即检查它是否会收到leader发给它的投票消息（如果确实存在）。 最后，补充一点，ZAB的选举过程同样加入了超时机制（且很可能并非线性超时），以应对当节点超时时间内未收到任何消息时，重新进入下一轮选举。 DiscoveryDiscovery阶段的目的是发现全局（quorum个也符合条件）最新的事务日志，并从此事务日志中获取epoch以构建新的epoch，这可以使历史epoch的leader失效，即不再能提交事务日志。另外，一旦一个处于非leadering状态节点收到其它节点的FOLLOWERINFO消息时，它将拒绝此消息，并重新发起选举。简而言之，此阶段中每一个节点会与它的准leader进行通信，以保证准leader能够获取当前集群中所包含的被提交的最新的事务日志。更详细的流程阐述如下： 首先，由follower向其准leader发送FOLLOWERINFO（包含节点的accpetedEpoch）消息。当leader收到quorum个FOLLOWERINFO消息后，从这些消息中选择出最大的epoch值，并向此quorum个follower回复NEWEPOCH（包含最大的epoch）消息。接下来，当follower收到leader的回复后，将NEWEPOCH中的epoch与其本地的epoch进行对比，若回复消息中的epoch更大，则将自己本地的accpetedEpoch设置为NEWEPOCH消息中的epoch值，并向leader回复ACKEPOCH（包含节点的currentEpoch，history及lastZxid）消息。反之，重新进入选举阶段，即进入阶段零。当leader从quorum个节点收到follower的ACKEPOCH消息后，从这些ACKEPOCH消息中(history)查找出最新的（先比较currentEpoch，再比较lastZxid）历史日志信息，并用它覆盖leader本地的history事务日志。随即进入阶段二。 SynchronizationSynchronization阶段包含了失败恢复的过程，在这个阶段中，leaer向follower同步其最新的历史事务日志。简而言之，leader向follower发送其在阶段一中更新的历史事务日志，而follower将其与自己本地的历史事务日志进行对比，如果follower发现本地的日志集更旧，则会将这些日志应用追加到其本地历史日志集合中，并应答leader。而当leader收到quorum个回复消息后，立即发送commit消息，此时准leader(prospective leader)变成了正式leader(established leader)。更详细的流程阐述如下： 首先由准leader向quorum发送NEWLEADER（包含阶段一中的最大epoch及history），当follower收到NEWLEADER消息后，其对比消息中的epoch与其本地的acceptedEpoch，若二者相等，则更新自己的currentEpoch并且接收那些比自己新的事务日志，最后，将本地的history设置为消息中的history集合。之后向leader回复ACKNEWLEADER消息。若leader消息中的epoch与本地的不相等，则转为election状态，并进入选举阶段。当leader收到quorum个ACKNEWLEADER消息后，接着向它们发送COMMIT消息，并进入阶段三。而follower收到COMMIT消息后，将上一阶段接收的事务日志进行正式提交，同样进入阶段三。 事实上，在有些实现中，会对同步阶段进行优化，以提高效率。具体而言，leader实际上拥有两个与日志相关的属性（在前述中，我们只用了history来描述已提交的事务日志），其一为outstandingProposals：每当leader提出一个事务日志，都会将该日志存放至outstandingProposals字典中，一旦议案被过半认同了，就要提交该议案，则从outstandingProposals中删除该议案；其二为toBeApplied：每当准备提交一个议案，就会将该议案存放至toBeApplied中，一旦议案应用到ZooKeeper的内存树中了，就可以将该议案从toBeApplied集合中删除。因此，这将日志同步大致分为两个方面： 一方面，对于那些已应用的日志（已经从toBeApplied集合中移除）可以通过不同的方式来进行同步：若follower消息中的lastZxid要小于leader设定的某一个事务日志索引(minCommittedLog)，则此时采用快照会更高效。也存在这样一种情况，follower中包含多余的事务日志，此时其lastZxid会大于leader的最新的已提交的事务日志索引(maxCommittedLog)，因此，会把多余的部分删除。最后一种情况是，消息中的lastZxid位于二个索引之间，因此，leader需要把follower缺失的事务日志发送给follower。当然，也会存在二者存在日志冲突的情况，即leader并没有找到lastZxid对应的事务日志，此时需要删除掉follower与leader冲突的部分，然后再进行同步。 另一方面，对于那些未应用的日志的同步方式为：对于toBeApplied集合中的日志（已提交，但未应用到内存），则直接将大于follower的lastZxid的索引日志发送给follower，同时发送提交命令。对于outstandingProposals的事务日志，则同样依据同样的规则发送给follower，但不会发送提交命令。 需要注意的的，在进行日志同步时，需要先获取leader的内存数据的读锁（因此在释放读锁之前不能对leader的内存数据进行写操作）。但此同步过程仅涉及到确认需要同步的议案，即将需要被同步的议案放置到对应follower的队列中即可，后续会通过异步方式进行发送。但快照同步则是同步写入阻塞。 当同步完成后，leader会几follower发送UPTODATE命令，以表示同步完成。此时，leader开始进入心跳检测过程，周期性地向follower发送心跳，并检查是否有quorum节点回复心跳，一旦出现心跳断连，则转为election状态，进入leader选举阶段。 BroadcastBroadcast为ZAB正常工作所处的阶段。当进入此阶段，leader会调用ready(epoch)，以使得ZooKeeper应用层能够开始广播事务日志到ZAB协议。同时，此阶段允许动态的加入新节点(follower)，因此，leader必须在新节点加入的时候，与这些节点建立通信连接，并将最新日志同步到这些节点。更详细的流程阐述如下： 当leader(primary)收到客户端发送的消息（写）请求value，它将消息请求转化为事务日志(epoch, &lt;value,zxid&gt;), zxid=(epoch|counter)，广播出去。当follower从leader收到事务请求时，将此事务日志追加到本地的历史日志history，并向leader回复ACK。而一旦leader收到quorum个ACK后，随即向quorum节点发送COMMIT日志，当follower收到此命令后，会将未提交的日志正式进行提交。需要注意的是，当有新的节点加入时，即在Broadcast阶段，若leader收到FOLLOWINFO消息，则它会依次发送NEWEPOCH和NEWLEADER消息，并带上epoch及history。收到此消息的节点会将设置节点本地的epoch并更新本地历史日志。 根据在Synchronization提到的两个数据结构outstandingProposals及toBeApplied。因此，事实上，leader会将其提出的事务日志放至outstandingProposals，如果获得了quorum节点的回复，则会将其从outstandingProposals中移除，并将事务日志放入toBeApplied集合，然后开始提交议案，即将事务日志应用到内存中，同时更新lastZxid，并将事务日志保存作缓存，同时更新maxCommittedLog和minCommittedLog。 最后，讨论ZAB协议中两个额外的细节： 若leader宕机，outstandingProposals字典及toBeApplied集合便失效（并没有持久化），因此它们对于leader的恢复并不起作用，而只是在Synchronization阶段（该阶段实际上是leader向follower同步日志，即也可以看成是follower挂了，重启后的日志同步过程），且同步过程包含快照同步及日志恢复。 另外，在日志恢复阶段，协议会将所有最新的事务日志作为已经提交的事务来处理的，换言之，这里面可能会有部分事务日志还未真正提交，而这里全部当做已提交来处理。（这与raft不同，个人认为，这并不会产生太大影响，因为在日志恢复过程中，并不会恢复那些未被quorum节点通过的事务日志，只是在ZAB在提交历史任期的日志的时机与raft不同，rfat不会主动提交历史任期未提交的日志，只在新的leader提交当前任期内的日志时顺便提交历史的未提交但已经复制到quorum节点的日志项）。 需要注意的是，本文使用的一些术语与Yahoo!官方发表的论文[2]可能不一样（个人参照另外一篇论文[4]阐述），但它们的问题意义相同。而且，对于每个阶段，本文先是大概阐述其流程，然后从实际实现的角度进行拓展，希望不要造成读者的困扰。另外，实际工程实现可能并不完全符合这些阶段，而且ZooKeeper各版本的实现也可能会包含不同的工程优化细节。具体参考论文，当然，查看ZooKeeper源码实现可能更清晰。 参考文献 [1] Gray J N. Notes on data base operating systems[M]//Operating Systems. Springer, Berlin, Heidelberg, 1978: 393-481.[2] Junqueira F P, Reed B C, Serafini M. Zab: High-performance broadcast for primary-backup systems[C]//Dependable Systems &amp; Networks (DSN), 2011 IEEE/IFIP 41st International Conference on. IEEE, 2011: 245-256.[3] Reed B, Junqueira F P. A simple totally ordered broadcast protocol[C]//proceedings of the 2nd Workshop on Large-Scale Distributed Systems and Middleware. ACM, 2008: 2.[4] Medeiros A. ZooKeeper’s atomic broadcast protocol: Theory and practice[J]. Aalto University School of Science, 2012, 20.[5] 倪超. 从 Paxos 到 Zookeeper: 分布式一致性原理与实践[J]. 2015.[6]. ZooKeeper的一致性算法赏析]]></content>
      <categories>
        <category>分布式系统</category>
        <category>一致性算法</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>一致性协议</tag>
        <tag>原子广播协议</tag>
        <tag>选举算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 Paxos Made Simple]]></title>
    <url>%2F2018%2F12%2F20%2F%E7%90%86%E8%A7%A3-Paxos-Made-Simple%2F</url>
    <content type="text"><![CDATA[Paxos 算法在分布式系统领域早已是如雷贯耳般的存在，基本成为了分布式一致性协议的代名词，想必对于任何一个从事分布式领域的人来说都充满敬畏——即感叹算法的精巧，也畏惧算法的晦涩。Leslie Lamport 早在 1980s 就写作了描述 Paxos最原始的论文 《The Part-Time Parliament》，但因其难以理解（与论述方式相关?）而没有得到过多的关注（相反，Lamport 本人却坚持认为自己采用了一种更加形象恰当且容易理解的方式阐述，摈弃了传统学术论文的”死板“风格）。在 2001年，Lamport 对 Paxos 论文进行整理简化并发表了《Paxos Made Simple》，引起了广泛关注。论文的第一句话 The Paxos algorithm, when presented in plain English, is very simple 可以体会到 Leslie Lamport 似乎仍旧对众人对 Paxos 冠以难理解性的言行的”不屑“。 最近重新阅读了《Paxo Made Simple》论文，想从论文本身出发，阐述自己对论文的一些（浅显，且可能有误）的理解，因为还未了解Paoxs系列其它论文（如 Fast Paxos），因此个人的理解可能存在一定的局限性。同时，个人坚持认为，反复读原始论文是理解算法的最根本途径，最好结合开源实现进行理解（开源实现一般都会对算法进行工程上的优化与”妥协”）。当然读完原论文可能会有困惑，因此，也可以尝试参考别人的理解（从不同的角度思考问题，或许会有收获），但最终还是要回归论文。如果你对本文有兴趣，你需要先阅读论文。另外，你需要先了解其应用场景。本文先简述其应用场景，然后按照原论文推理的逻辑和步骤来逐步阐述自己对这些步骤的理解。 Paxos 应用场景Paxos用于解决分布式场景的一致性问题。换言之，Paxos是一个一致性（共识）算法。这个说法可能比较笼统宽泛，因为你可能在很多领域了解过一致性问题（虽然这些解释背后的含义可能也存在共性）。比如对于分布式存储，典型的Nosql数据库领域，所谓的一致性可能是要求客户端能够读取其最新写入的数据。换言之，最近写入的数据需要对所后续的客户端的读都可见，强调的是可见性。这可以用线性一致性(Linearizability)来描述；再者，在数据库领域，顺序一致性(serializability)是事务正确性的保证，即强调正确性；而复制状态机(replicated state machine)是很多一致性算法的典型应用场景（包括Paxos），其强调的是让一组互为备份的节点执行一系列相同的命令日志来保证存储在此节点集合中的数据的一致，以达到容错目的。另外，从一致性算法的强弱角度来考虑，一致性算法包括强一致性，弱一致性以及最终一致性。而Paxos则属于强一致性算法。另外，我们再简单了共识算法的正确性的保证： Agreement - all N (or a majority) nodes decide on the same value Validity - the value that is decided upon must have been proposed by some node in N Termination - all nodes eventually decide 这些都容易理解，比如，对于Agreement而言，若某个算法都不难最后表决出来的值是同一个，那就不能称之为共识算法，而Validity可能觉得是很显然的事情，可以从这样一个角度思考，如果所有节点始终回复相同的值，而不管实际提出的值是什么，那么Agreement能够得到保证，但却违反了Validity条件。最后的Termination保证了算法最终能够停止，即我们不仅希望你们能够做表决，也希望能够最终表决出一个结果，否则此表决过程没有意义。而Paxos论文提到的safty requirement 如下： Only a value that has been proposed may be chosen, Only a single value is chosen, and A process never learns that a value has been chosen unless it actually has been. 明确提出了，只保证了前面两点(Agreement及Validity，只是换了一种说法，并颠倒1与2的顺序)，换言之，理论上而言，Paxos是存在活锁的问题，后面会详细阐述。当然Paxos算法只考虑节点存在non-Byzantine及asynchronous网络的条件下。 那么Paxos如何应用于复制状态机呢？简单而言，Paxos试图通过对所有的（客户端发送的）命令日志（如SET X=1）进行全局编号，如果能够全局编号成功，那么互为备份的节点按照此全局编号顺序来执行对应的命令日志，即能够保证数据的一致性。在一个分布式系统中，若执行命令日志序列前，系统处于一致的状态，且节点都执行了相同的命令日志序列，那么最终整个系统也处于一个一致的状态。因此为了保证每个节点都能够以相同的顺序执行命令日志，所有节点必须对于每一条命令日志达成共识（比如，有两个节点尝试提交命令日志，节点a尝试让v=i，而节点b尝试让v=j，明显这会产生冲突，因此需要协调以达成共识，即最终v的值要么是i，那么所有节点都会认为v=a），即每个节点看到的指令顺序是一致的。显然，问题在于不同的节点可能接收到的日志的编号的顺序是不同的，因此不能按照单个节点的意愿进行命令日志的执行（否则会出现数据一致的情况），换言之，所有节点需要相互通信协调，每个节点都对全局编号的排序进行表决。每一次表决，只能对一条命令日志（数据）进行编号，这样才能保证确定的日志执行，这也正是Paxos所做的，即Paxos的核心在于确保每次表决只产生一条命令日志（一个value，这里的命令日志可以表示一个操作，也可以表示一个值）。当然某一次表决成功（达成一致）并不意味着此时所有节点的本地的value都相同，因为可能有节点宕机，即通常而言，只要保证大多数(quorum)个节点存储相同的value即可。 论文理解这里省略了协议的一些基本术语及概念。但还是再强调一下，协议对某个数据达成一致的真正含义提什么，其表示proposer、acceptor及learner都要认为同一个值被选定。详细而言，对于acceptor而言，只要其接受了某个proposal，则其就认定该proposal的value被选定了。而对于proposer而言，只要其issue的proposal被quorum个acceptor接受了，则其就认定该proposal对应的value就被选定了。最后对于learner而言，需要acceptor将最终决定的value发送给它，则其就认定该value被选定了。另外，acceptor是可能有多个的，因为单个acceptor很明显存在单点故障的问题。 我们直接一步步来观察 Lamport 论文中的推导，以达到最终只有一个值被选中的目的（确定一个值），即Only a single value is chosen。这句话很重要，它暗示了不能存在这样的情形，某个时刻v被决定为了i，而在另一时刻v又被决定成了j。 P1. An acceptor must accept the ﬁrst proposal that it receives. 乍一看此条件，让人有点不知所措。论文前一句提到，在没有故障的情况，我们希望当只有一个proposer的时候，并且其只提出一个value时，能够有一个value被选中，然后就引出了P1。这是理所当然的，因为此acceptor之前没有收到任何的value，或许后面也不会收到了，那它选择此value就无可厚非。换言之，此时acceptor并没有一个合适的拒绝策略，只能先选择这个值。但很明显，这个条件远不能达到我们的目的（比如，多个acceptor可能会接受到不同的proposer提出的不同的value，直接导致不同的value被选定，因此不可能只决定一个值）。而且仔细想想，作者提出的这个条件确实比较奇怪，因为你不知道此条件与最终协议的充要条件有什么联系，而且，你可能会想，既然已经选择了第一个值，若后面又有第二个proposal来了应该如何处理（才能保证最终只选择一个值）。直观上我们可能会推断出，每个acceptor只接受一个proposal是行不通的，即它可能会接受多个proposal，那既然会接受多个proposal，这些proposal肯定是不同的（至少是不同时间点收到的），因此需要进行区分衡量，这也正是提案编号proposal id的作用。另外还暗示了一点，正常情况下，对于proposer而言，一个proposal不能由只被一个acceptor接受了就认定其value被选定，必须要由大多数的（即法定集合quorum）选定才能说这个值被选定了。 直观上理解，虽然我们允许了一个acceptor可以accept多个proposal，但为了保证最终只能决定一个value，因此很容易想到的办法是保证acceptor接受的多个proposal的value相同。这便引出了P2： P2. If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v. 为了保证每次只选定一个值，P2规定了如果在一个value已经被选定的情况下，若还有的proposer提交value，那么之后（拥有更高编号higher-numbered）被accept的value应该与之前已经被accept的保持一致。这是一个比较强的约束条件。显然，如果能够保证P2，那么也能够够保证Paxos算法的正确性。 但从另一方面考虑，对比P1与P2，感觉它们有很大的不同，它们阐述的不是同一个问题。P1讨论的是如何选择proposal的问题，而P2则直接跳到了选出来后的问题：一旦value被选定了，后面的被选出来的value应该保持不变。从论文中后面的推断不断增强可以分析出，P2其实包含了P1，两个条件并不是相互独立的，因为P2其实也是一个如何选的过程，只不过它表示了一般情况下应该如何选的问题，而P1是针对第一个proposal应该如何选的问题。换言之，P1是任何后续的推论都需要保证的，后续作出的任何推断都不能与P1矛盾。 注意到，后续若有其它的proposal被选定，前提肯定是有acceptor接受了这个proposal。自然而然，可以转换P2的论述方式，于是就有了P2a： P2a . If a proposal with value v is chosen, then every higher-numbered proposal accepted by any acceptor has value v. P2a其实是在对acceptor做限制。事实上，P2与P2a是一致的，只要满足了P2a就能满足P2。但前面提到过P1是后续推断所必须满足的，而仔细考量P2a，它似乎违反了P1这个约束底线。可以考虑这样一个场景：若有 2 个proposer和 5 个acceptor。首先由proposer-1提出了[id1, v1]的提案，恰好acceptor1~3都顺利接受了此提案，即quorum个节点选定了该值v1，于是对于proposer-1及acceptor1~3而言，它们都选定了v1。而acceptor4在proposer-1提出提案的时候，刚好宕机了（事实上，只要其先接受proposer-2的提案即可，且proposer-2的编号大于proposer-1的编号）而后有proposer-2提出了提案[id2, v2]且id2&gt;id1 &amp; v1!=v2。那么由P1知，acceptor-4在宕机恢复后，必须接受提案[id2, v2]，即选定v2。很明显这不符合P2a的条件。因此，我们只有对P2a进行加强，才能让它继续满足P1所设定的底线。 我们自己可以先直观思考，为了保证acceptor后续通过的proposal的值与之前已经认定的值是相同的。如果直接依据之前的简单流程：proposer直接将其提案发送给acceptor，这可能会产生冲突。所以，我们可以尝试限制后续的proposer发送的提案的value，以保证proposer发送的提案的value与之前已经通过的提案的value相同，于是引出了P2b： P2b. If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v. P2b的叙述同P2a类似，但它强调（约束）的是proposer的issue提案的过程。因为，issue是发生在accept之前，那么accept的proposal一定已经被issue过的。因此，P2a可以由P2b来保证，而且，P2b的限制似乎更强。另外，P1也同时得到满足。 对于P2b这个条件，其实是难以实现。因为直观上，你不能限定各个proposer该issue什么样的proposal，不能issue什么样的proposal。那么又该如何保证P2b呢？我们同样可以先自己主观思考，为了让proposer之后issue的proposal的value与之前已经被通过的proposal的value的值保持一致，我们是不是可以尝试让proposer提前与acceptor进行沟通，以获取之前已经通过的proposal的value呢？具体如何沟通，无非是相互通信，接收消息或者主动询问，接收消息未免显得过于消极，而主动询问显然是更好的策略。如果的确存在这样的value，那为了保证一致，我就不再指定新的value了，与先前的value保持一致即可。而原论文给出了P2c: P2c. For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that either (a) no acceptor in S has accepted any proposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S. 作者认为，P2c里面包含了P2b。P2c中的(a)容易理解，因为如果从来没有accept过编号小于n的提案，那由P1自然而然就可以接受。而对于(b)可以用法定集合的性质简单证明，即两个法定集合(quorum)必定存在一个公共元素。我们可以采用反证法结合归纳法来简单证明。假定编号为m且值为v的提案已经被选定，那么，存在一个法定集合C，C中每一个acceptor都选定了v。然后有编号为n的proposal被提出 ：那么， ① 当n=m+1 时，假设编号为n的提案的value不为v而为w。则根据P2c，存在一个法定集合S，要么S中的acceptor从来没有批准过小于n的提案；要么在批准的所有编号小于n的提案中，编号最大的提案的值为w。但因为S和C至少存在一个公共acceptor，明显两个条件都不满足。所以假设不成立。因此n的值为v。② 当编号m属于m ... (n-1)，同样假设编号为n的提案的value不为v，而为w’ 。则存在一个法定集合S’，要么在S’中没有一个acceptor批准过编号小于n的提案；要么在S’中批准过的所有的编号小于n的提案中，编号最大的提案的值为w’。根据假设条件，编号属于m...(n-1)的提案的值都为v，并且S’和C至少有一个公共acceptor，所以由S’中的acceptor批准的小于n的提案中编号最大的那个提案也属于m...(n-1)。从而必然有w’=v。 若要满足P2c，其实也从侧面反映出若要使得proposer提交一个正确的value，必须同时对proposer和acceptor作出限制。我们现在回顾一下先前的推断的递推关系：P2c=&gt;P2b=&gt;P2a=&gt;P2。因此，P2c最终确保了P2，即当一个value被选定之后，后续的编号更大的被选定的proposal都具有先前已经被选定的value。整个过程，先是对整个结果提出要求形成P2，然后转为对acceptor提出要求P2a，进行转为对proposer提出要求P2b，最后，同时对acceptor及proposer作出要求P2c。 Paxos 算法步骤最后，我们简单阐述一下Paxos算法的步骤。其大致可以分为两个阶段。 阶段一，prepare阶段。 proposer选择一个新的编号n发送给quorum个acceptor，并等待回应。 如果acceptor收到一个针对编号为n的prepare请求，则若此prepare请求的编号n大于它之前已经回复过的proposal的所有编号的值，那么它会 (1) 承诺不再接受编号小于n的proposal。(b) 向proposer回复之前已经接受过的proposal中编号最大的proposal（如果有的话）。否则，不予回应。或者，回复一个error给proposer以让proposer终止此轮决议，并重新生成编号。 阶段二，accept阶段。 如果proposer收到了quorum个acceptor对其编号为n的prepare请求的回复，那么它就发送一个针对[n, v]的proposal给quorum个acceptor（此quorum与prepare阶段的quorum不必相同）。其中，v是收到的prepare请求的响应的proposal集合中具有最大编号的proposal的value。如果收到的响应集合中不包含任何proposal，则由此proposer自己决定v的值。 如果acceptor收到一个针对编号为n的accept请求，则若其没有对编号大于n的prepare请求做出过响应，就接受该proposal。 Paxos 算法活性前面提到，理论上Paxos可能永远不会终止（即永远无法达成一致），即使是在没有故障发生的情况。考虑这样一个场景，proposer-1发起了prepare阶段并获得了大多数acceptor的支持，然后proposer-2立刻带着更高的编号来了，发起了prepare阶段，同样获得了大多数的acceptor的支持（因为proposer-2的编号更高，acceptor只能对prepare请求回复成功）。紧接着proposer-a进入了accept阶段，从acceptor的回复中得知大家又都接受了一个更高的编程，因此不得不选择更大的编号并重新发起一轮prepare阶段。同样，proposer-2也会面临proposer-1同样的问题。于是，它们轮流更新编号，始终无法通过。这也就是所谓的活锁问题。FLP定理早就证明过即使允许一个进程失败，在异步环境下任何一致性算法都存在永不终止的可能性。论文后面提出为了避免活锁的问题，可以引入了一个proposer leader，由此leader来提出proposal。但事实上，leader的选举本身也是一个共识问题。而在工程实现上，存在一些手段可以用来减少两个提案冲突的概率（在raft中采用了随机定时器超时的方式来减小选票瓜分的可能性）。 最后，为了更好地理解Paxos算法时，补充（明确）以下几点。 Paxos算法的目的是确定一个值，一轮完整的paxos交互过程值用于确定一个值。且为了确定一个值，各节点需要协同互助，不能”各自为政”。且一旦接受提案，提案的value就被选定。 Paxos算法的强调的是值value，而不是提案proposal，更加不是编号。提案和编号都是为了确定一个值所采用的辅助手段。显然，当一个值被确定时，acceptor接受的提案可能是多个，编号当然也就不同，但是这些提案所对应的值一定是一样的。 Paxos流程保证最终对选定的值达到一致，这需要一个投票决议过程，需要一定时间。 上面描述的大多流程都是正常情况，但毫无疑问，acceptor收到的消息有可能错位，比如 (1) acceptor还没收到prepare请求就直接收到了accept请求，此时要直接写入日志。(2) acceptor还未返回对prepare请求的确认，就收到了accept请求，此时直接写入日志，并拒绝后续的prepare请求。 因为节点任何时候都可能宕机，因此必须保证节点具备可靠的存储。具体而言，(1) 对于proposer需要持久化已提交的最大proposal编号、决议编号(instance id)（表示一轮Paxos的选举过程）。(2) 对于acceptor需要持久化已经promise的最大编号、已accept的最大编号和value以及决议编号。 参考资料 [1]. Lamport L. Paxos made simple[J]. ACM Sigact News, 2001, 32(4): 18-25.[2]. https://blog.csdn.net/chen77716/article/details/6166675[3]. 如何浅显易懂地解说 Paxos 的算法]]></content>
      <categories>
        <category>分布式系统</category>
        <category>一致性算法</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>一致性算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析分布式事务]]></title>
    <url>%2F2018%2F12%2F11%2F%E6%B5%85%E6%9E%90%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[分布式系统中，将数据块冗余到不同节点使得系统具备容错能力，但其代价是必须要保证各数据副本间的一致性。同样，我们可以将计算（执行）分发到不同节点，以更有效地利用节点并行处理能力，但其代价是必须要对各节点的执行进行协调，以产生应用程序期望的结果。换言之，我们需要推断出节点内部执行的正确性，以保证应用程序可见的语义。而数据库通常能提供涉及事务(transaction)和可序列化(serializability)的相对较强的语义。因此，对于分布式系统而言，此种语义的正确性可以通过分布式事务(distributed transaction)来保证，其通常涉及两个或多个在物理上分离且通过网络连接的主机的数据库事务。 正式而言，分布式事务包含两个方面：并发控制(concurrency control)及原子提交(atomic commit)。并发控制描述事务并发执行的正确性，而原子提交表示事务包含的一组操作要么全部执行，要么全部不执行，这通常与失败(failure)相关。本文会依次阐述分布式事务的并发控制、原子提交相关内容。 并发控制我们以一个例子来展开对并发控制的讨论。考虑一个银行转账的场景：有两个银行账户x和y，且x与y仅次于不同的服务器上，x与y账户初始数目都是 10 。客户端c1将从x账户转账1到y账户，同时，c2是一个审计者以检查银行各账户的钱是否有丢失。因此，抽象化c1及c2的操作为： 1234c1: c2:add(x, 1) tmp1 = get(x)add(y, -1) tmp2 = get(y) print tmp1, tmp2 我们（应用程序）期待最终的结果为：x=11, y=9，同时c2打印 10, 10或者11, 9。但并发执行的操作可能并不会按照应用期待的结果输出。比如，若c2的两个操作完全运行在c1的两个操作之间，导致最终的结果为：x=11, y=9，同时打印11, 10。显然，对于此应用场景而言，我们并不希望出现后者。因此我们需要对并发执行的操作进行协调，以保证其操作结果的语义能够符合应用程序。 先引出一个概念before-or-after atomicity，其定义如下。 Concurrent actions have the before-or-after property if their effect from the point of view of their invokers is the same as if the actions occurred either completely before or completely after one another. 显然，若并发操作能before-or-after atomicity属性，则此转账应用产生的结果是正确的。基于此，我们尝试给出一个能够保证应用程序的正确性的论断。 Coordination among concurrent actions can be considered to be correct if every result is guaranteed to be one that could have been obtained by some purely serial application of those same actions. 可以通过如下几个步骤来认证此观点的正确性：考虑一个系统被应用（可能是并发执行的）操作之后从一个状态转换到另一个状态，如果系统的初始状态是正确的（由具体应用程序确定），并且操作正确地被执行应用到系统，则系统新的状态也是正确的。并且此论述独立于应用程序。同样，如果如果是多个操作并发执行，则上述的论断变更为，若系统最终所处的状态是应用到系统的并发操作集的某个顺序执行后系统的状态，那么此时系统的新的状态也是正确的。换言之，结合before-or-after atomicity属性，可以得出这样的结论：若协调并发操作的规则遵循before-or-after atomicity，则这些并发操作是可序列化的，即存在某些并发事务构成的串行执行顺序，若遵循这些顺序，将导致系统处于相同的终止状态，此时并发操作的结果是正确的。这同样是传统数据事务正确性定义——serializability所要求的。 理论上而言，并发操作的中间过程是不重要的，因为只要保证并发操作所产生的系统新的状态与按照某一个顺序顺序执行所有的“单个”的原子操作所产生的系统新的状态相同，我们并不关心具体与哪一个操作顺序相同，甚至，我们都不要求所谓的顺序操作的中间状态是否真实存在（如图所示，即若操作执行的中间状态的路径是按照虚线进行的），只要此中间状态不会被外部应用程序所观察到，那么我们同样认为这样的操作具备before-or-after atomicity属性，即符合serializability的要求。值得注意的是，对系统应用并发操作的目的是提高性能，但具备before-or-after atomicity属性，或满足serializability的并发操作并不保证系统具备最佳的执行性能。另外，满足serializability特性的并发操作，对编程人员是友好的，因为，我们不必关心并发操作细节。 基于锁的 before-or-after atomicity 属性的实现基于锁来实现事务的并发控制可以分为两个类别：悲观锁(pessimistic)及乐观锁(optimistic)。前者会在操作共享对象之前获取锁，如果在获取锁时，锁已经被其它事务占用了，则必须等待。而后者并不要求在操作共享对象之前获取锁，它会先将对象进行拷贝，然后操作对象，在提交事务的时候检查原始对象是否有被更改过，若没有，则提交事务，否则中止事务，换言之，在获取锁失败时，乐观并发控制(optimistic cocurrency controll)采用的是abort+retry的模式来操作共享对象，因为它没有直接给对象加锁，因此若对象访问没有冲突时，它比悲观锁要快，反之，若在一个充满锁竞争的事务环境下，乐观锁的效果一种会比悲观锁要差。而本节下面提到的基于锁的before-or-after atomicity属性（或serializability）的实现都属于悲观锁的实现。 system-wide lock，即系统级锁。这是基于锁实现的before-or-after atomicity属性最简单的版本。顾名思义，它在系统开始运行时便在内存中创建一个（唯一一个）锁对象，并且必须在事务执行的开始与结束位置插入获取锁与释放锁的代码。显然，system-wide lock一次只允许运行单个事务，它会将所有的事务按照其获取锁的顺序依次执行，不支持事务的并发执行。因为system-wide lock锁住孙事务涉及的所有对象，因此在某些场合其是不必根据，换言之，其锁的粒度（范围）过大。 simple locking，即简单锁。它满足两个规则：其一，每个事务在对某一对象执行实际的读写操作时，必须提前获取此对象的锁。其二，当事务所有操作完成后被提交或者事务被中断时才释放锁。其中，lock point被定义为事务获取其范围内操作所有对象的锁的时刻。而lock set被定义为截止lock point时间点，其所获取的锁的集合。因此，为了保证能正确地协调事务的并发执行，应用程序在执行其每个事务前必须获取事务所对应的lock set，同样，在事务执行完成时释放lock set中的锁。下面简单证明simple locking的策略能够保证before-or-after atomicity。 假定有一个外部观察者维护一个事务标识符的列表，并且一旦某个事务到达其lock point，其标识符就会被添加到此列表，并在事务执行完毕即将释放锁时将其从列表中移除。simple locking能够保证：每个事务都不会在其被添加到列表之前读或写任何对象，并且列表中此事务前面的所有事务都已经通过其对应的lock point。由于任意两个事务lock set不会出现相同的数据对象，因此任何事务的lock set中的数据对象都不会出现在列表中它前面的事务的lock set中，所以也不会出现在列表中更早的事务的lock set中。因此，此事务的输入所涉及的对象内容与列表中的其前一个事务commit（事务顺利完成）或abort（事务中止）后的输出的对象的内容相同。因此，simple locking规则保证此事务before-or-after atomicity属性。 显然，simple locking 所提供的并发粒度也过大，因为，它必须对事务可能涉及到的每一个共享对象加锁，因此它有可能锁住那些原本并不需要的对象。 two-phase locking(2PL)，即两阶段锁。相比于simple locking，它并不要求事务在操作共享对象之前获取其所涉及到的所有对象的锁（准确而言，对于simple locking，一旦事务操作任一共享对象，都需要获取所有对象的锁，而two-phase locking只有等到事务真正操作某一对象时，才去尝试获取此对象对应的锁，因此其锁的粒度要比simple locking要小）。典型地，2PL包括两个过程：1. 扩展锁阶段(expanding phase)，根据操作共享对象的顺序依次获取锁，在此过程中不会有锁被释放。2. 收缩锁阶段(shrinking phase)，锁逐渐被释放，并且在此过程不会尝试获取锁（如果阶段一没有明确的完成标志，那么为了保证事务安全，会等到事务提交或者事务中止时，才会一次性释放所有锁）。但同simple locking类似的是，2PL也允许应用程序并发执行事务，其也会保证所有事务的执行所产生的结果同它们以某一个顺序（到达lock point的顺序）执行所产生的结果相同（因此，2PL有可能导致死锁）。虽然，同simple locking相比，2PL提供更强的事务并发执行能力，但其同样会导致原本允许并发执行的事务的串行顺序执行。参考文献[1]还讨论了当事务执行失败时，锁与日志的交互如何保证事务的顺序执行。 原子提交若构成事务的操作分布在不同机器上，为了确保事务被正确执行，则必须保证事务原子性提交，即分布在不同机器上的事务要么全部执行，要么都不执行。 Two-phase commit(2PC)，两阶段提交。它被用于解决分布式事务原子提交问题（但并没有完全解决）。先简要阐述经典的2PC协议，整个事务由一个事务协调者(transaction coordinator及若干事务参与者(participant)构成，协议的执行大致可以分为如下两个阶段： prepare阶段：客户端向TC发送事务提交请求，TC开始执行两阶段提交。它首先通过RPC向所有的participant发送prepare消息，若participant当前能够执行事务，则向TC回复prepare成功(YES)，并且锁定事务执行所需要的锁与资源，否则回复NO。 commit阶段：若TC收到所有participant回复的YES消息，则开始正式commit事务。它会给所有的participant发送commit消息。participant收到commit消息后，释放事务过程中持有的锁和其他资源，并将事务在本地提交，然后向TC回复commit成功，即YES，否则回复NO。TC收到所有participant回复的commit成功的消息后，向客户端返回成功。反之，一旦TC收到某个participant对preapre消息回复了NO消息，则向所有的participant回复abort消息。 显然，若整个过程无任何故障发生，2PC能够保证分布式事务提交的原子性，因为所有事务参与者对事务的提交都是经由事务协调者来协调决定，因此它们要么全部提交事务要么都不会提交事务。 上述为正常条件下协议执行流程，即没有节点宕机，也没有网络故障。下面讨论若发生失败，会有怎样的情况： 事务参与者宕机，然后重启。若此participant在宕机前对TC的prepare消息回复了YES，那么它必须在宕机前对日志记录。因为其它的participant也有可能同意了prepare消息。具体而言，如果participant重启后，其日志文件记录了prepare的YES消息，但其并没有commit事务，此时它必须主动发消息给TC，或者等待TC重新向它发送commit消息。且在整个过程中，participant必须一直保持对资源及锁的占用。 事务协调者宕机，然后重启。因为TC可能在宕机前对所有协调者发送了commit消息，因此它也必须对此作日志记录。因为或许某个participant已经执行了事务的commit。如果其在重启后，收到了participant的询问消息，必须重新发送commit消息（或者等待一段超时时间后，重新发送commit消息）。 事务协调者一直未收到事务参与者的对prepare消息的回复。可能此时participant已经宕机并且没有重启，也有可能网络发生了故障。因此TC必须设置超时机制，一旦超时未收到回复，则中止事务的提交（因为此时并没有发送commit消息，所有participant都不会提交，保证了事务提交的原子性）。 事务参与者在收到prepare消息前宕机，或超时（一直未收到）。此时，因为participant并没有回复prepare消息（即未对TC作出事务执行的任何承诺），因此其允许单方面中止事务，释放锁及其它资源，此时可能是协议还未开始执行（自然而然，participant的宕机对协议是没有任何影响，直到下一次协议开始执行了，若此participant仍旧处于宕机状态，则将导致abort事务）。 事务协调者在未发送prepare消息前宕机。此时，同上一种情况类似，协议很可能还未开始执行，因此TC的宕机并不影响事务正确性。 事务参与者对prepare消息回复了YES，但是一直未收到commit/abort消息。此时，participant不能单方面中止事务，因为其已经向TC的prepare消息回复了YES，且其它participant也有可能向TC回复了YES，因此TC可能已经向除此participant之外的所有participant发送了commit消息，然后TC发生了宕机。但收到commit消息的participant可能已经commit本地事务。因此，此participant不能单方面abort事务（否则造成事务不一致）。同时，此participant也不能单方面的commit本地事务，因为同样，其它的participant也有可能对TC的prepare消息回复了NO，因此TC在收到所有的prepare消息的回复后，中止了事务的提交。总而言之，若participant对TC的prepare消息回复了YES，则它不能单方面作出任何决定，只能一直阻塞等待TC对事务的决定。 因此，通过上述分述，经典的2PC协议存在明显的局限性： 事务协调者宕机：2PC为一个阻塞式协议，一旦事务协调者宕机，则若有参与者处于执行commit/abort之前的任何阶段，事务进程都将会被阻塞，必须等待事务协调者重启后，事务才能继续执行。 交互延迟：事务协调者必须将事务的commit/abort写日志后才能发送commit/abort‘消息。因此整个过程至少包含2次RPC(prepare+commit)，以及3次日志记录的延迟（prepare写日志+事务协调者状态持久化+commit写日志）。 参考文献 [1] https://en.wikipedia.org/wiki/Two-phase_locking[2] Saltzer J H, Kaashoek M F. Principles of computer system design: an introduction[M]. Morgan Kaufmann, 2009.[3]. 两阶段提交的工程实践]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式事务</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
        <tag>并发控制</tag>
        <tag>原子提交</tag>
        <tag>二阶段提交</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解分布式协调服务 zookeeper]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%90%86%E8%A7%A3%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E6%9C%8D%E5%8A%A1-ZooKeeper%2F</url>
    <content type="text"><![CDATA[ZooKeeper是 Yahoo! 于 2010 年在 USENIX 会议上发表的一篇论文中提出的，被用作分布式应用程序的协调服务(coordination service)。虽然ZooKeeper被认为是 Google Chubby的开源实现，但其设计理念却存在较大差异：ZooKeeper致力于提供一个简单且高性能(high performance)的内核(kernel)以为客户端（应用程序）构建更复杂、更高层(high level)的协调原语(coordination primitives)。换言之，ZooKeeper并不针对特定应用或者具体某一协调服务而设计实现，它只提供构建应用协调原语的内核，而将具体协调原语的构建逻辑放权给客户端，并且，它确保了客户端在不需要更改内核服务的前提下，能够灵活构建出新的、更高级的且更强大的协调原语，比如分布式互斥锁、分布式队列等。ZooKeeper为每个客户端操作提供FIFO顺序保证，并且为所有写操作提供linearlizablity保证。ZooKeeper的实现原理为构建在其之上的服务提供高性能保证。 Zookeeper 为分布式应用提供诸如配置管理(configuration managation)、leader 选举等协调服务，这通过为应用程序提供构建协调原语的 API来实现。并且，与那些提供阻塞原语的服务不同，ZooKeeper实现的 wait-free 数据对象确保其容错和高性能特性，因为若利用阻塞原语来构建协调服务，可能会导致那些慢的(slow)或者有错误的(faulty)的客户端影响正常的客户端的服务性能。此博客阐述个人对ZooKeeper的理解，并从一个ZooKeeper的应用实例开始讨论，分别阐述ZooKeeper两个ordering guarantees、。因为本文并非对原论文的完整翻译，因此你需要提前阅读原论文，确保熟知ZooKeeper数据模型以及客户端API等内容，而且，博客也会省略论文所阐述的利用ZooKeeper来实现部分协调服务部分，具体内容可以参考原论文。 一个应用实例阐述我们知道MapReduce需要知道集群master的ip:port以使得其它节点能够与master建立连接通信，为此，MapReduce可以利用ZooKeeper作为动态配置服务，让master candidate在ZooKeeper上并发注册（创建）各自ephemeral类型的ip:port节点，并让slave监听对应节点的watch event，因此一旦有master candidate注册成功（且只能有一个创建成功），则其它节点将能获取到master的ip:port。 若使用基于raft构建的复制状态机实现，比如在raft集群上构建一个key/value存储系统来存放GFS master的元信息。则整个过程大致如下：首先，master candidate向raft发送Put(&quot;gfs-master&quot;, &quot;ip:port&quot;)命令日志，当raft集群apply此命令日志后，其它节点可通过向raft发送Get(&quot;gfs-master&quot;)命令来获取master的ip:port。但此过程存在几个问题：其一，若多个master candidate同时向raft发送节点地址的注册命令日志，此时将产生race condition，其会导致后发送的命令被应用到状态机，因此master candidate需要进一步判断自己是否成为真正的master（不能仅通过发送了节点地址命令日志来确定）；其二，若master失效，其地址项日志必须要从存储中移除，那么谁来执行此操作？因此，必须对master的元数据信息设置timeout timestamp，并且让master通过定期向raft发送Put(ip:port, timestamp)日志来更新timeout的timestamp，而集群其它节点通过向raft轮询(poll)此timestamp来确保master正常工作，毫无疑问，这将产生大量不必要的poll cost。对比使用ZooKeeper来提供此协调服务（上一段），问题是如何被ZooKeeper高效便捷地解决呢？首先它会确保在多个master candidate同时注册地址信息时，只会有一个操作成功；其次，ZooKeeper的session机制简化了timestamp timeout设置，一旦master宕机，其在ZooKeeper上注册的元信息节点将会自动清除。而且，对应的节点移除消息也会通知到其它节点，避免了slave的大量的轮询消耗。由此可见，使用ZooKeeper来进行集群配置信息的管理，有利于简化服务实现的逻辑。 ZooKeeper 两个 ordering guarantees在讨论ZooKeeper两个基本的ordering guarantees之前，先了解什么是 wait-free，你可以从维基或者 Herlihy的论文 上找到其明确定义： A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. Wait-freedom is the strongest non-blocking guarantee of progress, combining guaranteed system-wide throughput with starvation-freedom. An algorithm is wait-free if every operation has a bound on the number of steps the algorithm will take before the operation completes 而对于ZooKeeper而言，其提供的API被称为是wait-free的，因为ZooKeeper直接响应客户端请求，即此请求的返回并不会受到其它客户端操作的影响（通常是slow或者faulty）。换言之，若此客户端请求为写节点数据操作，只要ZooKeeper收到状态变更，则会立即响应此客户端。如果在这之前某一客户端监听了此节点的数据变更事件，则一旦此节点的数据发生变化，则ZooKeeper会推送变更事件给监听的客户端，然后立即返回给写数据的客户端，并不会等待此监听客户端确认此事件。相比于同步阻塞的调用，wait-free明显提供更好的性能，因为客户端不用同步等待每次调用的返回，且其可以进行异步的批量调用batch call操作，以均摊(amortize)网络传输和IO开销。wait-free的API是ZooKeeper具备高性能的基础，因此也是ZooKeeper的设计核心。 ZooKeeper提供了两个基本的ordering guarantees：Linearizable writes及FIFO client order。Linearizable write表示对ZooKeeper的节点状态更新的请求都是线性化的(serializable)，而FIFO client order则表示对于同一个客户端而言，ZooKeeper会保证其操作的执行顺序与客户端发送此操作的顺序一致。毫无疑问，这是两个很强的保证。 ZooKeeper提供了Linearizable write，那什么是Linearizablility？Herlihy的论文同样给出了其定义，为了方便，你也可以参考这里或者这里。 Linearizability is a correctness condition for concurrent objects that provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object’s operations can be given by pre- and post-conditions. 简单而言，Linearizability是分布式系统领域的概念（区别于数据库领域与事务相关的概念Serializability），一个分布式系统若实现了linearizability，它必须能够保证系统中存在一个时间点，在此时间点之后，整个系统会提交到新的状态，且绝不会返回到旧的状态，此过程是即时的(instantaneous)，一旦这个值被提交，其它所有的进程都会看到，系统的写操作会保证是全局有序(totally ordered)。 而ZooKeeper论文提到其write具备Linearizability，确切而言是A-linearizability(asynchronous linearizability)。简而言之，Linearizability原本（原论文）是针对单个对象，单个操作(single object, single operation)而言的，但ZooKeeper扩大其应用范围，它允许客户端同时执行多个操作（读写），并且保证每个操作同样会遵循Linearizability。 值得注意的是，ZooKeeper对其操作（create,delete等）提供pipelining特性，即ZooKeeper允许客户端批量地执行异步操作（比如发送了setData操作后可以立即调用geData），而不需要等到上一个操作的结果返回。毫无疑问，这降低了操作的延迟(lantency)，增加了客户端服务的吞吐量(throughtout)，也是ZooKeeper高性能的保证。但通常情况下，这会带来一个问题，因为所有操作都是异步的，因此这些操作可能会被重排序(re-order)，这肯定不是客户端希望发生的（比如对于两个写操作而言，re-order后会产生奇怪的行为）。因此，对于特定客户端，ZooKeeper还提供client FIFO order的保证。 ZooKeeper 实现原理同分布式存储系统类似，ZooKeeper也会对数据进行冗余备份。在客户端发送请求之前，它会连接到一个ZooKeeper server，并将后续的请求提交给对应的server，当server收到请求后，有做如下三个保证：其一，若请求所操作的节点被某些客户端注册了监听事件，它会向对应的客户端推送事件通知。其二，若此请求为写操作，则server一次性只会对一个请求做处理（不会同时处理其它的读或者写请求）。其三，写操作最终是交由leader来处理（若接收请求的server并非leader，其主动会对请求进行转发），leader会利用Zab（原子广播协议，ZooKeper atomic broadcast）对此请求进行协调，最终各节点会对请求的执行结果达成一致，并将结果 replica到ensemble servers。ZooKeeper将数据存储到内存中（更快），但为了保证数据存储的可靠性，在将数据写到内存数据库前，也会将数据写到磁盘等外部存储。同时，对操作做好相应的replay log，并且其定期会对数据库进行snapshot。 若请求为读操作，则接收请求的server直接在本地对请求进行处理（因此读操作仅仅是在server的本地内存数据库进行检索处理，这也是ZooKeeper高性能的保证）。正因为如此，同GFS可能向客户端返回过期数据的特点类似，ZooKeeper也有此问题。如果应用程序不希望得到过期数据（即只允许得到最近一次写入的数据），则可以采用sync操作进行读操作前的写操作同步，即如果在读操作之前集群还有pending的写操作，会阻塞直至写操作完成。值得注意的是，每一次的读操作都会携带一个zxid，它表示ZooKeeper最近一次执行事务的编号（关于事务，后面会介绍），因此zxid定义了读操作与写操作之间的偏序关系。同时，当客户端连接到server时，如果此server发现其本地存储的当前zxid小于客户端提供的zxid的大小，其会拒绝客户端的连接请求，直至其将本地数据库同步至全局最新的状态。 在ZooKeeper内部，它会将接收到的写操作转换为事务(transaction)操作。因为ZooKeeper可能需要同时处理若干个操作，因此其会提前计算好操作被提交后数据库所处的状态。这里给出论文中提到的一个事务转换的示例：如果客户端发送一个条件更新的命令setData并附带上目标节点的version number及数据内容，当ZooKeeper server收到请求后，会根据更新后的数据，版本号以及更新的时间戳，为此请求生成一个setDataTXN事务。当事务执行出错时（比如版本号不对应），则会产生一个errorTXN的事务。 值得注意的是，ZooKeeper内部所构建的事务操作是幂等的(idempotent)。这有利于ZooKeeper执行失效恢复过程。具体而言，为了应对节点宕机等故障，ZooKeeper会定期进行snapshot操作，ZooKeeper称其为fuzzy snapshot。但与普通的分布式系统不同的是，它在进行快照时，并不会锁定当前ZooKeeper集群（一旦锁定，便不能处理客户端的写操作，且快照的时间一般也相对较长，因此会降低客户端的服务性能），它会对其树形存储进行深度优先搜索，并将搜索过程中所遍历的每一个节点的元信息及数据写到磁盘。因为ZooKeeper快照期间并没有锁定ZooKeeper的状态，因此在此过程中，若有server在同步写操作，则写操作可能只被replica到部分节点，最终使得snapshot的结果处于不一致的状态。但正是由于ZooKeeper的事务操作是idempontent，因此，在recover过程应用snapshot时，还会重新按顺序提交从快照启动开始到结束所涉及到的事务操作。原论文给出了一个快照恢复过程示例。因此我们会发现，fuzzy snapshot同样是ZooKeeper 高性能的体现。另外，事务幂等的特性也使得ZooKeeper不需要保存请求消息的ID（保存的目的是为了防止对重复执行同一请求消息），因为事务的重复执行并不会导致节点数据的不一致性。由此可见，事务幂等性的大大设计简化了ZooKeeper的请求处理过程及日志恢复的过程。 最后，关于原论文所阐述的基于ZooKeeper内核来构建协调服务的相关实例部分，参考实现代码在这里。 参考文献 [1] Hunt P, Konar M, Junqueira F P, et al. ZooKeeper: Wait-free Coordination for Internet-scale Systems[C]//USENIX annual technical conference. 2010, 8(9).[2] Herlihy M P, Wing J M. Linearizability: A correctness condition for concurrent objects[J]. ACM Transactions on Programming Languages and Systems (TOPLAS), 1990, 12(3): 463-492.[3] https://medium.com/databasss/on-ways-to-agree-part-2-path-to-atomic-broadcast-662cc86a4e5f[4] https://en.wikipedia.org/wiki/Non-blocking_algorithm[5] http://www.bailis.org/blog/linearizability-versus-serializability/]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式协调服务</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式协调服务</tag>
        <tag>分布式锁</tag>
        <tag>原子广播协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 原型实现]]></title>
    <url>%2F2018%2F11%2F16%2FMapReduce-%E5%8E%9F%E5%9E%8B%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[MapReduce 最早是由谷歌于 2004 年在操作系统顶会 OSDI 上发表的一篇面向大规模数据处理的分布式计算框架（并行计算模型）论文中提出。MapReduce使用 Google File System 作为数据存储，支撑起了谷歌全网搜索等大规模数据存储与处理业务。MapReduce 对于大规模数据的高效处理体现在三个方面：其一，大规模数据并行处理，分而治之；其二，MapReduce编程模型；最后，MapReduce运行时环境（失败恢复、任务调度以及负载均衡等）。它简化了并行编程，使得开发人员很容易编写出高效且具备容错能力的并行化程序。 博客基于 MIT 6.824 (2018) 的课程 Lab1。整个实验实现了MapReduce原型，并且对其关键特性进行测试，主要包括MapReduce编程模型，集中在 Map与Reduce两个阶段，以及任务失败处理。在阅读原论文 MapReduce 的基础，Lab1 能够让我们对 MapReduce原理有更为深刻的理解，也能够提高我们实现分布式系统的实践能力，这包括节点通信模型、系统构建框架以及诸如失败恢复机制等。而且，仔细阅读整个 Lab 的代码可以学习到很多原理及设计知识，而不仅仅是完成其 Lab 任务。下文会简单介绍整个 Lab1 框架，然后阐述几个关键点（模块）。 Sequential 及 Distributed 运行模式Lab1 实现了两种不同运行模式的MapReduce原型框架：一种是Sequential运行模式，它顺序编程实现MapReduce过程，也不具备容错功能，因此并非真正意义上的实现。具体地，基于此种运行模式，所有task串行执行且Map与Reduce两个阶段也是串行执行，且未提供任务执行失败的恢复机制。大概地，它首先创建输入文件并读取Map输入，同时创建对应数量的Map task（即循环调用Map函数来处理输入文件），并顺序调度执行，将中间结果写到磁盘上，当所有Map task执行完成后，启动一定数量的Reduce task，并让Reduce task从本地磁盘相应位置读取Map task输出，同样被顺序调度执行，最后，将Reduce task输出写到本地磁盘，最终merge所有输出文件，以合并写到本地输出文件。 另一种是 Distributed运行模式，它更接近真实的MapReduce原型框架实现。客户端会依次启动一个master节点及多个slave节点(go 的goroutine)，并将输入文件信息传给master节点，此后客户端会阻塞等待master返回 。master启动后开始监听slave的连接(one client one goroutine），slave启动后会主动往master节点注册，并等待master分配任务。所有节点通过go rpc实现对等通信。一旦有slave/worker注册成功，master开始实施任务调度，通过rpc将任务信息（任务类型、任务输入文件位置等）发送给worker，而worker在注册成功后，就不断监听master的连接并调用worker的任务执行handler(doTask)， doTask会调用应用程序的Map或Reduce执行MapReduce任务，所有的worker在本节点执行任务的过程同Sequential运行模式下类似，只是各个worker并行执行，互不干扰。值得注意的是，在整个MapReduce Job调度执行过程中，worker允许动态加入，master一旦发现worker注册加入，若此时有未完成的任务等待调度，就会将此任务让新加入的worker调度执行。只有所有的Map task调度完成后，Reduce task才会被调度。当所有Reduce task执行完成后，同样会进行merge的过程，然后从MapReduce框架返回。 Map 及 Reduce 工作流程这里简要阐述 Map &amp; Reduce阶段执行流程。当worker执行map task时，包括以下几个步骤：首先从本地磁盘读取其负责处理的原始输入文件；然后，通过将文件名及文件内容作为参数传递给MapFun来执行用户自定义逻辑；最后，对于每一个Reduce task，通过迭代MapFunc返回的执行结果，并按记录(record)的key进行partition以将分配给对应的Reducer的中间输出结果写到本地磁盘对应文件。 Reduce task的执行过程大致如下：首先读取本Reduce task负责的输入文件，并使用JSON来decode文件内容，并将decode后的kev/value存储到map中，同一个key对应一个value list，然后将整个map的key进行排序，并对每一个key/value list通过调用ReduceFunc来执行用户名自定义逻辑，同时，将其返回的结果，经JSON encode后写入输出文件。这些由Reduce task输出的文件内容，会被merge到最终的输出文件。 再谈失败恢复容错（失败恢复）是MapReduce运行时的一个关键特性。且 Lab1 也模拟实现了任务执行失败后所采取的措施。任务执行失败，典型的包括两种情况：网络分区（网络故障）及节点宕机，且事实上无法很好地区分这两种情形（在两种情形下，master都会发现不能成功ping通 worker）。而实验则是采用阻止worker与master的rpc连接来模拟实现。具体地，所有worker在执行若干个rpc连接请求后（一个rpc连接请相当于一次任务分配），关闭其rpc连接，如此master不能连接worker而导致任务分配执行失败。个人认为，一般情况下会让 master缓存worker的连接handler，并不会在每次发送rpc请求时，都需要执行Dial/DialHttp，若是如此，便不能以原实验的方式来模拟任务执行失败（虽然这可能并不影响）。另外 Lab1 显式禁止了worker同时被分配两个任务的情况，这是显而易见的。 关于失败恢复（节点容错），下面讨论更多细节。容错是MapReduce的一个重要特性，因为节点失效在大数据处理工作中过于频繁，而且当发生节点宕机或者网络不可达时，整个MapReduce job会执行失败，此时MapReduce并不是重启整个job，那样会导致重新提交执行一个庞大的job而耗时（资源）过多，因此它只会重启对应worker所负责执行的task。值得注意的是，正是因为worker并不维护task相关信息，它们只是从磁盘读取输入文件或者将输出写到磁盘，也不存在与其它worker进行通信协调，因此task的执行是幂等的，两次执行会产生相同的执行结果，这也可以说是MapReduce并行执行任务的约束条件之一，也是MapReduce同其它的并行执行框架的不同之处，但无论如何，这样设计使得MapReduce执行任务更为简单。因为Map task会为Reduce task产生输入文件，因此若Reduce task已经从Map task获得了其所需要的所有输入，此时Map的失败，并不会导致其被重新执行。另外关键的是，GFS的atomic rename机制确保即使Map/Reduce task在已经溢写了部分内容到磁盘后失败了，此时重新执行也是安全的，因为GFS会保证直到所有输出写磁盘完成，才使得其输出文件可见，这种情况也会发生在两个Reduce task执行同一个任务，GFS atomic rename机制同样会保证其安全性。那么，若两个Map执行同一个task结果会如何？这种情况发生在，master错误地认为Map task宕机（可能只是发生了网络拥塞或者磁IO过慢，事实上，MapReduce的stragger worker正描述的是磁盘IO过慢的情况），此时即便两个Map task都执行成功（它们不会输出到相同的中间文件，因此不会有写冲突），MapReduce运行时也保证只告诉Reduce task从其中之一获取其输入。最后，注意MapReduce的失败恢复机制所针对的错误是fail-stop故障类型，即要么正常运行，要么宕机，不会产生不正确的输出。 参考代码在这里。 参考文献 [1] Dean J, Ghemawat S. MapReduce: simplified data processing on large clusters[J]. Communications of the ACM, 2008, 51(1): 107-113.[2].MIT 6.824]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式计算框架</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式计算框架</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 The Google File System]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%90%86%E8%A7%A3-The-Google-File-System%2F</url>
    <content type="text"><![CDATA[分布式文件系统是构建整个分布式系统的基石，为分布式计算提供底层数据存储。谷歌早在 2013 年就发表了论文 The Google File System，它在谷歌内部是配合其分布式计算框架MapReduce使用，共同为谷歌搜索等业务提供技术栈支撑。虽然数据量激增以及技术革新使得GFS不断演进，但理解其最初的设计理念、运行原理以及关键实现技术同样让人受益匪浅，并指导着我们实际的学习和工程实践。这篇博文阐述个人对原论文的一些理解与心得，并不是对原论文的完整翻译，因此你需要提前阅读论文。 设计动机与目标设计一个通用的分布式文件系统是不现实的，它不仅在实现上异常困难（因为不得不考虑所有应用场景），而且实际使用也难以满足要求（往往存在显而易见的性能或容错瓶颈）。GFS 设计初衷是利用数以千计的廉价机器为MapReduce提供底层可靠且高性能的分布式数据存储，以应对海量离线数据存储与处理的应用场景，比如存储应用程序持续产生的日志流以提供离线日志分析。由此，其设计目标为容错可靠(fault tolerance)、高性能读写(high-performance read&amp;write)以及节约网络带宽(save bandwidth)。 一致性(consistency)是分布式系统不可回避的问题。对于分布式文件系统而言，为了提供容错，必须维持数据副本(replica），那如何保证各副本间一致显得至关重要，特别是在应用并发访问场合。一致性是个极其宽泛的术语，你可以实现数据的强一致性(strong consistency)以保证用户始终读到的是最新的数据，这对于用户（客户端）而言是个极佳选择，但它提高了系统的实现难度，因为你必须设计复杂的一致性协议（如Paxos或Raft）来实现强一致性，它也会损害系统性能，典型的它需要机器之间通信以对副本状态达成一致。而弱一致性(weak consistency)则几乎相反。因此，必须根据特定的应用场景，在保证系统逻辑正确的提前下，放宽一致性要求，设计具备良好性能且能提供足够的一致性(sufficient consistency)的系统。对于GFS而言，它针对MapReduce应用程序进行了特定优化，比如，对大文件高性能读取、允许出现文件空洞(hole)、数据记录重复(record duplicate)以及偶尔读取不一致(inconsistent reads)。具体在数据读写方面，其侧重于大规模一次性写入和追加写入，而并非覆盖写和随机写；读取同样倾向于顺序读取，并不关心随机读取。 Trade-off 理念哲学GFS在设计上存在大量的trade-off。正如前文所述，你不能企图设计出一个完美的系统，而只能针对具体应用场景作出各方面的权衡考量，以达到工程最佳实践目的。 chunk大小设计。GFS针对大文件存储（数百上千兆）设计，因此若存储大量小文件则不能体现其性能。其默认块大小为 64MB。选择大文件作为存储目标原因如下：首先它减少了client与master的交互次数（即使client并不需要整个数据块，但实际上往往存在“就近原则”）；另外，这也直接减少了网络带宽；最后，它减少了存储在master内存中的元数据(metadata)的大小。但凡事总利弊相随。较大的块大小设定使得小文件也不得不占用整个块，浪费空间。 集群元数据‘存储。在master的内存中存放着三种类型的元数据：文件和chunk的名称空间(namespace)、文件到chunk的映射信息以及chunk副本的位置信息。且前两种元数据会定期通过operation log持久化到磁盘以及副本冗余。为什么将这些元信息存储到内存？一方面，缓存在内存无疑会提高性能，另外它也不会造成内存吃紧，因为每个64MB 的chunk只会占用到 64B 的内存空间（粗略估算普通 2G 内存的机器可以容纳 2PB 数据），而且为机器增加内存的代价也很小。那为什么chunk位置信息没有持久化？首先master在启动的时候可以通过heartbeat从各chunk server获取。另一方面，chunk的位置信息有时会变动频繁，比如进行chunk garbage collection、chunk re-replication以及chunk migration，因此，若master也定期持久化chunk位置信息，则master可能会成为集群性能bottleneck。从另一个角度来看，chunck是由chunk server保存，而且随时可能发生disk failure而导致chunk暂时不可被访问，因此其位置信息也应该由chunk server负责提供。 chunk副本（默认3个）存放策略。chunk副本选择目标机器的原则包括两个方面：一是最大化数据可靠性(reliability)及可用性(availability)，这就要求不能把所有的副本存放在一台机器上，如果此机器的发生disk failure，则数据的所有副本全部不可用。放在同一个机架也类似，因为机架之间的交换机或其它网络设计也可能出现故障。另外一个原则是，最大化网络带宽，如果两个副本的位置相隔太远，跨机架甚至跨数据中心，那么副本的写复制代价是巨大的。因此一般的存放位置包括本机器、同一机架不同机器以及不同机架机器。 垃圾回收。当一个文件被删除，GFS不会真正回收对应的chunk，而只是在log operation记录删除日志后，将对应的文件名设置为隐藏。在一定期限内（默认3天），用户可以执行撤销删除操作。否则，master会通过其后台进程定期扫描其文件系统，回收那些隐藏的文件，并且对应的元数据信息也会从内存中擦除。另外，master的后台进程同时还会扫描孤儿块(orphaned chunk)，即那些不能链接到任何文件的chunk，并将这些chunk的元信息删除，这样在后续的heartbeat中让chunk server将对应的chunk删除。这种垃圾回收机制的优点如下：其一，很明显允许用户作出撤销删除操作。其二，统一管理的垃圾回收机制对于故障频繁的分布式系统而言是便捷且可靠的（系统中很容易出现孤儿块）；最后，也有利于提升系统性能。垃圾回收发生在后台进程定期扫描活动中，此时masetr相对空闲，它不会一次性将大量文件从系统移除，从而导致 IO 瓶颈，换言之，其chunk回收成本被均摊(amortized）。但其同样有缺点：如果系统中一段时间内频繁出现文件删除与创建操作时，可能使得系统的存储空间紧张（原论文中也提供了解决方案）。 一致性模型 和 原子 Record Append前文提到GFS并没有采用复杂的一致性协议来保证副本数据的一致性，而是通过定义了三种不同的文件状态，并保证在这三种文件状态下，能够使得客户端看到一致的副本。三种状态描述如下：GFS将文件处于consistent状态定义为：当chunk被并发执行了操作后，不同的客户端看到的并发执行后的副本内容是一致的。而defined状态被定义为：在文件处于consistent状态的基础上，还要保证所有客户端能够看到在此期间对文件执行的所有并发操作，换言之，当文件操作并发执行时，如果它们是全局有序执行的（执行过程中没有被打断），则由此产生的文件状态为defined（当然也是consistent）。换言之，如果某一操作在执行过程中被打断，但所有的并发操作仍然成功执行，只是对文件并发操作的结果不能反映出任一并发操作，因为此时文件的内容包含的是各个并发操作的结果的混合交叉，但无论如何，所有客户端看到的副本的内容还是一致的，在这种情况下就被称为consistent。自然而然，如果并发操作文件失败，此时各客户端看到的文件内容不一致，则称文件处于undefined状态，当然也处于inconsistent状态。 我们先区分几种不同的文件写类型：write指的是由应用程序在写入文件时指定写入的offset；而append同样也是由应用程序来指定写入文件时的offeset，只是此时的offset默认为文件末尾；而record append则指的是应用程序在写入文件时，只提供文件内容，而写入的offset则由GFS来指定，并在写成功后，返回给应用程序，而record append操作正是GFS提供一致性模型的关键，因为它能够保证所有的record append都是原子的(atomic)，并且是at least once atomically。这一点并非我们想像的简单，其所谓的at least once atomic，并不表示采用了atomic record append后，即使在客户端并发操作的情况，也能保证所有的副本完全相同(bytewise idetical)，它只保证数据是以原子的形式写入的，即一次完整的从start chunk offset到end chunk offset的写入，中间不会被其它操作打断。且所有副本被数据写入的chunk offset是相同的。但存在这种情况，GFS对某一副本的执行结果可能会出现record duplicate或者inset padding，这两种情况的写入所占居的文件区域被称为是inconsistent。而最后为了保证应用程序能够从所有副本看到一致的状态，需要由应用程序协同处理。 如果文件的并发操作成功，那么根据其定义的一致性模型，文件结果状态为defined。这通过两点来保证：其一，对文件的副本应用相同的客户端操作顺序。其二，使用chunk version number来检测过期(stale)副本。 record append操作流程如下：客户端首先去请求master以获取chunk位置信息，之后当客户端完成将数据 push 到所有replica的最后一个chunk后，它会发送请求给primiary chuck server准备执行record append。primary首先为每一个客户端操作分配sequence number，然后立即检查此次的record append操作是否会使得chunk大小超过chunk预设定的值（64MB），若超过了则必须先执行insert padding，并将此操作命令同步给所有副本chunk server，然后回复客户端重新请求一个chunk并重试record append。如果未超过chunk阈值，primary会选择一个offset，然后先在本地执行record append操作，然后同样将命令发送给所有副本chunk server，最后回复写入成功给客户端。如果副本chunk server在执行record append的过程中宕机了，则primary会回复客户端此次操作失败，要求进行重试。客户端会请求master，然后重复上述流程。此时，毫无疑问会造成副本节点在相同的chunk offset存储不同的数据，因为有些副本chunk server可能上一次已经执行成功写入了所有数据(duplicate record)，或者写了部分数据(record segment)，因此，必须先进行inset padding，使得各副本能够有一个相同且可用的offset，然后才执行record append。GFS将这种包含paddings &amp; record segments的操作结果交由应用程序来处理。 应用程序的writer会为每个合法的record在其起始位置附加此record的checksum或者一个predictable magic number以检验其合法性，因此能检测出paddings &amp; record segments。如果应用程序不支持record duplicate（比如非幂等idempotent操作），则它会为每一个record赋予一个unique ID，一旦发现两个record具有相同的ID它便认为出现了duplicate record。由GFS为应用程序提供处理这些异常情况的库。 除此之外，GFS对namespace的操作也是原子的（具体通过文件与目录锁实现）。 我们再来理解为什么GFS的record append提供的是at least once atomically语义。这种一致性语义模型较为简单（简单意味着正确性易保证，且有利于工程实践落地，还能在一定程度上提升系统性能），因为如果客户端写入record失败，它只需要重试此过程直至收到操作成功的回复，而server也只需要正常对等待每一个请求，不用额外记录请求执行状态（但不表示不用执行额外的检查）。除此之外，若采用Exactly-once语义模型，那将使整个实现变得复杂：primary需要对请求执行的状态进行保存以实现duplicate detection，关键是这些状态信息必须进行冗余备份，以防primary宕机。事实上，Exactly-once的语义模型几乎不可能得到保证。另外，如果采用at most once语义模型，则因为primary可能收到相同的请求，因此它必须执行请求duplicate detection，而且还需缓存请求执行结果（而且需要处理缓存失效问题），一旦检测到重复的请求，对客户端直接回复上一次的请求执行结果。最后，数据库会采用Zero or once的事务语义(transactional semantics)模型，但严格的事务语义模型在分布式场景会严重影响系统性能。 延迟 Copy On Write快照(snapshot)是存储系统常见的功能。对于分布式系统而言，一个关键挑战是如何尽可能地降低snapshot对成百上千的客户端并发访的性能影响。GFS同样采用的是copy on write技术。事实上，它延迟了snapshot的真正执行时间点，因为在分布式系统中，副本是必须的，大多数情况下，快照涉及的副本可能不会被修改，这样可以不用对那些副本进行 copy，以最大程度提升系统性能。换言之，只有收到客户端对快照的副本执行mutations才对副本进行 copy，然后，将客户端的mutations应用到新的副本。具体的操作流程如下：当master收到客户端的snapshot指令时，首先会从primary节点revoke相应chunk的lease（或者等待lease expire），以确保客户端后续对涉及snapshot的chunk的mutations必须先与master进行交互，并对这些操作执行log operation，然后会对涉及到的chunk的metadata执行duplicate操作，并且会对chunk的reference count进行累加（换言之，那些chunk reference count大于1的chunk即表示执行了snapshot）。如此一来，当客户端发现对已经快照的chunk的操作请求时，master发现请求的chunk的reference count大于1。因此，它会先defer客户端的操作请求，然后选择对应chunk的handler并将其发送给对应的chunk server，让chunk server真正执行copy操作，最后将chunk handler等信息返回给客户端。这种delay snapshot措施能够改善系统的性能。 最后，值得注意的是，虽然客户端并不缓存实际的数据文件（为什么？），但它缓存了chunk位置信息，因此若对应的chunk server因宕机而miss了部分chunk mutations，那客户端是有可能从这些stale的replica中读取到premature数据，这种读取数据不一致的时间取决于chunk locations的过期时间以及对应的文件下一次被open的时间（因为一旦触发这两个操作之一，客户端的cache信息会被purge）。 参考文献： [1] Ghemawat S, Gobioff H, Leung S T. The Google file system[M]. ACM, 2003.[2].MIT 6.824 Lecture]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式文件系统</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式文件系统</tag>
        <tag>GFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式互斥算法解析与实现]]></title>
    <url>%2F2018%2F11%2F09%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[资源共享非常普遍，在单机系统中，进程间对共享资源的互斥访问可以通过互斥锁、信号量以及进程间通信等机制来实现。而在分布式系统中，也不可避免多个节点共享某一资源或同时执行某一函数，比如全局配置文件，因此分布式互斥算法必须保证任何时刻只允许一个进程访问资源或执行临界区(critical section)代码，即互斥算法的安全性，有些场景也有公平性要求。另外，好的互斥算法应该能尽可能降低消息带宽(message overhead)，减少进程（节点）等待时间，即时延(latency)，无系统瓶颈(bottleneck)，也能容忍消息乱序。 分布式互斥算法分为集中式算法(centralized algorithm)和分布式算法(distributed algorithm)，而分布式算法又包括了基于令牌的算法(token based algorithm)以及基于请求的算法(permission based algorithm)。无论基于何种原理实现，一般而言，理想的分布式互斥算法需要保证以下目标： 安全性，即任何时刻只能有一个进程访问共享资源，即持有互斥锁。 公平性，有些场景需要尽量保证访问共享资源的公平性，这表明：系统不能出现死锁，任何进程持有锁的时间是有限的，任何等待的进程最终都能获取锁，以及等待获取锁的进程的等待时间是有限的。 低带宽，即尽量减少消息传输的数目。 低延迟，即进程进入临界区之前的等待的时间。 动态性，即允许进程在任何时刻加入到访问共享资源的进程集合中，或者从其中退出。 容忍进程失败，即允许访问共享资源的进程集合中的进程因失败而退出，而保证整个系统不受影响。 容忍消息丢失，即在消息不能按时到达、乱序甚至丢失的情况下，整个系统依然正常工作。 在本文我们讨论前四个要求，假设进程数目是确定的，没有进程会失败，消息也不会丢失。下面我们通过简要阐述算法原理以及实现关键点来依次介绍Centralized Mutual Server算法、Ricart Agrawala算法、Lamport Distributed Mutual Exclusion算法以及Token Ring算法。 Centralized Mutual Server顾名思义，Centralized Muutal Server为集中式的互斥算法。整个系统内部包括两种消息：请求(reqeust)消息、授权(grant)消息以及释放(release)消息。核心数据结构为一个请求消息队列。算法核心为：它选取一个进程(centralized server)作为协调者，负责对名进程的请求进行即时或推迟(defer)授权。它内部维护一个互斥锁锁请求队列，当收到请求消息时，如果队列为空，则直接授权，否则将其加入到队列中。当收到释放消息时，如果列队不为空，则从队列中取出一个请求并授权响应。算法公平性依赖于队列实现，如使用FIFO则能够保证各个进程的锁请求消息能够被公平地授权。消息带宽为3(1 request, 1 grant, 1 release)，即在某一进程从准备进入临界区到退出临界区所传输消息的数量。很明显，集中式互斥算法的缺点是协调者的瓶颈。 集中式互斥算法最容易实现。在网络通信层，可以采用基于TCP的client-server通信模型。关于协调者的实现，你可能需要关注当前是否已经授权了锁请求。同时，如果有必要，注意单进程内部锁的使用。 Ricart AgrawalaRicart &amp; Agrawala算法是在1981年被提出的一个基于请求的分布式互斥算法。它基于lamport clock，即依赖于全局有序的逻辑时钟。整个系统内部包括两种消息：请求(reqeust,i,ts)消息与回复(reply,j)消息。核心数据结构包括缓存其它进程回复消息的队列(replyQueue)以及缓存推迟回复进程请求消息队列(deferQueue)。算法核心为：当进程i准备进入临界区时，必须发送一个带（逻辑）时间戳的请求消息给其它所有进程，当其收到了其它所有进程的对此请求的回复（响应）时，则进入临界区。但如果某一进程j在收到进程i的请求之前，发出了一个更早的请求消息，则它会将此进程(i)的请求消息放入到延迟队列(deferQueue)，并且先执行完临界区的代码，当准备退出临界区时，才发送请求响应给进程(i)。算法的公平性容易保证。而消息带宽为{request: n-1, reply: n-1} =&gt; 2(n-1)，其中n为进程数。 Ricart Agrawala算法的相比集中式算法在实现上更为复杂。同样在通信层，则不能构建one server, muliti-client模型，而采用peer to peer模型，因为所有进程都是对等的，即同时充当server与client，而且作为一种简化实现，所有进程在启动后，应该互相建立连接。除此之外，你需要实现（模拟）lamport clock 算法，否则互斥算法的正确性不能得到保证，注意对于某些消息（如reply）的发送事件，虽然可以更新消息时间戳，但其实不影响算法正确性。 Lamport Distributed Mutual ExclusionLmpoart Distributed Mutual Exclusion算法于1978年由 Lamport 在关于lamport clock理论论文中提出，其作为lamport clock的实际应用，因此，显然其依赖于lamport clock。事实上，此算法不仅可以作为分布式互斥算法，其内部的请求优先级队列也能作为分布式节点副本一致性的实现参考模型。但原论文提出的互斥算法基于消息按顺序到达的假设，解释如下： 比如，进程i在时间片1发送锁请求a，但因为网络原因被极端延时了。而且在其它进程收到进程i发送的请求消息a之前，进程j在时间片5发送了请求b，而请求b恰好被顺利传输，很快被其他进程接收，并且其他进程（包括进程i）立刻发送了对请求消息b的回复消息，同时回复消息也立刻被进程j接收，但此时进程j仍未收到进程i的请求消息a，所以进程j以为自己成功获取到锁（收到了其它所有进程对请求消息b的回复）。而事实上，进程i的请求消息a要比进程j的请求消息b更早发送，因此应该是进程i先获取锁。其根本原因在于，进程i在收到其他节点请求消息（进程j的请求消息b）时，没有进行额外检查，理论上它需要判定自己是否在更早前发出过请求消息，而不只是直接对请求消息回复，即使最后其在请求消息队列里移除的消息是它自己的请求消息（因为自己是请求消息是最早的）。但这造成了整个系统的不正确性。 因此，改变Lmpoart Distributed Mutual Exclusion算法在接收请求消息后发送回复消息的条件，消除了消息按序到达的假设，但同时也使得变更后的算法更为复杂。 系统内部包括三种消息：请求(reqeust,i,ts)消息、回复(reply,j)消息以及释放(release)消息。核心数据结构包括缓存其它进程回复消息的队列(replyQueue)、缓存推迟回复进程请求消息队列(deferQueue)以及一个以时间戳为依据的请求消息优先级队列(requestPriorityQueue)。算法变更的核心为：进程i在收到进程j的请求消息(request, j, t)时，（条件1）先判断自己是否发送过更早的请求消息，（条件2）并且未收到进程j针对此请求消息的回复消息。如果二者之中任一个未被满足，则对进程i的请求消息发送回复，否则将其加入到deferQueue。原因如下：条件1是明显的；关于条件2，如果进程j已经收到了进程i的消息回复，说明进程i先前发出的请求消息肯定已经被进程j接收（换言之，进程i若发送过请求消息，则此请求消息必定已经缓存到了进程j的requestPriorityQueue），因此消除了消息延迟（乱序）的影响。另一方面，当进程i收到请求回复消息时，它会先将其加入到replyQueue，并判断发送此回复消息的进程是否被加入到了其deferQueue中，如果已经加入到了，则将其移除，然后对此进程发送回复消息（因为进程i确认它已经收到被移除进程的回复消息）。其它的算法逻辑同论文中描述一致。事实上，消除消息按序到达的关键为deferQueue。算法的公平性容易保证。而消息带宽为{request: n-1, reply: n-1, release: n-1} =&gt; 3(n-1)，其中n为进程数。 Lmpoart Distributed Mutual Exclusion算法的相比Ricart Agrawala算法在实现上更为复杂。通信层采用peer to peer模型。 Token RingToken Ring是基于令牌的互斥算法。是一种简单的互斥算法模型，局限性也较大。系统内部只有一种消息：传递 token 的(OK)消息。算法核心为：将所有进程在逻辑上组成一个环，并将 token 在环上依次传递，获取到 token 的进程则具备进行临界区的条件，未收到 token 的进程则必须等待。在进程启动时，必须先将 token 传递给某一进程，若此接收进程需要锁，则进入临界区，执行完临界区代码后，再将 token 传递给相邻的下一个进程。否则直接将 token 传递给相邻下一个进程。算法的公平性同样易保证。消息带宽为n-1，其中n为进程数。 Token Ring算法较易实现，同样采用peer to peer通信模型。注意进程启动时，初始的 token 持有者。 关于测试在实现上述四种算法时(go语言)，采用TCP协议（可靠的）。测试的流程包含两个独立的阶段：Phase a. 每个进程独立的重复以下操作若干次。 执行本地操作。采用 sleep [100, 300]ms 来模拟。 开始进入临界区(critical section)。执行获取互斥锁逻辑。 执行临界区代码。对一个共享变量进行累加，在 [100, 200]ms超时时间内，每隔100ms，对共享变量随机增加 [1,10]。将累加过程写入文件，同时将累加的中间值记录到全局数组。 退出临界区。执行释放互斥锁逻辑。 Phase b. 每个进程独立的重复以下操作若干次。 进程号为偶数的进程 sleep [100, 300]ms，然后重复 Phase a 操作流程。进程号奇数的进程直接重复 Phase b 流程。 对上述四个分布式互斥算法的测试结果的验证侧重于两个方面： 算法正确性。通过检查 Phase a&amp;b 中全局数组的记录情况来确保共享资源的互斥访问。另外，核查 Phase a&amp;b 中进程访问共享资源的访问日志文件。 带宽与延时。统计每个进程的消息读写数目，及获取互斥锁的延时，并计算平均延时。 参考代码在这里。 参考文献： [1] Ricart G, Agrawala A K. An Algorithm for Mutual Exclusion in Computer Networks[R]. MARYLAND UNIV COLLEGE PARK DEPT OF COMPUTER SCIENCE, 1980.[2] Lamport L. Time, clocks, and the ordering of events in a distributed system[J]. Communications of the ACM, 1978, 21(7): 558-565.[3].CMU Distributed System Lecture.]]></content>
      <categories>
        <category>分布式系统</category>
        <category>互斥算法</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>互斥算法</tag>
        <tag>资源共享</tag>
        <tag>lamport clock</tag>
        <tag>逻辑时钟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统时间、时钟与事件顺序]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%97%B6%E9%97%B4%E3%80%81%E6%97%B6%E9%92%9F%E4%B8%8E%E4%BA%8B%E4%BB%B6%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[如何确定分布式系统各节点（进程）中事件发生的先后顺序至关重要。时钟不一致会导致系统发生不可预料的逻辑错误。然而绝大部分情况下，不能依赖于物理时钟，因为不同的系统的物理时钟总会存在不同程度的时钟漂移（多处理器机器中也类似），即便各节点定期通过网络从时钟源进行时钟同步，也无法确保各节点时钟完全一致。因此，早在1978年，Leslie Lamport 便提出了逻辑时钟的概念，并描述了如何利用逻辑时钟来定义分布式系统中事件的发生顺序。它大致基于事件发生的因果关系，并保证能够正确排列系统中具有因果关系的事件，这使得分布式系统在逻辑上不会将具有因果关系事件的发生顺序倒置。 时钟同步事实上，计算机的时钟会以不同速率来计时，普通的石英钟漂移(skew drift)1秒所需的时间大概为11-12天。因此如果使用物理时钟physical clock所定义的时间戳来确定系统中事件发生顺序，需要对物理时钟进行定期同步(clock synchronization)。在理想网络环境下，通过网络将带有时间戳的消息在时钟源（UTC,Coordinated Universal Time）与本地机器之间传输，能够保证本地时间与时钟源基本一致。但事实上，网络是异步的且有延时，因此无法保证不同节点之间的时钟完全同步。尽管如此，我们可以通过算法来尽可能提高时钟同步精度。著名的时钟同步算法如Cristian&#39;s Time Sync和Berkeley algorithm。 Lamport Clock相比于通过同步物理时钟的方式来协调各节点的时间，在分布式系统中，更为普遍且合理的方式是使用逻辑时钟(logical clock)。Lamport 提出的逻辑时钟舍弃了物理时钟固有的无限粒度的性质，它基于事件发生的因果关系(causality)。换言之，所有的事件通过happened before来关联，以-&gt;表示。对于事件a与b，a-&gt;b表示a happened before b，它是一种偏序关系(partial order)，分布式系统中所阐述的事件发生的先后顺序一般为偏序。Lamport 在分布式系统内定义了三种类型的事件，包括进程（节点）内事件、进程发送消息事件以及进程接收消息事件。a happened before b由以下三个条件中任一一个触发： 若a与b表示同一进程内的事件，并且a发生在b之前，则有a-&gt;b。 若a代表某一进程发送消息的事件，b代表另一进程接收此消息的事件，则有a-&gt;b。 happened before关系满足传递性。 如下图（水平方向表示物理时钟增加方向，垂直方向表示不同进程），由规则(1): a-&gt;b及c-&gt;d; 由规则(2): b-&gt;c和d-&gt;f; 由规则(3): b-&gt;f。但并非所有的事件都能通过-&gt;关联，比如a与e为不同进程不同消息链上的事件，则只能被定义为并发的两个事件，记作a||e。事实上，事件a与e没有因果关系，因此，从系统正确性的角度而言，它们之间真正的发生顺序不会影响到系统的正确性，所以我们不需要关注它们发生的先后顺序。 如果将系统中所有发生的事件e标记一个单调递增的时间戳(L(e))（与物理时钟没有关系），也称为lamport timestamp/clock，每一个进程都会维护自己的逻辑时钟，时间戳标记原理如下： 每个事件对应一个初始的时间戳，初始值为0。 如果发生的事件为进程内事件，则时间戳加1。 如果事件为发送事件，则将时间戳加1，并在消息中带上该时间戳。 如果事件为接收事件，则其时间戳为max(进程时间戳，消息中附带的时间戳)+1。 如下图，p1、p2以及p3都有自己的初始的逻辑时钟0；进程的全局（当前）逻辑时钟即为当某一事件发生之后的逻辑时钟值，如事件a与b的逻辑时钟值分别为0+1=1和1+1=2。而发p1发送消息m1给p2后，c的逻辑时钟值为max(0, 2)+1=3。 在lamport clock表示法中，对于事件e1与e2，e1-&gt;e2能推断出L(e1)&lt;L(e2)，但反之不成立，即L(e1)&lt;L(e2)不能推断出e1-&gt;e2，如在图(2)中，L(b)&gt;L(e)，但实际上有b||e。另外，并发事件也是类似的，即若L(e1)=L(e2)可以推断出e1||e2，但反之不成立。对于lamport clock，并发事件没有可比性，正如上文所述，并发事件发生的先后顺序并不影响系统逻辑正确性。 在lamport clock表示法中，无法确定没有因果关系的事件的先后顺序，而大多数分布式系统确实需要对所有的事件进行全局排序(total order)，而不仅仅得到影响系统正确性的事件之间的偏序关系(partial order)。换言之，为了得到一个全局的事件发生顺序，必须对并发事件进行先后发生顺序的判定。因为并发事件真正发生的先后顺序不影响系统的准确性，因此可以为它们统一制定一个任意顺序规则（事实上，lamport clock就是这么考虑的）。比如同其它因果关系事件类似，以逻辑时钟(L(e))的大小来判定，逻辑时钟小的发生在前，反之则发生在后。而对于逻辑时钟相同的并发事件，在lamport clock算法当中，给出的解释是根据进程号(PID)的大小来确定，进程编号更小的发生在前。其实，Lamport 在论文中提到过，也可以采用其它方式来确定并发事件的先后顺序，这似乎没有理论依据，但是正如前面所述，通过引入lamport clock，可以在逻辑上保证系统的正确性，我们不关心那些不影响系统正确运行的事件之间的顺序。但以进程编号作为依据，似乎影响到了系统的公平性，比如当两个进程竞争同一物理资源，物理时间上先发出请求的进程不一定能先锁定资源，但这并不会造成系统逻辑错误。在lamport clock原论文中，给出的实例便是分布式系统资源竞争或者互斥占用，大家可以参考原论文。 Vector Clock前文提到lamport clock存在一个缺点，即对于事件e1与e2，L(e1)&lt;L(e2)并不能推导出e1 happened before e2，换言之，其只能确定单方的因果关系关联。vector clock是在lamport clock上演进的一种逻辑时钟表示法，它完善了lamport clock这一缺陷，能提供完整的因果关系关联。在vector clock表示法中，每个进程维护的不仅仅是本进程的时间戳，而是通过一个向量(vector)来记录所有进程的lamport clock以此作为进程的逻辑时钟，即进程事件的逻辑时钟被表示为：v(e)[c1, c2..., cn]，其中ci为进程i中先于事件e发生的事件。vector clock的逻辑时钟标记原理同lamport clock原理类似。 如下图，在进程p1中，事件a的逻辑时钟为(1,0,0)，发送消息m的事件b的逻辑时钟为(2,0,0)。在进程p2中，其接收消息m1的事件c的逻辑时钟为max((0,0,0),(2,0,0))+1=(2,1,0)。此时，对于事件e1与e2，由e1 happened before e2推断出v(e1)&lt;v(e2)，反之亦然。在vector clock表示法中，事件c与e是并行事件，记作c&lt;-&gt;e，因为我们不能推导出v(c)&lt;=v(e)，也不能推导出V(e)&lt;=v(c)。注意，此时逻辑时钟值的比较(&lt;|&gt;|&lt;=|&gt;=)是对向量的分量逐一比较。 参考资料：（插图出自 CMU Lecture）[1] https://en.wikipedia.org/wiki/Cristian%27s_algorithm[2] https://en.wikipedia.org/wiki/Berkeley_algorithm[3] Lamport L. Time, clocks, and the ordering of events in a distributed system[J]. Communications of the ACM, 1978, 21(7): 558-565.[4].CMU 15-440 Distributed System Lecutre 9]]></content>
      <categories>
        <category>分布式系统</category>
        <category>逻辑时钟</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>lamport clock</tag>
        <tag>逻辑时钟</tag>
        <tag>时钟同步</tag>
        <tag>物理时钟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[groupcache 设计原理剖析]]></title>
    <url>%2F2018%2F10%2F29%2Fgroupcache-%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[groupcache是一个用go实现的分布式k/v缓存及缓存填充库，它的作者也是memcached的作者，它已在Google多个生产环境中使用。它非常小巧精致，比较适用于分布式缓存的学习。它本身只是一个代码包（大约2000行代码，不需要配置服务器，在不同的请求处理场合，它可以充当客户端或者服务器的角色。它支持一致性哈希，即通过一致性哈希来对查询请求进行路由。对于缓存的具体策略，groupcache采用的是LRU，使用了一个List和一个Map来实现，非常简单。下面先简述本地缓存的基本模型和常见问题，然后剖析groupcache的设计原理。 单机缓存或者本地缓存是简单的，通过在内存中维护一个cache，当收到查询时，先查询cache是否已缓存查询结果，如果命中则直接返回，否则必须到存储系统执行查询，然后将结果先缓存到cache，然后返回结果。当然，这是本地缓存的基本模型，一般而言，缓存系统都面临着诸如缓存穿透、缓存雪崩及缓存击穿等问题。 缓存穿透指的是查询一定不存在的数据，此时从数据源查询不到结果，因此也无法对结果进行缓存，这直接导致此类型的查询请求每次都会落到数据源层，不仅使得缓存失效，当请求数量过多时也会浪费资源。 缓存雪崩指的是大量的缓存的过期时间被设置为相同或近似，使得缓存失效时，所有的查询请求全部落地到数据源层，同样，此时数据源层存在服务不可用的可能性。 缓存击穿则指的是对于那些热点数据，在缓存失效时，高并发的查询请求也会导致后端数据源层崩溃。 对于groupcache的设计，文章从一致性哈希、缓存命名空间、热数据扩散以及缓存过滤几个方面进行阐述。 一致性哈希一致性哈希最初是在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中提出，目标是致力于解决因特网中的热点(Hot spot)问题，并真正应用于p2p环境，它弥补简单的哈希算法的不足。一般可以从四个方面来衡量哈希算法的适用性。 平衡性。平衡性即哈希的结果能够尽可能的分散到所有节点或缓冲，以保证缓冲空间被最大程度使用。 单调性。单调性即如果当前缓存系统已经存在被映射的缓冲内容，当有新的节点加入到系统时，哈希算法应该能够尽可能保证原有已分配的的缓冲内容只能被映射到原有的对应节点或者新的节点，而不能被映射到旧的节点集合的其它节点。 分散性。分布式环境中，不同终端所见的节点范围有可能不同（因为可能只能看见部分节点），这会导致不同终端的哈希结果不一致，最终，相同的内容被映射到了不同的节点。而分散性则专门用于描述此种情况发生的严重程度。好的哈希算法应该尽量避免发生这种情况，即降低分散性。 负载。本质上与分散性阐述的是同一问题。但它从节点出发，即某一特定的节点应该尽可能被相同的缓冲内容所映射到，换言之，避免（不同终端）将相同的内容映射到不同的节点。 所谓一致性哈希，简而言之，即将节点与缓冲内容分别映射到一个巨大的环形空间中，最终内容的缓存节点为在顺时针方向上最靠近它的节点。可以发现，系统中节点的添加与删除，一致性哈希算法仍能基本满足以上四个特性。另外一个关键问题是，当集群中节点数量较少时，节点分布不均匀（即节点所负责的内容范围相差较大）会直接导致内容（数据）倾斜，因此一般会引入虚拟节点，即将节点映射为虚拟节点。如此，整个缓存映射过程便拆分为两个阶段：对于特定缓冲内容，先找到其映射的虚拟节点，然后再由虚拟节点映射到物理节点。 一致性哈希在分布式缓存中充当查询路由角色，因为不同节点负责特定的key集合。因此，如果此时当查询没能在本节点缓存中命中时，则需通过一致性哈希路由特定节点(peer)，然后借助http发送数据查询请求，请求的协议格式为: GET http://peer/key。因此，所有节点必须监听其它节点的数据查询请求，同时具备相应的请求处理模块。 缓存命名空间即便是在单个节点上，也可以创建若干个不同名称的缓存命名空间，以使得不同命名空间的缓存相互独立。如此，可以在原本针对key进行分片的基础上，丰富缓存功能。因此，节点间的数据查询请求协议格式变更为：GET http://peer/groupname/key。 热点数据扩散分布式缓存系统，不同的节点会负责特定的key集合的查询请求。但因为并非所有的key的访问量是均匀的，因此，存在这种情况：某些key属于热点数据而被大量访问，这可能导致包含该key的节点无法及时处理甚至瘫痪。考虑到这一点，groupcache增加了热点数据自动扩展的功能。即针对每一个节点，除了会缓存本节点存在且大量被访问的key之外（缓存这些key的对象被称之为maincache），也会缓存那些不属于本节点，但同样被大量访问（发生大量地miss cache）的key，而缓存这些key的对象被称这为hotcache，如此便能缓解热点数据的查询请求集中某一个节点的问题。 缓存过滤机制groupcache的singleflight模块实现了缓存过滤机制。即在大量相同的请求并发访问时，若缓存未能命中，则会触发大量的Load过程。即所有的查询请求全部会落到数据源（如DB）或从其它节点加载数据，因此考虑到节点可靠性，此时DB存在因压力过大而导致服务不可用的情况，同时也浪费资源。groupcache设计所提供的解决方案是：尽管存在并发的查询，但能保证只有一个请求能够真正的转发到DB执行查询，而其余的请求都会阻塞等待，直至第一个请求的查询结果返回，同时，其它请求会使用第一个请求的查询结果，最后再返回给客户端。singleflight通过go的sync.WaitGroup实现同一时间相同查询请求的合并。 最后，虽然官方声称groupcache在很多场景下已经成为memcached的替代版，但其本身存在固有的”局限性”。 groupcache采用的是LRU缓存机制，使用List和Map实现，不支持过期机制（不支持设置过期时间），也没有明确的回收机制（只是简单地将队尾的数据移除），但能够控制缓存总大小在用户设置的阈值之下。 groupcache不支持set、update以及delete，即对于客户端而言，只能执行get操作。 groupcache针对key不支持多个版本的值。 总而言之，groupcache是一个值得学习的开源分布式缓存系统，通过阅读源码，一方面可以了解分布式缓存相关的设计原则，也能学习编程相关的设计经验。]]></content>
      <categories>
        <category>分布式系统</category>
        <category>分布式缓存</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>分布式缓存</tag>
        <tag>LRU缓存</tag>
        <tag>一致性哈希</tag>
        <tag>缓存过滤机制</tag>
        <tag>缓存击穿</tag>
        <tag>热点数据扩散</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Live Sequence Protocol 实现]]></title>
    <url>%2F2018%2F10%2F25%2FLive-Sequence-Protocol-%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[分布式环境中，网络不稳定导致消息（数据包）的传输存在乱序、重复和丢失的情况，同时，节点宕机也不可避免。如何优雅地处理这些问题，是构建一个健壮的分布式系统的关键。网络的复杂性使得数据包传输协议至关重要。低级别的IP协议提供不可靠的数据报服务，即消息可能延时、重复或丢失，另外，它也限制了在网络节点中传输的消息的最大字节数，因此很少直接利用IP协议来构建分布式应用。在传输层，UDP也不提供可靠的数据报服务，但它可以通过端口定向传输报文。而TCP则会保证消息传输的可靠性、有序性，并允许任意字节大小的消息传递，还提供额外的功能，如流量控制、拥塞控制。 我们的目的是实现一个基于UDP、具备TCP几个关键特性的消息传输协议 (Live Sequence Protocol），同时它还具备如下功能： 不同于UDP或TCP，它支持 client-server通信模型 。 server会维护若干个client的连接。 server与client的通信是通过向对方发送消息来实现，消息大小限制与UDP相同。 消息传输是可靠的：消息一旦发送，就会被顺序接收，且每个消息只会被接收一次。 server与client可以检测连接的状态。 协议具体的工作原理、关键特性、运行流程及开放使用的接口可以参考p1.pdf。下面我会讨论协议实现过程中的几个关键点，以及个人在实现过程中遇到的棘手的问题。 系统逻辑框架构建清晰且优雅地构建整个系统的逻辑框架至关重要，代码框架设计关系到后期功能模块调试与扩展，不合理的系统逻辑框架设计会使得后期的扩展寸步难行，也会导致代码的可调试性、可读性变差。因此，在编写出你的第一个可用的版本之前，尽可能合理地安排系统框架，这需要理解并梳理系统的主干及各分支（异常）运行流程，为了更简单、高效且合理地实现模块功能，必须尽可能熟悉(go)语言的特性(channel、gorountine及interface)。 协议实现文档清晰地描述了协议的完整工作流程，按照此流程，其核心是epoch event触发后，协议的应对逻辑，可以实现出一个可运行的版本。合理安排程序框架关键在于处理好以下三个方面的问题： 哪些功能逻辑应该被顺序执行，如何保证同步顺序执行。比如，当创建client后，只有当其与server建立连接connection（抽象连接，并非消息传输所使用的连接）后才能返回，同时启动后台服务。注意client创建UDP连接到server可能会尝试多次，因为server可能存在慢启动问题，而且Connect消息也可能丢失。 系统需要哪些后台服务(background goroutine)， 后台服务如何可靠地同主线程协调交互。比如，对于client而言，至少需要三个goroutine来处理消息。 read goroutine持续从连接中读取消息，直到连接关闭。 write goroutine，因为写操作的调用是非阻塞的，但由于滑动窗口大小限制，并非所有消息都能立刻cache到滑动窗口并立即发送出去，因此，可以将用户wirte的消息放入到消息的write channel中，然后由专门的后台服务从channel中取消息，并在恰当的时候发送消息。 epoch event trigger goroutine，即处理与epoch相关的逻辑，超时如何处理？接收到Ack消息或Data消息如何处理？达到max epoch时如何处理？ 确保开放接口的实现符合协议规范中预定义的准则要求。比如，server的Read接口的调用会阻塞直到其从任一client收到消息，然后返回消息的payload及对应的connection ID。如果连接丢失，关闭或者Server主动关闭终止，都应该返回错误提示。这个方法不应该被简单地设计成从连接中持续读取数据，因为Server可能连接多个client，针对每一个client 连接的读取，必须启用单独的goroutine。所以，一种简单的设计是server并发地从各连接读取数据，若通过了校验（如保证用户调用Read所返回的数据正是用户所期望的），则将数据放入到channel，让Read持续从channel中取数据，注意数据一旦添加到channel中，则会以放入的顺序被Read取出，并返回给用户。 理解UDP通信本质大家可能对TCP原理及编程更为熟悉，UDP相对简单，但因为lsp(Live Sequence Protocol)基于UDP，并在更高的协议抽象层面具备TCP的特性，所以，不要混淆了二者的通信原理。UDP是无连接的！它会完全按照程序员的意愿发送消息，它不考虑对方主机是否存在或正常工作，也不会主动重发消息，因此，也就无法保证消息的可靠接收与发送。 所以，server不需要也不能维护其与client的连接！但应当在sever端创建并维护与其通信的client关联的信息实体（需包含哪些数据？），那何时创建？答案是当server读取到数据时，因为此时可以获取读取所返回的client地址，server可以通过cache已经连接的信息来判断此次读取对应的连接是否是新的连接。若不是，则直接进入消息读取处理逻辑，否则需要先初始化server维护的client相关联的信息实体。 最后，注意server与client使用的是不同的UDP读写通信接口。（client直接持有与server通信的连接，而server是通过指定地址（IP+port）发送与接收消息）。 如何实现滑动窗口滑动窗口sliding window是协议实现流量控制的关键，是整个协议的功能核心，并且其与TCP的滑动窗口机制类似。关于滑动窗口，在理解它的工作原理后，重点考虑以下三个方面： 设计滑动窗口的数据结构。 消息应该被有序添加到滑动窗口。 发送消息窗口需要标识每一条消息是否已经被ack。 发送消息所关联的滑动窗口latestSentDataMsg。以client作为示例，维持其发送消息的窗口，以便对未按时返回Ack的消息进行重发（已发送的data消息可能会丢失，或者接收主机响应的Ack消息丢失）。 因为窗口内的消息所返回的Ack是无序的（消息异步发送，网络传输也不能保证消息按序到达），所以，需要维护一个指针，表示当前返回的Ack消息的最小的序号receivedAckSeqNum，以作为窗口向前推进的依据。 当client发送data消息时，需同时将其cache到latestSentDataMsg。而当其接收到Ack消息时，需要执行更新此指针receivedAckSeqNum的逻辑。而server则需要对其所维护的每一个连接构建对应的发送消息窗口，但处理逻辑类似。 接收消息所关联的滑动窗口latestReceivedDataMsg。同样以client作为示例，维持其接收消息的窗口，以便在计时器超时后，对最近收到的若干个data消息，重发Ack消息。 同样，接收消息窗口也是无序的，因此，为了保证返回给用户的消息有序，需要维护一个指针，表示下一个期望接收到的data消息序号nextReceiveDataSeqNum（或者是当前已经接收到的最大的data消息的消息序号），它是依次递增的。对于接收到的任何data消息，若其SeqNum在此指针之后，都应该直接添加（暂时缓存）到latestReceivedDataMsg中，而不应该作为Read调用的返回结果。 当client收到server的data消息时，也需要将其cache到latestReceivedDataMsg，并判断是否需要更新nextReceiveDataSeqNum，若需要更新，则应当将更新过程中所涉及到的cache在接收消息窗口中的data消息按序添加到供Read接口所读取的channel。server同样是一个连接对应一个接收消息窗口。 如何实现流量控制流量控制表示若当前主机有过多的消息未被ack（网络拥塞），因此发送主机需要对用户调用Write接口的data消息进行阻塞以延缓发送。其实现关键是滑动窗口机制。具体实现原理为： 当用户调用Write接口以发送消息时，将消息添加到消息发送队列channel，然后返回，不能阻塞。 后台服务write goroutine从消息发送队列中不断的取消息，但在消息正式发送前，需要检测消息发送滑动窗口是否空闲idle，并且包含多少空闲的slot。 空闲的slot数目可以根据以下表达式计算：idleSlotNum := cli.params.WindowSize-(lastMsg.SeqNum-cli.receivedAckSeqNum)，其中，lastMsg为消息发送窗口中最后一个消息，即SeqNum最大的消息。 如果idleSlotNum大于0，则可以发送对应数目的消息，并将已经发送的消息记录到消息发送窗口，同时递增nextSendDataSeqNum指针。否则，write goroutine应该被阻塞住。那如何解除阻塞？每当client在接收到Ack消息时都要去尝试解除阻塞。 如图所示，当滑动窗口处于(a)的情况下，当用户调用Write以发送消息时，消息会被阻塞在write channel中，因为此时receivedAckSeqNum为9，消息发送窗口的idle的slot数目为：5-(14-9)=0。而当client接收若干Ack消息后，滑动窗口转移到(b)状态时，注意到receivedAckSeqNum从9逐一递增到11，消息发送窗口idleSlotNum为：5-(14-11)=2，因此窗口前移，并可以从write channel中顺序取出两个消息，进行发送。 如何检测消息重复消息重复主要包括data和Ack消息的重复接收。以client作为示例。 data消息的重复接收。 当client读取到data消息后，需要判断消息是否已经接收过。若消息重复，则直接返回Ack消息，否则应该先将消息cache到latestReceivedDataMsg。 可以通过消息的SeqNum来去重。这涉及两种情况：其一，消息已经被Ack，并且已经从latestReceivedDataMsg中移除，我们称之为消息被丢弃(discarded)。其二，消息被Ack，但仍然cache在latestReceivedDataMsg中。 Ack消息的重复接收。Ack消息的去重逻辑同data消息类似。 如何保证消息顺序发送主机异步发送消息，且消息在网络中传输也有不同程度的延迟，因此接收主机接收的消息序列的顺序很可能与发送主机发送的消息顺序不同。如何保证消息顺序？准确而言，如何以发送主机发送消息的顺序来返回给用户。 针对具备滑动窗口机制的消息传输，可以保证滑动窗口前所接收的消息，即已经被discarded的消息肯定是有序返回给用户的。而滑动窗口内的消息，因为无法规避从网络中读取乱序消息的问题，但在读取到消息后可以控制以何种顺序将消息返回给用户。简单而言，将收到的data消息先cache在latestReceivedDataMsg中，然后通过指针nextReceiveDataSeqNum来判断是否应该将窗口中cache的消息返回给用户。 如何优雅地关闭连接保证连接优雅地关闭是一个非常棘手的问题。其中，相比于client端的连接关闭，server的关闭又更为复杂。协议规范清晰地描述了client及server在关闭连接时需要注意的问题。其核心是： 当存在pending消息时，需要将其处理完成（即需保证接收到Ack消息）。 同时，一旦data消息被加入到write channel，它必须保证最后能够被发送出去。client的关闭相对简单，具体处理逻辑为：当用户调用Close接口时，需要判断是否存在pending消息，如何检测？两个条件： 保证消息发送窗口的最后一个消息的SeqNum恰好为其持有的receivedAckSeqNum的值。 保证write channel中没有任何未被处理的消息。因此如果此时存在pending消息，Close会被阻塞。那如何解除阻塞？每当client在接收到Ack消息时都要去尝试解除阻塞。此外，值得注意的是，在阻塞的过程中，如果触发了max epoch event，则client应该立刻返回，因为这表明连接已经discarded，此时要么所有pending消息已被处理，要么server主动关闭了连接。server的CloseConn接口可以看作是client的Close接口的非阻塞版本。而Close接口需要协调所有的connection的关闭。同样，server的某个连接也可能到达max epoch，此时其对应的连接应该被关闭。当所有连接都关闭时，Close才能返回。在连接关闭时，需要及时退出对应的background goroutine。 需注意的细节问题往往一些编程方面的细节，包括逻辑漏洞或者被忽视的语法问题会造成很长时间的调试。而且，当通信过程中，数据交换复杂变得越发复杂时，很难从庞大的日志文件中找出错误的根源。个人在实现的过程中，遇到两个问题： dead lock。死锁很容易产生，一般有两个原因，其一，资源的相互持有，造成两个线程都无法向前推进。其二，没有正确嵌套使用锁，你需要清楚锁是否可重入。 buffered channel。其导致的问题比较隐蔽，你首先要明确是使用带缓冲的channel或者不带缓冲的channel，如果是buffered channel，你需要确定它的大小，如果你不确定缓冲区数量是否足够，建议设置的稍大一些，但这个前提是，必须在合适的时机清空buffered channel，避免在复用buffered channel之后导致逻辑受到影响。 最后，需要提醒的是，分布式程序异步、并发，且网络复杂的特性导致其很难debug。所以，尽可能设计完善的日志流程，以帮助跟踪未符合期望的执行逻辑，并定位问题。 另外，cmu提供较为完善的测试程序，如果程序出现问题，可以对某一个或几个子测试用例进行单独测试，熟悉测试用例代码，了解测试用例流程是有必要的。 参考代码在这里]]></content>
      <categories>
        <category>分布式系统</category>
        <category>传输协议</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>网络编程</tag>
        <tag>传输协议</tag>
        <tag>可靠服务</tag>
        <tag>流量控制</tag>
      </tags>
  </entry>
</search>
